[
    {
        "idx": 0,
        "text": "Continuous generative models proved their usefulness in high-dimensional data, such as image and audio generation.",
        "labels": "BAC"
    },
    {
        "idx": 0,
        "text": "However, continuous models for text generation have received limited attention from the community.",
        "labels": "GAP"
    },
    {
        "idx": 0,
        "text": "In this work, we study continuous text generation using Transformers for neural machine translation (NMT).",
        "labels": "PUR"
    },
    {
        "idx": 0,
        "text": "We argue that the choice of embeddings is crucial for such models, so we aim to focus on one particular aspect{''}:'' target representation via embeddings.",
        "labels": "PUR"
    },
    {
        "idx": 0,
        "text": "We explore pretrained embeddings and also introduce knowledge transfer from the discrete Transformer model using embeddings in Euclidean and non-Euclidean spaces.",
        "labels": "MTD"
    },
    {
        "idx": 0,
        "text": "Our results on the WMT Romanian-English and English-Turkish benchmarks show such transfer leads to the best-performing continuous model",
        "labels": "CLN"
    },
    {
        "idx": 1,
        "text": "Pretrained multilingual encoders enable zero-shot cross-lingual transfer, but often produce unreliable models that exhibit high performance variance on the target language. ",
        "labels": "GAP"
    },
    {
        "idx": 1,
        "text": "We postulate that this high variance results from zero-shot cross-lingual transfer solving an under-specified optimization problem.",
        "labels": "PUR"
    },
    {
        "idx": 1,
        "text": "We show that any linear-interpolated model between the source language monolingual model and source + target bilingual model has equally low source language generalization error, yet the target language generalization error reduces smoothly and linearly as we move from the monolingual to bilingual model, suggesting that the model struggles to identify good solutions for both source and target languages using the source language alone.",
        "labels": "CLN"
    },
    {
        "idx": 1,
        "text": "Additionally, we show that zero-shot solution lies in non-flat region of target language error generalization surface, causing the high variance.",
        "labels": "CLN"
    },
    {
        "idx": 2,
        "text": " Linguistic style is an integral component of language.",
        "labels": "BAC"
    },
    {
        "idx": 2,
        "text": "Recent advances in the development of style representations have increasingly used training objectives from authorship verification (AV){''}:'' Do two texts have the same author? The assumption underlying the AV training task (same author approximates same writing style) enables self-supervised and, thus, extensive training.",
        "labels": "BAC"
    },
    {
        "idx": 2,
        "text": "However, a good performance on the AV task does not ensure good {``}general-purpose{''} style representations.",
        "labels": "GAP"
    },
    {
        "idx": 2,
        "text": "For example, as the same author might typically write about certain topics, representations trained on AV might also encode content information instead of style alone.",
        "labels": "GAP"
    },
    {
        "idx": 2,
        "text": "We introduce a variation of the AV training task that controls for content using conversation or domain labels.",
        "labels": "PUR"
    },
    {
        "idx": 2,
        "text": "We evaluate whether known style dimensions are represented and preferred over content information through an original variation to the recently proposed STEL framework.",
        "labels": "MTD"
    },
    {
        "idx": 2,
        "text": "We find that representations trained by controlling for conversation are better than representations trained with domain or no content control at representing style independent from content.",
        "labels": "CLN"
    },
    {
        "idx": 3,
        "text": "A popular approach to decrease the need for costly manual annotation of large data sets is weak supervision, which introduces problems of noisy labels, coverage and bias.",
        "labels": "BAC"
    },
    {
        "idx": 3,
        "text": "Methods for overcoming these problems have either relied on discriminative models, trained with cost functions specific to weak supervision, and more recently, generative models, trying to model the output of the automatic annotation process.",
        "labels": "BAC"
    },
    {
        "idx": 3,
        "text": "In this work, we explore a novel direction of generative modeling for weak supervision{''}:'",
        "labels": "PUR"
    },
    {
        "idx": 3,
        "text": "Instead of modeling the output of the annotation process (the labeling function matches), we generatively model the input-side data distributions (the feature space) covered by labeling functions.",
        "labels": "MTD"
    },
    {
        "idx": 3,
        "text": "Specifically, we estimate a density for each weak labeling source, or labeling function, by using normalizing flows.",
        "labels": "MTD"
    },
    {
        "idx": 3,
        "text": "An integral part of our method is the flow-based modeling of multiple simultaneously matching labeling functions, and therefore phenomena such as labeling function overlap and correlations are captured.",
        "labels": "MTD"
    },
    {
        "idx": 3,
        "text": "We analyze the effectiveness and modeling capabilities on various commonly used weak supervision data sets,",
        "labels": "MTD"
    },
    {
        "idx": 3,
        "text": "and show that weakly supervised normalizing flows compare favorably to standard weak supervision baselines.",
        "labels": "CLN"
    },
    {
        "idx": 4,
        "text": "This paper critically examines the current practices of benchmark dataset sharing in NLP and suggests a better way to inform reusers of the benchmark dataset.",
        "labels": "PUR"
    },
    {
        "idx": 4,
        "text": "As the dataset sharing platform plays a key role not only in distributing the dataset but also in informing the potential reusers about the dataset, we believe data-sharing platforms should provide a comprehensive context of the datasets.",
        "labels": "MTD"
    },
    {
        "idx": 4,
        "text": "We survey four benchmark dataset sharing platforms: HuggingFace, PaperswithCode, Tensorflow, and Pytorch to diagnose the current practices of how the dataset is shared which metadata is shared and omitted.",
        "labels": "MTD"
    },
    {
        "idx": 4,
        "text": "To be specific, drawing on the concept of data curation which considers the future reuse when the data is made public, we advance the direction that benchmark dataset sharing platforms should take into consideration.",
        "labels": "MTD"
    },
    {
        "idx": 4,
        "text": "We identify that four benchmark platforms have different practices of using metadata and there is a lack of consensus on what social impact metadata is.",
        "labels": "CLN"
    },
    {
        "idx": 4,
        "text": "We believe the problem of missing a discussion around social impact in the dataset sharing platforms has to do with the failed agreement on who should be in charge.",
        "labels": "CLN"
    },
    {
        "idx": 4,
        "text": "We propose that the benchmark dataset should develop social impact metadata and data curator should take a role in managing the social impact metadata.",
        "labels": "IMP"
    },
    {
        "idx": 5,
        "text": "Natural language processing (NLP) systems are often used for adversarial tasks such as detecting spam, abuse, hate speech, and fake news.",
        "labels": "BAC"
    },
    {
        "idx": 5,
        "text": "Properly evaluating such systems requires dynamic evaluation that searches for weaknesses in the model, rather than a static test set.",
        "labels": "BAC"
    },
    {
        "idx": 5,
        "text": "Prior work has evaluated such models on both manually and automatically generated examples, but both approaches have limitations: manually constructed examples are time-consuming to create and are limited by the imagination and intuition of the creators, while automatically constructed examples are often ungrammatical or labeled inconsistently.",
        "labels": "GAP"
    },
    {
        "idx": 5,
        "text": "We propose to combine human and AI expertise in generating adversarial examples, benefiting from humans{'} expertise in language and automated attacks{'} ability to probe the target system more quickly and thoroughly.",
        "labels": "PUR"
    },
    {
        "idx": 5,
        "text": "We present a system that facilitates attack construction, combining human judgment with automated attacks to create better attacks more efficiently.",
        "labels": "MTD"
    },
    {
        "idx": 5,
        "text": "Preliminary results from our own experimentation suggest that human-AI hybrid attacks are more effective than either human-only or AI-only attacks.",
        "labels": "CLN"
    },
    {
        "idx": 6,
        "text": "A user-generated text on social media enables health workers to keep track of information, identify possible outbreaks, forecast disease trends, monitor emergency cases, and ascertain disease awareness and response to official health correspondence.",
        "labels": "BAC"
    },
    {
        "idx": 6,
        "text": "This exchange of health information on social media has been regarded as an attempt to enhance public health surveillance (PHS). ",
        "labels": "BAC"
    },
    {
        "idx": 6,
        "text": "Despite its potential, the technology is still in its early stages and is not ready for widespread application.",
        "labels": "GAP"
    },
    {
        "idx": 6,
        "text": "Advancements in pretrained language models (PLMs) have facilitated the development of several domain-specific PLMs and a variety of downstream applications.",
        "labels": "GAP"
    },
    {
        "idx": 6,
        "text": "However, there are no PLMs for social media tasks involving PHS.",
        "labels": "GAP"
    },
    {
        "idx": 6,
        "text": "We present and release PHS-BERT, a transformer-based PLM, to identify tasks related to public health surveillance on social media.",
        "labels": "PUR"
    },
    {
        "idx": 6,
        "text": "We compared and benchmarked the performance of PHS-BERT on 25 datasets from different social medial platforms related to 7 different PHS tasks.",
        "labels": "MTD"
    },
    {
        "idx": 6,
        "text": "Compared with existing PLMs that are mainly evaluated on limited tasks, PHS-BERT achieved state-of-the-art performance on all 25 tested datasets, showing that our PLM is robust and generalizable in the common PHS tasks.",
        "labels": "CLN"
    },
    {
        "idx": 6,
        "text": "By making PHS-BERT available, we aim to facilitate the community to reduce the computational cost and introduce new baselines for future works across various PHS-related tasks.",
        "labels": "CTN"
    },
    {
        "idx": 7,
        "text": "Relation classification models are conventionally evaluated using only a single measure, e.g., micro-F1, macro-F1 or AUC.",
        "labels": "BAC"
    },
    {
        "idx": 7,
        "text": "In this work, we analyze weighting schemes, such as micro and macro, for imbalanced datasets.",
        "labels": "PUR"
    },
    {
        "idx": 7,
        "text": "We introduce a framework for weighting schemes, where existing schemes are extremes, and two new intermediate schemes.",
        "labels": "MTD"
    },
    {
        "idx": 7,
        "text": "We show that reporting results of different weighting schemes better highlights strengths and weaknesses of a model.",
        "labels": "CLN"
    },
    {
        "idx": 8,
        "text": "Recent improvements in automatic news summarization fundamentally rely on large corpora of news articles and their summaries.",
        "labels": "BAC"
    },
    {
        "idx": 8,
        "text": "These corpora are often constructed by scraping news websites, which results in including not only summaries but also other kinds of texts.",
        "labels": "BAC"
    },
    {
        "idx": 8,
        "text": "Apart from more generic noise, we identify straplines as a form of text scraped from news websites that commonly turn out not to be summaries.",
        "labels": "GAP"
    },
    {
        "idx": 8,
        "text": "The presence of these non-summaries threatens the validity of scraped corpora as benchmarks for news summarization.",
        "labels": "GAP"
    },
    {
        "idx": 8,
        "text": "We have annotated extracts from two news sources that form part of the Newsroom corpus (Grusky et al., 2018), labeling those which were straplines, those which were summaries, and those which were both.",
        "labels": "MTD"
    },
    {
        "idx": 8,
        "text": "We present a rule-based strapline detection method that achieves good performance on a manually annotated test set.",
        "labels": "MTD"
    },
    {
        "idx": 8,
        "text": "Automatic evaluation indicates that removing straplines and noise from the training data of a news summarizer results in higher quality summaries,",
        "labels": "CLN"
    },
    {
        "idx": 8,
        "text": "with improvements as high as 7 points ROUGE score.",
        "labels": "RST"
    },
    {
        "idx": 9,
        "text": "Measuring the performance of natural language processing models is challenging.",
        "labels": "BAC"
    },
    {
        "idx": 9,
        "text": "Traditionally used metrics, such as BLEU and ROUGE, originally devised for machine translation and summarization, have been shown to suffer from low correlation with human judgment and a lack of transferability to other tasks and languages.",
        "labels": "BAC"
    },
    {
        "idx": 9,
        "text": "In the past 15 years, a wide range of alternative metrics have been proposed.",
        "labels": "BAC"
    },
    {
        "idx": 9,
        "text": "However, it is unclear to what extent this has had an impact on NLP benchmarking efforts.",
        "labels": "GAP"
    },
    {
        "idx": 9,
        "text": "Here we provide the first large-scale cross-sectional analysis of metrics used for measuring performance in natural language processing.",
        "labels": "PUR"
    },
    {
        "idx": 9,
        "text": "We curated, mapped and systematized more than 3500 machine learning model performance results from the open repository {`}Papers with Code{'} to enable a global and comprehensive analysis.",
        "labels": "MTD"
    },
    {
        "idx": 9,
        "text": "Our results suggest that the large majority of natural language processing metrics currently used have properties that may result in an inadequate reflection of a models{'} performance.",
        "labels": "CLN"
    },
    {
        "idx": 9,
        "text": "Furthermore, we found that ambiguities and inconsistencies in the reporting of metrics may lead to difficulties in interpreting and comparing model performances, impairing transparency and reproducibility in NLP research.",
        "labels": "CLN"
    },
    {
        "idx": 10,
        "text": "Although recent Massively Multilingual Language Models (MMLMs) like mBERT and XLMR support around 100 languages, most existing multilingual NLP benchmarks provide evaluation data in only a handful of these languages with little linguistic diversity. ",
        "labels": "GAP"
    },
    {
        "idx": 10,
        "text": "We argue that this makes the existing practices in multilingual evaluation unreliable and does not provide a full picture of the performance of MMLMs across the linguistic landscape.",
        "labels": "GAP"
    },
    {
        "idx": 10,
        "text": "We propose that the recent work done in Performance Prediction for NLP tasks can serve as a potential solution in fixing benchmarking in Multilingual NLP by utilizing features related to data and language typology to estimate the performance of an MMLM on different languages.",
        "labels": "PUR"
    },
    {
        "idx": 10,
        "text": "We compare performance prediction with translating test data with a case study on four different multilingual datasets,",
        "labels": "MTD"
    },
    {
        "idx": 10,
        "text": "and observe that these methods can provide reliable estimates of the performance that are often on-par with the translation based approaches, without the need for any additional translation as well as evaluation costs.",
        "labels": "RST"
    },
    {
        "idx": 11,
        "text": "Behavioural testing{---}verifying system capabilities by validating human-designed input-output pairs{---}is an alternative evaluation method of natural language processing systems proposed to address the shortcomings of the standard approach: computing metrics on held-out data.",
        "labels": "BAC"
    },
    {
        "idx": 11,
        "text": "While behavioural tests capture human prior knowledge and insights, there has been little exploration on how to leverage them for model training and development.",
        "labels": "GAP"
    },
    {
        "idx": 11,
        "text": "With this in mind, we explore behaviour-aware learning by examining several fine-tuning schemes using HateCheck, a suite of functional tests for hate speech detection systems.",
        "labels": "PUR"
    },
    {
        "idx": 11,
        "text": "To address potential pitfalls of training on data originally intended for evaluation, ",
        "labels": "PUR"
    },
    {
        "idx": 11,
        "text": "we train and evaluate models on different configurations of HateCheck by holding out categories of test cases, which enables us to estimate performance on potentially overlooked system properties.",
        "labels": "MTD"
    },
    {
        "idx": 11,
        "text": "The fine-tuning procedure led to improvements in the classification accuracy of held-out functionalities and identity groups,",
        "labels": "RST"
    },
    {
        "idx": 11,
        "text": "suggesting that models can potentially generalise to overlooked functionalities.",
        "labels": "CLN"
    },
    {
        "idx": 11,
        "text": "However, performance on held-out functionality classes and i.i.d. hate speech detection data decreased, which indicates that generalisation occurs mostly across functionalities from the same class and that the procedure led to overfitting to the HateCheck data distribution.",
        "labels": "CLN"
    },
    {
        "idx": 12,
        "text": "Meaning is context-dependent, but many properties of language (should) remain the same even if we transform the context.",
        "labels": "BAC"
    },
    {
        "idx": 12,
        "text": "For example, sentiment or speaker properties should be the same in a translation and original of a text.",
        "labels": "BAC"
    },
    {
        "idx": 12,
        "text": "We introduce language invariant properties: i.e., properties that should not change when we transform text, and how they can be used to quantitatively evaluate the robustness of transformation algorithms.",
        "labels": "PUR"
    },
    {
        "idx": 12,
        "text": "Language invariant properties can be used to define novel benchmarks to evaluate text transformation methods.",
        "labels": "MTD"
    },
    {
        "idx": 12,
        "text": "In our work we use translation and paraphrasing as examples, but our findings apply more broadly to any transformation.",
        "labels": "MTD"
    },
    {
        "idx": 12,
        "text": "Our results indicate that many NLP transformations change properties.",
        "labels": "CLN"
    },
    {
        "idx": 12,
        "text": "We additionally release a tool as a proof of concept to evaluate the invariance of transformation applications",
        "labels": "MTD"
    },
    {
        "idx": 13,
        "text": "Large-scale pre-trained language models have shown remarkable results in diverse NLP applications.",
        "labels": "BAC"
    },
    {
        "idx": 13,
        "text": "However, these performance gains have been accompanied by a significant increase in computation time and model size, stressing the need to develop new or complementary strategies to increase the efficiency of these models. ",
        "labels": "GAP"
    },
    {
        "idx": 13,
        "text": "This paper proposes DACT-BERT, a differentiable adaptive computation time strategy for BERT-like models.",
        "labels": "PUR"
    },
    {
        "idx": 13,
        "text": "DACT-BERT adds an adaptive computational mechanism to BERT{'}s regular processing pipeline, which controls the number of Transformer blocks that need to be executed at inference time.",
        "labels": "MTD"
    },
    {
        "idx": 13,
        "text": "By doing this, the model learns to combine the most appropriate intermediate representations for the task at hand.",
        "labels": "RST"
    },
    {
        "idx": 13,
        "text": "Our experiments demonstrate that our approach, when compared to the baselines, excels on a reduced computational regime and is competitive in other less restrictive ones.",
        "labels": "CLN"
    },
    {
        "idx": 13,
        "text": "Code available at https://github.com/ceyzaguirre4/dact{\\_}bert.",
        "labels": "CTN"
    },
    {
        "idx": 14,
        "text": "Transformer-based Natural Language Processing models have become the standard for hate speech detection.",
        "labels": "BAC"
    },
    {
        "idx": 14,
        "text": "However, the unconscious use of these techniques for such a critical task comes with negative consequences.",
        "labels": "GAP"
    },
    {
        "idx": 14,
        "text": "Various works have demonstrated that hate speech classifiers are biased.",
        "labels": "BAC"
    },
    {
        "idx": 14,
        "text": "These findings have prompted efforts to explain classifiers, mainly using attribution methods.",
        "labels": "BAC"
    },
    {
        "idx": 14,
        "text": "In this paper, we provide the first benchmark study of interpretability approaches for hate speech detection.",
        "labels": "PUR"
    },
    {
        "idx": 14,
        "text": "We cover four post-hoc token attribution approaches to explain the predictions of Transformer-based misogyny classifiers in English and Italian.",
        "labels": "MTD"
    },
    {
        "idx": 14,
        "text": "Further, we compare generated attributions to attention analysis.",
        "labels": "MTD"
    },
    {
        "idx": 14,
        "text": "We find that only two algorithms provide faithful explanations aligned with human expectations.",
        "labels": "RST"
    },
    {
        "idx": 14,
        "text": "Gradient-based methods and attention, however, show inconsistent outputs, making their value for explanations questionable for hate speech detection tasks.",
        "labels": "CLN"
    },
    {
        "idx": 15,
        "text": "With many real-world applications of Natural Language Processing (NLP) comprising of long texts, there has been a rise in NLP benchmarks that measure the accuracy of models that can handle longer input sequences.",
        "labels": "BAC"
    },
    {
        "idx": 15,
        "text": "However, these benchmarks do not consider the trade-offs between accuracy, speed, and power consumption as input sizes or model sizes are varied.",
        "labels": "GAP"
    },
    {
        "idx": 15,
        "text": "In this work, we perform a systematic study of this accuracy vs. efficiency trade-off on two widely used long-sequence models - Longformer-Encoder-Decoder (LED) and Big Bird - during fine-tuning and inference on four datasets from the SCROLLS benchmark.",
        "labels": "MTD"
    },
    {
        "idx": 15,
        "text": "To study how this trade-off differs across hyperparameter settings,",
        "labels": "PUR"
    },
    {
        "idx": 15,
        "text": "we compare the models across four sequence lengths (1024, 2048, 3072, 4096) and two model sizes (base and large) under a fixed resource budget. ",
        "labels": "MTD"
    },
    {
        "idx": 15,
        "text": "We find that LED consistently achieves better accuracy at lower energy costs than Big Bird.",
        "labels": "RST"
    },
    {
        "idx": 15,
        "text": "For summarization, we find that increasing model size is more energy efficient than increasing sequence length for higher accuracy.",
        "labels": "RST"
    },
    {
        "idx": 15,
        "text": "However, this comes at the cost of a large drop in inference speed.",
        "labels": "RST"
    },
    {
        "idx": 15,
        "text": "For question answering, we find that smaller models are both more efficient and more accurate due to the larger training batch sizes possible under a fixed resource budget.",
        "labels": "RST"
    },
    {
        "idx": 16,
        "text": "A major issue in open-domain dialogue generation is the agent{'}s tendency to generate repetitive and generic responses.",
        "labels": "BAC"
    },
    {
        "idx": 16,
        "text": "The lack in response diversity has been addressed in recent years via the use of latent variable models, such as the Conditional Variational Auto-Encoder (CVAE), which typically involve learning a latent Gaussian distribution over potential response intents. ",
        "labels": "BAC"
    },
    {
        "idx": 16,
        "text": "However, due to latent variable collapse, training latent variable dialogue models are notoriously complex, requiring substantial modification to the standard training process and loss function. ",
        "labels": "GAP"
    },
    {
        "idx": 16,
        "text": "Other approaches proposed to improve response diversity also largely entail a significant increase in training complexity.",
        "labels": "GAP"
    },
    {
        "idx": 16,
        "text": "Hence, this paper proposes a Randomized Link (RL) Transformer as an alternative to the latent variable models.",
        "labels": "PUR"
    },
    {
        "idx": 16,
        "text": "The RL Transformer does not require any additional enhancements to the training process or loss function.",
        "labels": "MTD"
    },
    {
        "idx": 16,
        "text": "Empirical results show that, when it comes to response diversity, the RL Transformer achieved comparable performance compared to latent variable models.",
        "labels": "CLN"
    },
    {
        "idx": 17,
        "text": "Pre-trained Transformer-based models were reported to be robust in intent classification.",
        "labels": "BAC"
    },
    {
        "idx": 17,
        "text": " In this work, we first point out the importance of in-domain out-of-scope detection in few-shot intent recognition tasks and then illustrate the vulnerability of pre-trained Transformer-based models against samples that are in-domain but out-of-scope (ID-OOS).",
        "labels": "PUR"
    },
    {
        "idx": 17,
        "text": "We construct two new datasets,",
        "labels": "MTD"
    },
    {
        "idx": 17,
        "text": "and empirically show that pre-trained models do not perform well on both ID-OOS examples and general out-of-scope examples, especially on fine-grained few-shot intent detection tasks.",
        "labels": "CLN"
    },
    {
        "idx": 18,
        "text": "Retailing combines complicated communication skills and strategies to reach an agreement between buyer and seller with identical or different goals.",
        "labels": "BAC"
    },
    {
        "idx": 18,
        "text": "In each transaction a good seller finds an optimal solution by considering his/her own profits while simultaneously considering whether the buyer{'}s needs have been met.",
        "labels": "BAC"
    },
    {
        "idx": 18,
        "text": "In this paper, we manage the retailing problem by mixing cooperation and competition.",
        "labels": "PUR"
    },
    {
        "idx": 18,
        "text": "We present a rich dataset of buyer-seller bargaining in a simulated marketplace in which each agent values goods and utility separately.",
        "labels": "MTD"
    },
    {
        "idx": 18,
        "text": "Various attributes (preference, quality, and profit) are initially hidden from one agent with respect to its role; during the conversation, both sides may reveal, fake, or retain the information uncovered to come to a final decision through natural language.",
        "labels": "MTD"
    },
    {
        "idx": 18,
        "text": "Using this dataset, we leverage transfer learning techniques on a pretrained, end-to-end model and enhance its decision-making ability toward the best choice in terms of utility by means of multi-agent reinforcement learning.",
        "labels": "MTD"
    },
    {
        "idx": 18,
        "text": "An automatic evaluation shows that our approach results in more optimal transactions than human does.",
        "labels": "CLN"
    },
    {
        "idx": 18,
        "text": "We also show that our framework controls the falsehoods generated by seller agents.",
        "labels": "CLN"
    },
    {
        "idx": 19,
        "text": "Existing research studies on cross-sentence relation extraction in long-form multi-party conversations aim to improve relation extraction without considering the explainability of such methods.",
        "labels": "BAC"
    },
    {
        "idx": 19,
        "text": "This work addresses that gap by focusing on extracting explanations that indicate that a relation exists while using only partially labeled explanations.",
        "labels": "PUR"
    },
    {
        "idx": 19,
        "text": "We propose our model-agnostic framework, D-REX, a policy-guided semi-supervised algorithm that optimizes for explanation quality and relation extraction simultaneously.",
        "labels": "MTD"
    },
    {
        "idx": 19,
        "text": "We frame relation extraction as a re-ranking task and include relation- and entity-specific explanations as an intermediate step of the inference process.",
        "labels": "MTD"
    },
    {
        "idx": 19,
        "text": "We find that human annotators are 4.2 times more likely to prefer D-REX{'}s explanations over a joint relation extraction and explanation model.",
        "labels": "RST"
    },
    {
        "idx": 19,
        "text": "Finally, our evaluations show that D-REX is simple yet effective and improves relation extraction performance of strong baseline models by 1.2-4.7{\\%}.",
        "labels": "RST"
    },
    {
        "idx": 21,
        "text": "Existing studies on semantic parsing focus on mapping a natural-language utterance to a logical form (LF) in one turn.",
        "labels": "BAC"
    },
    {
        "idx": 21,
        "text": "However, because natural language may contain ambiguity and variability, this is a difficult challenge.",
        "labels": "GAP"
    },
    {
        "idx": 21,
        "text": "In this work, we investigate an interactive semantic parsing framework that explains the predicted LF step by step in natural language and enables the user to make corrections through natural-language feedback for individual steps.",
        "labels": "PUR"
    },
    {
        "idx": 21,
        "text": "We focus on question answering over knowledge bases (KBQA) as an instantiation of our framework, aiming to increase the transparency of the parsing process and help the user trust the final answer.",
        "labels": "MTD"
    },
    {
        "idx": 21,
        "text": "We construct INSPIRED, a crowdsourced dialogue dataset derived from the ComplexWebQuestions dataset.",
        "labels": "MTD"
    },
    {
        "idx": 21,
        "text": "Our experiments show that this framework has the potential to greatly improve overall parse accuracy.",
        "labels": "RST"
    },
    {
        "idx": 21,
        "text": "Furthermore, we develop a pipeline for dialogue simulation to evaluate our framework w.r.t. a variety of state-of-the-art KBQA models without further crowdsourcing effort.",
        "labels": "MTD"
    },
    {
        "idx": 21,
        "text": "The results demonstrate that our framework promises to be effective across such models.",
        "labels": "CLN"
    },
    {
        "idx": 22,
        "text": "Personalized news recommendation is an essential technique to help users find interested news.",
        "labels": "BAC"
    },
    {
        "idx": 22,
        "text": "Accurately matching user's interests and candidate news is the key to news recommendation.",
        "labels": "BAC"
    },
    {
        "idx": 22,
        "text": "Most existing methods learn a single user embedding from user's historical behaviors to represent the reading interest.",
        "labels": "BAC"
    },
    {
        "idx": 22,
        "text": "However, user interest is usually diverse and may not be adequately modeled by a single user embedding.",
        "labels": "GAP"
    },
    {
        "idx": 22,
        "text": "In this paper, we propose a poly attention scheme to learn multiple interest vectors for each user, which encodes the different aspects of user interest. ",
        "labels": "PUR"
    },
    {
        "idx": 22,
        "text": "We further propose a disagreement regularization to make the learned interests vectors more diverse.",
        "labels": "MTD"
    },
    {
        "idx": 22,
        "text": " Moreover, we design a category-aware attention weighting strategy that incorporates the news category information as explicit interest signals into the attention mechanism.",
        "labels": "MTD"
    },
    {
        "idx": 22,
        "text": "Extensive experiments on the MIND news recommendation benchmark demonstrate that our approach significantly outperforms existing state-of-the-art methods.",
        "labels": "RST"
    },
    {
        "idx": 23,
        "text": "Knowledge-enhanced methods have bridged the gap between human beings and machines in generating dialogue responses.",
        "labels": "BAC"
    },
    {
        "idx": 23,
        "text": "However, most previous works solely seek knowledge from a single source, and thus they often fail to obtain available knowledge because of the insufficient coverage of a single knowledge source.",
        "labels": "GAP"
    },
    {
        "idx": 23,
        "text": "To this end, infusing knowledge from multiple sources becomes a trend.",
        "labels": "BAC"
    },
    {
        "idx": 23,
        "text": "This paper proposes a novel approach Knowledge Source Aware Multi-Head Decoding, KSAM, to infuse multi-source knowledge into dialogue generation more efficiently.",
        "labels": "PUR"
    },
    {
        "idx": 23,
        "text": "Rather than following the traditional single decoder paradigm, KSAM uses multiple independent source-aware decoder heads to alleviate three challenging problems in infusing multi-source knowledge, namely, the diversity among different knowledge sources, the indefinite knowledge alignment issue, and the insufficient flexibility/scalability in knowledge usage.",
        "labels": "MTD"
    },
    {
        "idx": 23,
        "text": "Experiments on a Chinese multi-source knowledge-aligned dataset demonstrate the superior performance of KSAM against various competitive approaches.",
        "labels": "RST"
    },
    {
        "idx": 24,
        "text": " When building NLP models, there is a tendency to aim for broader coverage, often overlooking cultural and (socio)linguistic nuance.",
        "labels": "BAC"
    },
    {
        "idx": 24,
        "text": " In this position paper, we make the case for care and attention to such nuances, particularly in dataset annotation, as well as the inclusion of cultural and linguistic expertise in the process.",
        "labels": "PUR"
    },
    {
        "idx": 24,
        "text": "We present a playbook for responsible dataset creation for polyglossic, multidialectal languages.",
        "labels": "MTD"
    },
    {
        "idx": 24,
        "text": "This work is informed by a study on Arabic annotation of social media content.",
        "labels": "MTD"
    },
    {
        "idx": 25,
        "text": " Hate speech classifiers exhibit substantial performance degradation when evaluated on datasets different from the source.",
        "labels": "BAC"
    },
    {
        "idx": 25,
        "text": "This is due to learning spurious correlations between words that are not necessarily relevant to hateful language, and hate speech labels from the training corpus.",
        "labels": "BAC"
    },
    {
        "idx": 25,
        "text": "Previous work has attempted to mitigate this problem by regularizing specific terms from pre-defined static dictionaries.",
        "labels": "BAC"
    },
    {
        "idx": 25,
        "text": "While this has been demonstrated to improve the generalizability of classifiers, the coverage of such methods is limited and the dictionaries require regular manual updates from human experts.",
        "labels": "GAP"
    },
    {
        "idx": 25,
        "text": "In this paper, we propose to automatically identify and reduce spurious correlations",
        "labels": "PUR"
    },
    {
        "idx": 25,
        "text": "using attribution methods with dynamic refinement of the list of terms that need to be regularized during training.",
        "labels": "MTD"
    },
    {
        "idx": 25,
        "text": "Our approach is flexible and improves the cross-corpora performance over previous work independently and in combination with pre-defined dictionaries.",
        "labels": "CLN"
    },
    {
        "idx": 26,
        "text": "Users interacting with voice assistants today need to phrase their requests in a very specific manner to elicit an appropriate response.",
        "labels": "BAC"
    },
    {
        "idx": 26,
        "text": "This limits the user experience, and is partly due to the lack of reasoning capabilities of dialogue platforms and the hand-crafted rules that require extensive labor.",
        "labels": "GAP"
    },
    {
        "idx": 26,
        "text": "One possible solution to improve user experience and relieve the manual efforts of designers is to build an end-to-end dialogue system that can do reasoning itself while perceiving user's utterances.",
        "labels": "BAC"
    },
    {
        "idx": 26,
        "text": "In this work, we propose a novel method to incorporate the knowledge reasoning capability into dialog systems in a more scalable and generalizable manner.",
        "labels": "PUR"
    },
    {
        "idx": 26,
        "text": "Our proposed method allows a single transformer model to directly walk on a large-scale knowledge graph to generate responses.",
        "labels": "CLN"
    },
    {
        "idx": 26,
        "text": "To the best of our knowledge, this is the first work to have transformer models generate responses by reasoning over differentiable knowledge graphs. ",
        "labels": "CTN"
    },
    {
        "idx": 26,
        "text": "We investigate the reasoning abilities of the proposed method on both task-oriented and domain-specific chit-chat dialogues.",
        "labels": "MTD"
    },
    {
        "idx": 26,
        "text": "Empirical results show that this method can effectively and efficiently incorporate a knowledge graph into a dialogue system with fully-interpretable reasoning paths.",
        "labels": "CLN"
    },
    {
        "idx": 27,
        "text": "Keyphrase extraction (KPE) automatically extracts phrases in a document that provide a concise summary of the core content, which benefits downstream information retrieval and NLP tasks. ",
        "labels": "BAC"
    },
    {
        "idx": 27,
        "text": "Previous state-of-the-art methods select candidate keyphrases based on the similarity between learned representations of the candidates and the document. ",
        "labels": "BAC"
    },
    {
        "idx": 27,
        "text": "They suffer performance degradation on long documents due to discrepancy between sequence lengths which causes mismatch between representations of keyphrase candidates and the document.",
        "labels": "GAP"
    },
    {
        "idx": 27,
        "text": "In this work, we propose a novel unsupervised embedding-based KPE approach, Masked Document Embedding Rank (MDERank), to address this problem",
        "labels": "PUR"
    },
    {
        "idx": 27,
        "text": "by leveraging a mask strategy and ranking candidates by the similarity between embeddings of the source document and the masked document.",
        "labels": "MTD"
    },
    {
        "idx": 27,
        "text": "We further develop a KPE-oriented BERT (KPEBERT) model",
        "labels": "PUR"
    },
    {
        "idx": 27,
        "text": "by proposing a novel self-supervised contrastive learning method, which is more compatible to MDERank than vanilla BERT.",
        "labels": "MTD"
    },
    {
        "idx": 27,
        "text": "Comprehensive evaluations on six KPE benchmarks demonstrate that the proposed MDERank outperforms state-of-the-art unsupervised KPE approach by average 1.80 F1@15 improvement. ",
        "labels": "RST"
    },
    {
        "idx": 27,
        "text": "MDERank further benefits from KPEBERT and overall achieves average 3.53 F1@15 improvement over SIFRank.",
        "labels": "RST"
    },
    {
        "idx": 28,
        "text": " Probing is popular to analyze whether linguistic information can be captured by a well-trained deep neural model,",
        "labels": "BAC"
    },
    {
        "idx": 28,
        "text": "but it is hard to answer how the change of the encoded linguistic information will affect task performance.",
        "labels": "GAP"
    },
    {
        "idx": 28,
        "text": "To this end, we study the dynamic relationship between the encoded linguistic information and task performance from the viewpoint of Pareto Optimality.",
        "labels": "PUR"
    },
    {
        "idx": 28,
        "text": "Its key idea is to obtain a set of models which are Pareto-optimal in terms of both objectives.",
        "labels": "MTD"
    },
    {
        "idx": 28,
        "text": "From this viewpoint, we propose a method to optimize the Pareto-optimal models by formalizing it as a multi-objective optimization problem.",
        "labels": "MTD"
    },
    {
        "idx": 28,
        "text": "We conduct experiments on two popular NLP tasks, i.e., machine translation and language modeling, and investigate the relationship between several kinds of linguistic information and task performances. ",
        "labels": "MTD"
    },
    {
        "idx": 28,
        "text": "Experimental results demonstrate that the proposed method is better than a baseline method.",
        "labels": "RST"
    },
    {
        "idx": 28,
        "text": "Our empirical findings suggest that some syntactic information is helpful for NLP tasks whereas encoding more syntactic information does not necessarily lead to better performance, because the model architecture is also an important factor.",
        "labels": "CLN"
    },
    {
        "idx": 29,
        "text": "The automation of extracting argument structures faces a pair of challenges on (1) encoding long-term contexts to facilitate comprehensive understanding, and (2) improving data efficiency since constructing high-quality argument structures is time-consuming.",
        "labels": "GAP"
    },
    {
        "idx": 29,
        "text": "In this work, we propose a novel context-aware Transformer-based argument structure prediction model which, on five different domains, significantly outperforms models that rely on features or only encode limited contexts.",
        "labels": "PUR"
    },
    {
        "idx": 29,
        "text": "To tackle the difficulty of data annotation, we examine two complementary methods: (i) transfer learning to leverage existing annotated data to boost model performance in a new target domain, and (ii) active learning to strategically identify a small amount of samples for annotation.",
        "labels": "MTD"
    },
    {
        "idx": 29,
        "text": "We further propose model-independent sample acquisition strategies, which can be generalized to diverse domains.",
        "labels": "MTD"
    },
    {
        "idx": 29,
        "text": "With extensive experiments, we show that our simple-yet-effective acquisition strategies yield competitive results against three strong comparisons.",
        "labels": "RST"
    },
    {
        "idx": 29,
        "text": "Combined with transfer learning, substantial F1 score boost (5-25) can be further achieved during the early iterations of active learning across domains.",
        "labels": "RST"
    },
    {
        "idx": 30,
        "text": "Language models (LMs) have shown great potential as implicit knowledge bases (KBs).",
        "labels": "BAC"
    },
    {
        "idx": 30,
        "text": "And for their practical use, knowledge in LMs need to be updated periodically. ",
        "labels": "BAC"
    },
    {
        "idx": 30,
        "text": "However, existing tasks to assess LMs' efficacy as KBs do not adequately consider multiple large-scale updates. ",
        "labels": "GAP"
    },
    {
        "idx": 30,
        "text": "To this end, we first propose a novel task-Continuously-updated QA (CuQA)",
        "labels": "PUR"
    },
    {
        "idx": 30,
        "text": "in which multiple large-scale updates are made to LMs,",
        "labels": "MTD"
    },
    {
        "idx": 30,
        "text": "and the performance is measured with respect to the success in adding and updating knowledge while retaining existing knowledge.",
        "labels": "MTD"
    },
    {
        "idx": 30,
        "text": "We then present LMs with plug-in modules that effectively handle the updates.",
        "labels": "MTD"
    },
    {
        "idx": 30,
        "text": "Experiments conducted on zsRE QA and NQ datasets show that our method outperforms existing approaches.",
        "labels": "CLN"
    },
    {
        "idx": 30,
        "text": "We find that our method is 4x more effective in terms of updates/forgets ratio, compared to a fine-tuning baseline.",
        "labels": "RST"
    },
    {
        "idx": 31,
        "text": "Medical images are widely used in clinical decision-making, where writing radiology reports is a potential application that can be enhanced by automatic solutions to alleviate physicians' workload.",
        "labels": "BAC"
    },
    {
        "idx": 31,
        "text": "In general, radiology report generation is an image-text task, where cross-modal mappings between images and texts play an important role in generating high-quality reports.",
        "labels": "BAC"
    },
    {
        "idx": 31,
        "text": "Although previous studies attempt to facilitate the alignment via the co-attention mechanism under supervised settings, they suffer from lacking valid and accurate correspondences due to no annotation of such alignment. ",
        "labels": "GAP"
    },
    {
        "idx": 31,
        "text": "In this paper, we propose an approach with reinforcement learning (RL) over a cross-modal memory (CMM) to better align visual and textual features for radiology report generation.",
        "labels": "PUR"
    },
    {
        "idx": 31,
        "text": "In detail, a shared memory is used to record the mappings between visual and textual information,",
        "labels": "MTD"
    },
    {
        "idx": 31,
        "text": "and the proposed reinforced algorithm is performed to learn the signal from the reports to guide the cross-modal alignment even though such reports are not directly related to how images and texts are mapped.",
        "labels": "MTD"
    },
    {
        "idx": 31,
        "text": "Experimental results on two English radiology report datasets, i.e., IU X-Ray and MIMIC-CXR, show the effectiveness of our approach, where the state-of-the-art results are achieved.",
        "labels": "RST"
    },
    {
        "idx": 31,
        "text": "We further conduct human evaluation and case study",
        "labels": "MTD"
    },
    {
        "idx": 31,
        "text": "which confirm the validity of the reinforced algorithm in our approach.",
        "labels": "CLN"
    },
    {
        "idx": 32,
        "text": "Deep learning has demonstrated performance advantages in a wide range of natural language processing tasks, including neural machine translation (NMT).",
        "labels": "BAC"
    },
    {
        "idx": 32,
        "text": "Transformer NMT models are typically strengthened by deeper encoder layers,",
        "labels": "BAC"
    },
    {
        "idx": 32,
        "text": "but deepening their decoder layers usually results in failure.",
        "labels": "GAP"
    },
    {
        "idx": 32,
        "text": "In this paper, we first identify the cause of the failure of the deep decoder in the Transformer model.",
        "labels": "PUR"
    },
    {
        "idx": 32,
        "text": "Inspired by this discovery, we then propose approaches to improving it, with respect to model structure and model training, to make the deep decoder practical in NMT. ",
        "labels": "PUR"
    },
    {
        "idx": 32,
        "text": "Specifically, with respect to model structure, we propose a cross-attention drop mechanism to allow the decoder layers to perform their own different roles, to reduce the difficulty of deep-decoder learning.",
        "labels": "MTD"
    },
    {
        "idx": 32,
        "text": "For model training, we propose a collapse reducing training approach to improve the stability and effectiveness of deep-decoder training. ",
        "labels": "MTD"
    },
    {
        "idx": 32,
        "text": "We experimentally evaluated our proposed Transformer NMT model structure modification and novel training methods on several popular machine translation benchmarks. ",
        "labels": "MTD"
    },
    {
        "idx": 32,
        "text": "The results showed that deepening the NMT model by increasing the number of decoder layers successfully prevented the deepened decoder from degrading to an unconditional language model.",
        "labels": "CLN"
    },
    {
        "idx": 32,
        "text": "In contrast to prior work on deepening an NMT model on the encoder, our method can deepen the model on both the encoder and decoder at the same time, resulting in a deeper model and improved performance.",
        "labels": "CTN"
    },
    {
        "idx": 33,
        "text": "Code mixing is the linguistic phenomenon where bilingual speakers tend to switch between two or more languages in conversations.",
        "labels": "BAC"
    },
    {
        "idx": 33,
        "text": "Recent work on code-mixing in computational settings has leveraged social media code mixed texts to train NLP models.",
        "labels": "BAC"
    },
    {
        "idx": 33,
        "text": "For capturing the variety of code mixing in, and across corpus, Language ID (LID) tags based measures (CMI) have been proposed.",
        "labels": "BAC"
    },
    {
        "idx": 33,
        "text": "Syntactical variety/patterns of code-mixing and their relationship vis-a-vis computational model's performance is under explored.",
        "labels": "BAC"
    },
    {
        "idx": 33,
        "text": "In this work, we investigate a collection of English(en)-Hindi(hi) code-mixed datasets from a syntactic lens to propose, SyMCoM, an indicator of syntactic variety in code-mixed text, with intuitive theoretical bounds.",
        "labels": "PUR"
    },
    {
        "idx": 33,
        "text": "We train SoTA en-hi PoS tagger, accuracy of 93.4%, to reliably compute PoS tags on a corpus, ",
        "labels": "MTD"
    },
    {
        "idx": 33,
        "text": "and demonstrate the utility of SyMCoM by applying it on various syntactical categories on a collection of datasets,",
        "labels": "MTD"
    },
    {
        "idx": 33,
        "text": "and compare datasets using the measure.",
        "labels": "MTD"
    },
    {
        "idx": 34,
        "text": "A pressing challenge in current dialogue systems is to successfully converse with users on topics with information distributed across different modalities.",
        "labels": "BAC"
    },
    {
        "idx": 34,
        "text": "Previous work in multiturn dialogue systems has primarily focused on either text or table information.",
        "labels": "BAC"
    },
    {
        "idx": 34,
        "text": "In more realistic scenarios, having a joint understanding of both is critical as knowledge is typically distributed over both unstructured and structured forms.",
        "labels": "BAC"
    },
    {
        "idx": 34,
        "text": "We present a new dialogue dataset, HybriDialogue, which consists of crowdsourced natural conversations grounded on both Wikipedia text and tables.",
        "labels": "PUR"
    },
    {
        "idx": 34,
        "text": "The conversations are created through the decomposition of complex multihop questions into simple, realistic multiturn dialogue interactions.",
        "labels": "MTD"
    },
    {
        "idx": 34,
        "text": "We propose retrieval, system state tracking, and dialogue response generation tasks for our dataset and conduct baseline experiments for each. ",
        "labels": "MTD"
    },
    {
        "idx": 34,
        "text": "Our results show that there is still ample opportunity for improvement,",
        "labels": "RST"
    },
    {
        "idx": 34,
        "text": "demonstrating the importance of building stronger dialogue systems that can reason over the complex setting of informationseeking dialogue grounded on tables and text.",
        "labels": "CLN"
    },
    {
        "idx": 35,
        "text": " Text summarization models are approaching human levels of fidelity.",
        "labels": "BAC"
    },
    {
        "idx": 35,
        "text": "Existing benchmarking corpora provide concordant pairs of full and abridged versions of Web, news or professional content.",
        "labels": "BAC"
    },
    {
        "idx": 35,
        "text": "To date, all summarization datasets operate under a one-size-fits-all paradigm that may not reflect the full range of organic summarization needs.",
        "labels": "BAC"
    },
    {
        "idx": 35,
        "text": "Several recently proposed models (e.g., plug and play language models) have the capacity to condition the generated summaries on a desired range of themes.",
        "labels": "BAC"
    },
    {
        "idx": 35,
        "text": "These capacities remain largely unused and unevaluated as there is no dedicated dataset that would support the task of topic-focused summarization.",
        "labels": "GAP"
    },
    {
        "idx": 35,
        "text": "This paper introduces the first topical summarization corpus NEWTS, ",
        "labels": "PUR"
    },
    {
        "idx": 35,
        "text": "based on the well-known CNN/Dailymail dataset, and annotated via online crowd-sourcing.",
        "labels": "MTD"
    },
    {
        "idx": 35,
        "text": "Each source article is paired with two reference summaries, each focusing on a different theme of the source document.",
        "labels": "MTD"
    },
    {
        "idx": 35,
        "text": "We evaluate a representative range of existing techniques and analyze the effectiveness of different prompting methods.",
        "labels": "PUR"
    },
    {
        "idx": 36,
        "text": " Reddit is home to a broad spectrum of political activity,",
        "labels": "BAC"
    },
    {
        "idx": 36,
        "text": "and users signal their political affiliations in multiple ways-from self-declarations to community participation.",
        "labels": "BAC"
    },
    {
        "idx": 36,
        "text": "Frequently, computational studies have treated political users as a single bloc, both in developing models to infer political leaning and in studying political behavior. ",
        "labels": "BAC"
    },
    {
        "idx": 36,
        "text": "Here, we test this assumption of political users",
        "labels": "PUR"
    },
    {
        "idx": 36,
        "text": "and show that commonly-used political-inference models do not generalize,",
        "labels": "RST"
    },
    {
        "idx": 36,
        "text": "indicating heterogeneous types of political users.",
        "labels": "CLN"
    },
    {
        "idx": 36,
        "text": "The models remain imprecise at best for most users, regardless of which sources of data or methods are used.",
        "labels": "CLN"
    },
    {
        "idx": 36,
        "text": "Across a 14-year longitudinal analysis,",
        "labels": "MTD"
    },
    {
        "idx": 36,
        "text": "we demonstrate that the choice in definition of a political user has significant implications for behavioral analysis.",
        "labels": "CLN"
    },
    {
        "idx": 36,
        "text": "Controlling for multiple factors, political users are more toxic on the platform and inter-party interactions are even more toxic-but not all political users behave this way.",
        "labels": "CLN"
    },
    {
        "idx": 36,
        "text": "Last, we identify a subset of political users who repeatedly flip affiliations, ",
        "labels": "MTD"
    },
    {
        "idx": 36,
        "text": "showing that these users are the most controversial of all, acting as provocateurs by more frequently bringing up politics, and are more likely to be banned, suspended, or deleted.",
        "labels": "CLN"
    },
    {
        "idx": 37,
        "text": "In this position paper, we describe our perspective on how meaningful resources for lower-resourced languages should be developed in connection with the speakers of those languages.",
        "labels": "PUR"
    },
    {
        "idx": 37,
        "text": " Before advancing that position, we first examine two massively multilingual resources used in language technology development,",
        "labels": "MTD"
    },
    {
        "idx": 37,
        "text": "identifying shortcomings that limit their usefulness.",
        "labels": "RST"
    },
    {
        "idx": 37,
        "text": "We explore the contents of the names stored in Wikidata for a few lower-resourced languages",
        "labels": "MTD"
    },
    {
        "idx": 37,
        "text": "and find that many of them are not in fact in the languages they claim to be, requiring non-trivial effort to correct.",
        "labels": "RST"
    },
    {
        "idx": 37,
        "text": "We discuss quality issues present in WikiAnn and evaluate whether it is a useful supplement to hand-annotated data. ",
        "labels": "MTD"
    },
    {
        "idx": 37,
        "text": "We then discuss the importance of creating annotations for lower-resourced languages in a thoughtful and ethical way that includes the language speakers as part of the development process.",
        "labels": "MTD"
    },
    {
        "idx": 37,
        "text": "We conclude with recommended guidelines for resource development.",
        "labels": "CTN"
    },
    {
        "idx": 38,
        "text": "  Quality Estimation (QE) models have the potential to change how we evaluate and maybe even train machine translation models.",
        "labels": "BAC"
    },
    {
        "idx": 38,
        "text": "However, these models still lack the robustness to achieve general adoption.",
        "labels": "GAP"
    },
    {
        "idx": 38,
        "text": "We show that Stateof-the-art QE models, when tested in a Parallel Corpus Mining (PCM) setting, perform unexpectedly bad due to a lack of robustness to out-of-domain examples.",
        "labels": "GAP"
    },
    {
        "idx": 38,
        "text": "We propose a combination of multitask training, data augmentation and contrastive learning to achieve better and more robust QE performance.",
        "labels": "PUR"
    },
    {
        "idx": 38,
        "text": "We show that our method improves QE performance significantly in the MLQE challenge and the robustness of QE models when tested in the Parallel Corpus Mining setup.",
        "labels": "RST"
    },
    {
        "idx": 38,
        "text": "We increase the accuracy in PCM by more than 0.80, making it on par with state-of-the-art PCM methods that use millions of sentence pairs to train their models",
        "labels": "RST"
    },
    {
        "idx": 38,
        "text": " In comparison, we use a thousand times less data, 7K parallel sentences in total, and propose a novel low resource PCM method.",
        "labels": "CTN"
    },
    {
        "idx": 39,
        "text": "In this paper, we bring a new way of digesting news content",
        "labels": "PUR"
    },
    {
        "idx": 39,
        "text": "by introducing the task of segmenting a news article into multiple sections and generating the corresponding summary to each section.",
        "labels": "MTD"
    },
    {
        "idx": 39,
        "text": "We make two contributions towards this new task.",
        "labels": "MTD"
    },
    {
        "idx": 39,
        "text": "First, we create and make available a dataset, SegNews, consisting of 27k news articles with sections and aligned heading-style section summaries.",
        "labels": "MTD"
    },
    {
        "idx": 39,
        "text": "Second, we propose a novel segmentation-based language generation model adapted from pre-trained language models that can jointly segment a document and produce the summary for each section.",
        "labels": "MTD"
    },
    {
        "idx": 39,
        "text": " Experimental results on SegNews demonstrate that our model can outperform several state-of-the-art sequence-to-sequence generation models for this new task.",
        "labels": "RST"
    },
    {
        "idx": 40,
        "text": "Though nearest neighbor Machine Translation (kNN-MT) (CITATION) has proved to introduce significant performance boosts over standard neural MT systems, it is prohibitively slow since it uses the entire reference corpus as the datastore for the nearest neighbor search.",
        "labels": "GAP"
    },
    {
        "idx": 40,
        "text": "This means each step for each beam in the beam search has to search over the entire reference corpus.",
        "labels": "GAP"
    },
    {
        "idx": 40,
        "text": "kNN-MT is thus two-orders slower than vanilla MT models, making it hard to be applied to real-world applications, especially online services.",
        "labels": "GAP"
    },
    {
        "idx": 40,
        "text": "In this work, we propose Fast kNN-MT to address this issue.",
        "labels": "PUR"
    },
    {
        "idx": 40,
        "text": "Fast kNN-MT constructs a significantly smaller datastore for the nearest neighbor search:",
        "labels": "MTD"
    },
    {
        "idx": 40,
        "text": "for each word in a source sentence, Fast kNN-MT first selects its nearest token-level neighbors, which is limited to tokens that are the same as the query token.",
        "labels": "MTD"
    },
    {
        "idx": 40,
        "text": "Then at each decoding step, in contrast to using the entire corpus as the datastore, the search space is limited to target tokens corresponding to the previously selected reference source tokens.",
        "labels": "MTD"
    },
    {
        "idx": 40,
        "text": "This strategy avoids search through the whole datastore for nearest neighbors and drastically improves decoding efficiency.",
        "labels": "MTD"
    },
    {
        "idx": 40,
        "text": "Without loss of performance, Fast kNN-MT is two-orders faster than kNN-MT, and is only two times slower than the standard NMT model.",
        "labels": "RST"
    },
    {
        "idx": 40,
        "text": "Fast kNN-MT enables the practical use of kNN-MT systems in real-world MT applications.",
        "labels": "CLN"
    },
    {
        "idx": 40,
        "text": "The code is available at url{https://github.com/ShannonAI/fast-knn-nmt.",
        "labels": "CTN"
    },
    {
        "idx": 41,
        "text": "Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective.",
        "labels": "BAC"
    },
    {
        "idx": 41,
        "text": "We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model.",
        "labels": "BAC"
    },
    {
        "idx": 41,
        "text": "Accordingly, we explore a different approach altogether:",
        "labels": "PUR"
    },
    {
        "idx": 41,
        "text": "extracting latent vectors directly from pretrained language model decoders without fine-tuning.",
        "labels": "MTD"
    },
    {
        "idx": 41,
        "text": "Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (greater 99 BLEU) for English sentences from a variety of domains.",
        "labels": "RST"
    },
    {
        "idx": 41,
        "text": "We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task.",
        "labels": "RST"
    },
    {
        "idx": 41,
        "text": "We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B),",
        "labels": "RST"
    },
    {
        "idx": 41,
        "text": "outperforming pooled hidden states of models.",
        "labels": "RST"
    },
    {
        "idx": 41,
        "text": "Finally, we present an analysis of the intrinsic properties of the steering vectors.",
        "labels": "MTD"
    },
    {
        "idx": 41,
        "text": "Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.",
        "labels": "CLN"
    },
    {
        "idx": 42,
        "text": "Generalising to unseen domains is under-explored and remains a challenge in neural machine translation.",
        "labels": "BAC"
    },
    {
        "idx": 42,
        "text": "Inspired by recent research in parameter-efficient transfer learning from pretrained models, this paper proposes a fusion-based generalisation method that learns to combine domain-specific parameters.",
        "labels": "PUR"
    },
    {
        "idx": 42,
        "text": "We propose a leave-one-domain-out training strategy to avoid information leaking to address the challenge of not knowing the test domain during training time. ",
        "labels": "MTD"
    },
    {
        "idx": 42,
        "text": "Empirical results on three language pairs show that our proposed fusion method outperforms other baselines up to +0.8 BLEU score on average.",
        "labels": "RST"
    },
    {
        "idx": 43,
        "text": " What kinds of instructional prompts are easier to follow for Language Models (LMs)?",
        "labels": "BAC"
    },
    {
        "idx": 43,
        "text": "We study this question by conducting extensive empirical analysis that shed light on important features of successful instructional prompts.",
        "labels": "PUR"
    },
    {
        "idx": 43,
        "text": "Specifically, we study several classes of reframing techniques for manual reformulation of prompts into more effective ones.",
        "labels": "MTD"
    },
    {
        "idx": 43,
        "text": "Some examples include decomposing a complex task instruction into multiple simpler tasks or itemizing instructions into sequential steps. ",
        "labels": "MTD"
    },
    {
        "idx": 43,
        "text": "Our experiments compare the zero-shot and few-shot performance of LMs prompted with reframed instructions on 12 NLP tasks across 6 categories.",
        "labels": "MTD"
    },
    {
        "idx": 43,
        "text": "Compared with original instructions, our reframed instructions lead to significant improvements across LMs with different sizes.",
        "labels": "CLN"
    },
    {
        "idx": 43,
        "text": "For example, the same reframed prompts boost few-shot performance of GPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over all tasks.",
        "labels": "RST"
    },
    {
        "idx": 43,
        "text": "Furthermore, reframed instructions reduce the number of examples required to prompt LMs in the few-shot setting.",
        "labels": "CLN"
    },
    {
        "idx": 43,
        "text": "We hope these empirically-driven techniques will pave the way towards more effective future prompting algorithms.",
        "labels": "IMP"
    },
    {
        "idx": 44,
        "text": " A common method for extractive multi-document news summarization is to re-formulate it as a single-document summarization problem by concatenating all documents as a single meta-document. ",
        "labels": "BAC"
    },
    {
        "idx": 44,
        "text": "However, this method neglects the relative importance of documents.",
        "labels": "GAP"
    },
    {
        "idx": 44,
        "text": " We propose a simple approach to reorder the documents according to their relative importance before concatenating and summarizing them. ",
        "labels": "PUR"
    },
    {
        "idx": 44,
        "text": "The reordering makes the salient content easier to learn by the summarization model.",
        "labels": "MTD"
    },
    {
        "idx": 44,
        "text": "Experiments show that our approach outperforms previous state-of-the-art methods with more complex architectures.",
        "labels": "RST"
    },
    {
        "idx": 45,
        "text": " Natural language is generated by people, ",
        "labels": "BAC"
    },
    {
        "idx": 45,
        "text": "yet traditional language modeling views words or documents as if generated independently. ",
        "labels": "GAP"
    },
    {
        "idx": 45,
        "text": "Here, we propose human language modeling (HuLM), a hierarchical extension to the language modeling problem where by a human- level exists to connect sequences of documents (e.g. social media messages) and capture the notion that human language is moderated by changing human states. ",
        "labels": "PUR"
    },
    {
        "idx": 45,
        "text": "We introduce, HaRT, a large-scale transformer model for solving HuLM, pre-trained on approximately 100,000 social media users, and demonstrate it's effectiveness in terms of both language modeling (perplexity) for social media and fine-tuning for 4 downstream tasks spanning document- and user-levels.",
        "labels": "MTD"
    },
    {
        "idx": 45,
        "text": "Results on all tasks meet or surpass the current state-of-the-art.",
        "labels": "RST"
    },
    {
        "idx": 46,
        "text": "  Prompting methods recently achieve impressive success in few-shot learning.",
        "labels": "BAC"
    },
    {
        "idx": 46,
        "text": "These methods modify input samples with prompt sentence pieces, and decode label tokens to map samples to corresponding labels.",
        "labels": "BAC"
    },
    {
        "idx": 46,
        "text": "However, such a paradigm is very inefficient for the task of slot tagging.",
        "labels": "GAP"
    },
    {
        "idx": 46,
        "text": "Since slot tagging samples are multiple consecutive words in a sentence, the prompting methods have to enumerate all n-grams token spans to find all the possible slots, which greatly slows down the prediction.",
        "labels": "GAP"
    },
    {
        "idx": 46,
        "text": "To tackle this, we introduce an inverse paradigm for prompting.",
        "labels": "PUR"
    },
    {
        "idx": 46,
        "text": "Different from the classic prompts mapping tokens to labels, we reversely predict slot values given slot types.",
        "labels": "MTD"
    },
    {
        "idx": 46,
        "text": "Such inverse prompting only requires a one-turn prediction for each slot type and greatly speeds up the prediction.",
        "labels": "MTD"
    },
    {
        "idx": 46,
        "text": "Besides, we propose a novel Iterative Prediction Strategy, ",
        "labels": "PUR"
    },
    {
        "idx": 46,
        "text": "from which the model learns to refine predictions by considering the relations between different slot types.",
        "labels": "MTD"
    },
    {
        "idx": 46,
        "text": "We find, somewhat surprisingly, the proposed method not only predicts faster but also significantly improves the effect (improve over 6.1 F1-scores on 10-shot setting) and achieves new state-of-the-art performance.",
        "labels": "RST"
    },
    {
        "idx": 47,
        "text": " Decoding language from non-invasive brain activity has attracted increasing attention from both researchers in neuroscience and natural language processing.",
        "labels": "BAC"
    },
    {
        "idx": 47,
        "text": "Due to the noisy nature of brain recordings, existing work has simplified brain-to-word decoding as a binary classification task which is to discriminate a brain signal between its corresponding word and a wrong one.",
        "labels": "BAC"
    },
    {
        "idx": 47,
        "text": "This pairwise classification task, however, cannot promote the development of practical neural decoders for two reasons.",
        "labels": "GAP"
    },
    {
        "idx": 47,
        "text": "First, it has to enumerate all pairwise combinations in the test set, so it is inefficient to predict a word in a large vocabulary.",
        "labels": "GAP"
    },
    {
        "idx": 47,
        "text": "Second, a perfect pairwise decoder cannot guarantee the performance on direct classification.",
        "labels": "GAP"
    },
    {
        "idx": 47,
        "text": "To overcome these and go a step further to a realistic neural decoder, we propose a novel Cross-Modal Cloze (CMC) task",
        "labels": "PUR"
    },
    {
        "idx": 47,
        "text": "which is to predict the target word encoded in the neural image with a context as prompt. ",
        "labels": "MTD"
    },
    {
        "idx": 47,
        "text": "Furthermore, to address this task, we propose a general approach",
        "labels": "PUR"
    },
    {
        "idx": 47,
        "text": "that leverages the pre-trained language model to predict the target word.",
        "labels": "MTD"
    },
    {
        "idx": 47,
        "text": "To validate our method, we perform experiments on more than 20 participants from two brain imaging datasets.",
        "labels": "MTD"
    },
    {
        "idx": 47,
        "text": "Our method achieves 28.91% top-1 accuracy and 54.19% top-5 accuracy on average across all participants, significantly outperforming several baselines. ",
        "labels": "RST"
    },
    {
        "idx": 47,
        "text": "This result indicates that our model can serve as a state-of-the-art baseline for the CMC task.",
        "labels": "RST"
    },
    {
        "idx": 47,
        "text": "More importantly, it demonstrates that it is feasible to decode a certain word within a large vocabulary from its neural brain activity.",
        "labels": "CLN"
    },
    {
        "idx": 48,
        "text": "Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings.",
        "labels": "BAC"
    },
    {
        "idx": 48,
        "text": "However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions.",
        "labels": "GAP"
    },
    {
        "idx": 48,
        "text": "Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model's biases onto the distilled model.",
        "labels": "GAP"
    },
    {
        "idx": 48,
        "text": "To this end, we present a novel approach to mitigate gender disparity in text generation",
        "labels": "PUR"
    },
    {
        "idx": 48,
        "text": "by learning a fair model during knowledge distillation.",
        "labels": "MTD"
    },
    {
        "idx": 48,
        "text": " We propose two modifications to the base knowledge distillation based on counterfactual role reversal-modifying teacher probabilities and augmenting the training set.",
        "labels": "MTD"
    },
    {
        "idx": 48,
        "text": "We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT-2 models",
        "labels": "MTD"
    },
    {
        "idx": 48,
        "text": "and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility.",
        "labels": "CLN"
    },
    {
        "idx": 48,
        "text": "Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.",
        "labels": "CLN"
    },
    {
        "idx": 49,
        "text": "We propose a probabilistic approach to select a subset of a target domain representative keywords from a candidate set, contrasting with a context domain. ",
        "labels": "PUR"
    },
    {
        "idx": 49,
        "text": "Such a task is crucial for many downstream tasks in natural language processing. ",
        "labels": "CTN"
    },
    {
        "idx": 49,
        "text": "To contrast the target domain and the context domain, we adapt the two-component mixture model concept to generate a distribution of candidate keywords. ",
        "labels": "MTD"
    },
    {
        "idx": 49,
        "text": "It provides more importance to the distinctive keywords of the target domain than common keywords contrasting with the context domain. ",
        "labels": "MTD"
    },
    {
        "idx": 49,
        "text": "To support the representativeness of the selected keywords towards the target domain, we introduce an optimization algorithm for selecting the subset from the generated candidate distribution. ",
        "labels": "MTD"
    },
    {
        "idx": 49,
        "text": "We have shown that the optimization algorithm can be efficiently implemented with a near-optimal approximation guarantee.",
        "labels": "RST"
    },
    {
        "idx": 49,
        "text": "Finally, extensive experiments on multiple domains demonstrate the superiority of our approach over other baselines for the tasks of keyword summary generation and trending keywords selection.",
        "labels": "CLN"
    },
    {
        "idx": 50,
        "text": "Pre-trained models have achieved excellent performance on the dialogue task.",
        "labels": "BAC"
    },
    {
        "idx": 50,
        "text": "However, for the continual increase of online chit-chat scenarios, directly fine-tuning these models for each of the new tasks not only explodes the capacity of the dialogue system on the embedded devices but also causes knowledge forgetting on pre-trained models and knowledge interference among diverse dialogue tasks.",
        "labels": "GAP"
    },
    {
        "idx": 50,
        "text": "In this work, we propose a hierarchical inductive transfer framework to learn and deploy the dialogue skills continually and efficiently.",
        "labels": "PUR"
    },
    {
        "idx": 50,
        "text": "First, we introduce the adapter module into pre-trained models for learning new dialogue tasks.",
        "labels": "MTD"
    },
    {
        "idx": 50,
        "text": "As the only trainable module, it is beneficial for the dialogue system on the embedded devices to acquire new dialogue skills with negligible additional parameters.",
        "labels": "MTD"
    },
    {
        "idx": 50,
        "text": "Then, for alleviating knowledge interference between tasks yet benefiting the regularization between them, we further design hierarchical inductive transfer that enables new tasks to use general knowledge in the base adapter without being misled by diverse knowledge in task-specific adapters.",
        "labels": "MTD"
    },
    {
        "idx": 50,
        "text": "Empirical evaluation and analysis indicate that our framework obtains comparable performance under deployment-friendly model capacity",
        "labels": "RST"
    },
    {
        "idx": 51,
        "text": "Understanding causal narratives communicated in clinical notes can help make strides towards personalized healthcare. ",
        "labels": "BAC"
    },
    {
        "idx": 51,
        "text": "Extracted causal information from clinical notes can be combined with structured EHR data such as patients' demographics, diagnoses, and medications. ",
        "labels": "BAC"
    },
    {
        "idx": 51,
        "text": "This will enhance healthcare providers' ability to identify aspects of a patient's story communicated in the clinical notes and help make more informed decisions.",
        "labels": "BAC"
    },
    {
        "idx": 51,
        "text": "n this work, we propose annotation guidelines, develop an annotated corpus and provide baseline scores to identify types and direction of causal relations between a pair of biomedical concepts in clinical notes; communicated implicitly or explicitly, identified either in a single sentence or across multiple sentences",
        "labels": "PUR"
    },
    {
        "idx": 51,
        "text": "We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2 shared task dataset and train four different language model based architectures.",
        "labels": "MTD"
    },
    {
        "idx": 51,
        "text": "Annotation based on our guidelines achieved a high inter-annotator agreement i.e. Fleiss' kappa (kappa) score of 0.72, and our model for identification of causal relations achieved a macro F1 score of 0.56 on the test data.",
        "labels": "RST"
    },
    {
        "idx": 51,
        "text": "The high inter-annotator agreement for clinical text shows the quality of our annotation guidelines while the provided baseline F1 score sets the direction for future research towards understanding narratives in clinical texts.",
        "labels": "CLN"
    },
    {
        "idx": 52,
        "text": "How to learn highly compact yet effective sentence representation?",
        "labels": "BAC"
    },
    {
        "idx": 52,
        "text": "Pre-trained language models have been effective in many NLP tasks.",
        "labels": "BAC"
    },
    {
        "idx": 52,
        "text": "However, these models are often huge and produce large sentence embeddings.",
        "labels": "GAP"
    },
    {
        "idx": 52,
        "text": "Moreover, there is a big performance gap between large and small models.",
        "labels": "GAP"
    },
    {
        "idx": 52,
        "text": "In this paper, we propose Homomorphic Projective Distillation (HPD) to learn compressed sentence embeddings.",
        "labels": "PUR"
    },
    {
        "idx": 52,
        "text": "Our method augments a small Transformer encoder model with learnable projection layers to produce compact representations while mimicking a large pre-trained language model to retain the sentence representation quality.",
        "labels": "MTD"
    },
    {
        "idx": 52,
        "text": "We evaluate our method with different model sizes on both semantic textual similarity (STS) and semantic retrieval (SR) tasks.",
        "labels": "MTD"
    },
    {
        "idx": 52,
        "text": "Experiments show that our method achieves 2.7-4.5 points performance gain on STS tasks compared with previous best representations of the same size.",
        "labels": "RST"
    },
    {
        "idx": 52,
        "text": "In SR tasks, our method improves retrieval speed (8.2{mbox{times) and memory usage (8.0{mbox{times) compared with state-of-the-art large models.",
        "labels": "RST"
    },
    {
        "idx": 52,
        "text": "Our implementation is available at url{https://github.com/XuandongZhao/HPD.",
        "labels": "CTN"
    },
    {
        "idx": 53,
        "text": "We study event understanding as a critical step towards visual commonsense tasks",
        "labels": "PUR"
    },
    {
        "idx": 53,
        "text": "Meanwhile, we argue that current object-based event understanding is purely likelihood-based, leading to incorrect event prediction, due to biased correlation between events and objects",
        "labels": "PUR"
    },
    {
        "idx": 53,
        "text": "We propose to mitigate such biases with do-calculus, proposed in causality research, but overcoming its limited robustness, by an optimized aggregation with association-based prediction",
        "labels": "MTD"
    },
    {
        "idx": 53,
        "text": "We show the effectiveness of our approach, intrinsically by comparing our generated events with ground-truth event annotation, and extrinsically by downstream commonsense tasks.",
        "labels": "MTD"
    },
    {
        "idx": 54,
        "text": "Current Question Answering over Knowledge Graphs (KGQA) task mainly focuses on performing answer reasoning upon KGs with binary facts.",
        "labels": "BAC"
    },
    {
        "idx": 54,
        "text": "However, it neglects the n-ary facts, which contain more than two entities.",
        "labels": "GAP"
    },
    {
        "idx": 54,
        "text": "In this work, we highlight a more challenging but under-explored task: n-ary KGQA, i.e., answering n-ary facts questions upon n-ary KGs. Nevertheless, the multi-hop reasoning framework popular in binary KGQA task is not directly applicable on n-ary KGQA.",
        "labels": "PUR"
    },
    {
        "idx": 54,
        "text": "We propose two feasible improvements: 1) upgrade the basic reasoning unit from entity or relation to fact, and 2) upgrade the reasoning structure from chain to tree.",
        "labels": "MTD"
    },
    {
        "idx": 54,
        "text": "Therefore, we propose a novel fact-tree reasoning framework, FacTree, which integrates the above two upgrades.",
        "labels": "MTD"
    },
    {
        "idx": 54,
        "text": " FacTree transforms the question into a fact tree and performs iterative fact reasoning on the fact tree to infer the correct answer.",
        "labels": "MTD"
    },
    {
        "idx": 54,
        "text": "Experimental results on the n-ary KGQA dataset we constructed and two binary KGQA benchmarks demonstrate the effectiveness of FacTree compared with state-of-the-art methods.",
        "labels": "RST"
    },
    {
        "idx": 55,
        "text": "We introduce a method for improving the structural understanding abilities of language models.",
        "labels": "PUR"
    },
    {
        "idx": 55,
        "text": "Unlike previous approaches that finetune the models with task-specific augmentation,",
        "labels": "BAC"
    },
    {
        "idx": 55,
        "text": "we pretrain language models to generate structures from the text on a collection of task-agnostic corpora.",
        "labels": "MTD"
    },
    {
        "idx": 55,
        "text": "Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks.",
        "labels": "MTD"
    },
    {
        "idx": 55,
        "text": "We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking.",
        "labels": "MTD"
    },
    {
        "idx": 55,
        "text": "We further enhance the pretraining with the task-specific training sets.",
        "labels": "MTD"
    },
    {
        "idx": 55,
        "text": "We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate.",
        "labels": "RST"
    },
    {
        "idx": 55,
        "text": "Our code and datasets will be made publicly available.",
        "labels": "CTN"
    },
    {
        "idx": 56,
        "text": " Discourse analysis allows us to attain inferences of a text document that extend beyond the sentence-level.",
        "labels": "BAC"
    },
    {
        "idx": 56,
        "text": "The current performance of discourse models is very low on texts outside of the training distribution's coverage, diminishing the practical utility of existing models",
        "labels": "GAP"
    },
    {
        "idx": 56,
        "text": "There is need for a measure that can inform us to what extent our model generalizes from the training to the test sample when these samples may be drawn from distinct distributions.",
        "labels": "GAP"
    },
    {
        "idx": 56,
        "text": "While this can be estimated via distribution shift, we argue that this does not directly correlate with change in the observed error of a classifier (i.e. error-gap).",
        "labels": "GAP"
    },
    {
        "idx": 56,
        "text": "Thus, we propose to use a statistic from the theoretical domain adaptation literature which can be directly tied to error-gap.",
        "labels": "PUR"
    },
    {
        "idx": 56,
        "text": "We study the bias of this statistic as an estimator of error-gap both theoretically and through a large-scale empirical study of over 2400 experiments on 6 discourse datasets from domains including, but not limited to: news, biomedical texts, TED talks, Reddit posts, and fiction.",
        "labels": "MTD"
    },
    {
        "idx": 56,
        "text": "Our results not only motivate our proposal and help us to understand its limitations, but also provide insight on the properties of discourse models and datasets which improve performance in domain adaptation.",
        "labels": "IMP"
    },
    {
        "idx": 56,
        "text": "For instance, we find that non-news datasets are slightly easier to transfer to than news datasets when the training and test sets are very different.",
        "labels": "IMP"
    },
    {
        "idx": 56,
        "text": "Our code and an associated Python package are available to allow practitioners to make more informed model and dataset choices.",
        "labels": "CTN"
    },
    {
        "idx": 57,
        "text": "Having sufficient resources for language X lifts it from the under-resourced languages class, but not necessarily from the under-researched class.",
        "labels": "GAP"
    },
    {
        "idx": 57,
        "text": "In this paper, we address the problem of the absence of organized benchmarks in the Turkish language. ",
        "labels": "PUR"
    },
    {
        "idx": 57,
        "text": "We demonstrate that languages such as Turkish are left behind the state-of-the-art in NLP applications.",
        "labels": "MTD"
    },
    {
        "idx": 57,
        "text": "As a solution, we present Mukayese, a set of NLP benchmarks for the Turkish language that contains several NLP tasks.",
        "labels": "MTD"
    },
    {
        "idx": 57,
        "text": "We work on one or more datasets for each benchmark and present two or more baselines. ",
        "labels": "MTD"
    },
    {
        "idx": 57,
        "text": "Moreover, we present four new benchmarking datasets in Turkish for language modeling, sentence segmentation, and spell checking.",
        "labels": "MTD"
    },
    {
        "idx": 57,
        "text": "All datasets and baselines are available under: https://github.com/alisafaya/mukayese",
        "labels": "CTN"
    },
    {
        "idx": 58,
        "text": " Despite profound successes, contrastive representation learning relies on carefully designed data augmentations using domain-specific knowledge.",
        "labels": "BAC"
    },
    {
        "idx": 58,
        "text": "This challenge is magnified in natural language processing, where no general rules exist for data augmentation due to the discrete nature of natural language.",
        "labels": "GAP"
    },
    {
        "idx": 58,
        "text": "We tackle this challenge by presenting a Virtual augmentation Supported Contrastive Learning of sentence representations (VaSCL).",
        "labels": "PUR"
    },
    {
        "idx": 58,
        "text": "Originating from the interpretation that data augmentation essentially constructs the neighborhoods of each training instance, we, in turn, utilize the neighborhood to generate effective data augmentations. ",
        "labels": "MTD"
    },
    {
        "idx": 58,
        "text": "Leveraging the large training batch size of contrastive learning, we approximate the neighborhood of an instance via its K-nearest in-batch neighbors in the representation space.",
        "labels": "MTD"
    },
    {
        "idx": 58,
        "text": "We then define an instance discrimination task regarding the neighborhood and generate the virtual augmentation in an adversarial training manner.",
        "labels": "MTD"
    },
    {
        "idx": 58,
        "text": "We access the performance of VaSCL on a wide range of downstream tasks and set a new state-of-the-art for unsupervised sentence representation learning.",
        "labels": "RST"
    },
    {
        "idx": 59,
        "text": "  Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge.",
        "labels": "BAC"
    },
    {
        "idx": 59,
        "text": "However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs.",
        "labels": "GAP"
    },
    {
        "idx": 59,
        "text": "This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain.",
        "labels": "GAP"
    },
    {
        "idx": 59,
        "text": "To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication.",
        "labels": "PUR"
    },
    {
        "idx": 59,
        "text": "Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input",
        "labels": "MTD"
    },
    {
        "idx": 59,
        "text": "Experimental results show that MoEfication can conditionally use 10% to 30% of FFN parameters while maintaining over 95% original performance for different models on various downstream tasks.",
        "labels": "RST"
    },
    {
        "idx": 59,
        "text": "Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs.",
        "labels": "RST"
    },
    {
        "idx": 59,
        "text": "The source code of this paper can be obtained from https://github.com/thunlp/MoEfication.",
        "labels": "CTN"
    },
    {
        "idx": 60,
        "text": " Recent work has shown that self-supervised dialog-specific pretraining on large conversational datasets yields substantial gains over traditional language modeling (LM) pretraining in downstream task-oriented dialog (TOD)",
        "labels": "BAC"
    },
    {
        "idx": 60,
        "text": "These approaches, however, exploit general dialogic corpora (e.g., Reddit) and thus presumably fail to reliably embed domain-specific knowledge useful for concrete downstream TOD domains",
        "labels": "GAP"
    },
    {
        "idx": 60,
        "text": "In this work, we investigate the effects of domain specialization of pretrained language models (PLMs) for TOD",
        "labels": "PUR"
    },
    {
        "idx": 60,
        "text": "Within our DS-TOD framework, we first automatically extract salient domain-specific terms, and then use them to construct DomainCC and DomainReddit - resources that we leverage for domain-specific pretraining, based on (i) masked language modeling (MLM) and (ii) response selection (RS) objectives, respectively.",
        "labels": "MTD"
    },
    {
        "idx": 60,
        "text": "We further propose a resource-efficient and modular domain specialization by means of domain adapters - additional parameter-light layers in which we encode the domain knowledge.",
        "labels": "MTD"
    },
    {
        "idx": 60,
        "text": "Our experiments with prominent TOD tasks - dialog state tracking (DST) and response retrieval (RR) - encompassing five domains from the MultiWOZ benchmark demonstrate the effectiveness of DS-TOD.",
        "labels": "RST"
    },
    {
        "idx": 60,
        "text": "Moreover, we show that the light-weight adapter-based specialization (1) performs comparably to full fine-tuning in single domain setups and (2) is particularly suitable for multi-domain specialization, where besides advantageous computational footprint, it can offer better TOD performance.",
        "labels": "CLN"
    },
    {
        "idx": 61,
        "text": "Recently, the problem of robustness of pre-trained language models (PrLMs) has received increasing research interest.",
        "labels": "BAC"
    },
    {
        "idx": 61,
        "text": "Latest studies on adversarial attacks achieve high attack success rates against PrLMs, claiming that PrLMs are not robust.",
        "labels": "BAC"
    },
    {
        "idx": 61,
        "text": "However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality. ",
        "labels": "GAP"
    },
    {
        "idx": 61,
        "text": "We question the validity of the current evaluation of robustness of PrLMs based on these non-natural adversarial samples and propose an anomaly detector to evaluate the robustness of PrLMs with more natural adversarial samples.",
        "labels": "PUR"
    },
    {
        "idx": 61,
        "text": "We also investigate two applications of the anomaly detector: (1) In data augmentation, we employ the anomaly detector to force generating augmented data that are distinguished as non-natural, which brings larger gains to the accuracy of PrLMs.",
        "labels": "MTD"
    },
    {
        "idx": 61,
        "text": "(2) We apply the anomaly detector to a defense framework to enhance the robustness of PrLMs.",
        "labels": "MTD"
    },
    {
        "idx": 61,
        "text": "It can be used to defend all types of attacks and achieves higher accuracy on both adversarial samples and compliant samples than other defense frameworks.",
        "labels": "IMP"
    },
    {
        "idx": 62,
        "text": "We present a comprehensive study of sparse attention patterns in Transformer models.",
        "labels": "PUR"
    },
    {
        "idx": 62,
        "text": "We first question the need for pre-training with sparse attention and present experiments showing that an efficient fine-tuning only approach yields a slightly worse but still competitive model.",
        "labels": "MTD"
    },
    {
        "idx": 62,
        "text": "Then we compare the widely used local attention pattern and the less-well-studied global attention pattern, demonstrating that global patterns have several unique advantages.",
        "labels": "MTD"
    },
    {
        "idx": 62,
        "text": "We also demonstrate that a flexible approach to attention, with different patterns across different layers of the model, is beneficial for some tasks.",
        "labels": "MTD"
    },
    {
        "idx": 62,
        "text": "Drawing on this insight, we propose a novel Adaptive Axis Attention method, which learns-during fine-tuning-different attention patterns for each Transformer layer depending on the downstream task.",
        "labels": "MTD"
    },
    {
        "idx": 62,
        "text": "Rather than choosing a fixed attention pattern, the adaptive axis attention method identifies important tokens-for each task and model layer-and focuses attention on those.",
        "labels": "RST"
    },
    {
        "idx": 62,
        "text": "It does not require pre-training to accommodate the sparse patterns and demonstrates competitive and sometimes better performance against fixed sparse attention patterns that require resource-intensive pre-training.",
        "labels": "CLN"
    },
    {
        "idx": 63,
        "text": "Most research on question answering focuses on the pre-deployment stage; i.e., building an accurate model for deployment",
        "labels": "BAC"
    },
    {
        "idx": 63,
        "text": "In this paper, we ask the question: Can we improve QA systems further post-deployment based on user interactions?",
        "labels": "PUR"
    },
    {
        "idx": 63,
        "text": "We focus on two kinds of improvements: 1) improving the QA system's performance itself, and 2) providing the model with the ability to explain the correctness or incorrectness of an answer",
        "labels": "MTD"
    },
    {
        "idx": 63,
        "text": "We collect a retrieval-based QA dataset, FeedbackQA, which contains interactive feedback from users.",
        "labels": "MTD"
    },
    {
        "idx": 63,
        "text": "We collect this dataset by deploying a base QA system to crowdworkers who then engage with the system and provide feedback on the quality of its answers",
        "labels": "MTD"
    },
    {
        "idx": 63,
        "text": "The feedback contains both structured ratings and unstructured natural language explanations",
        "labels": "MTD"
    },
    {
        "idx": 63,
        "text": "We train a neural model with this feedback data that can generate explanations and re-score answer candidates",
        "labels": "MTD"
    },
    {
        "idx": 63,
        "text": "We show that feedback data not only improves the accuracy of the deployed QA system but also other stronger non-deployed systems",
        "labels": "CLN"
    },
    {
        "idx": 63,
        "text": "The generated explanations also help users make informed decisions about the correctness of answers.",
        "labels": "IMP"
    },
    {
        "idx": 64,
        "text": "The application of Natural Language Inference (NLI) methods over large textual corpora can facilitate scientific discovery, reducing the gap between current research and the available large-scale scientific knowledge.",
        "labels": "BAC"
    },
    {
        "idx": 64,
        "text": "However, contemporary NLI models are still limited in interpreting mathematical knowledge written in Natural Language, even though mathematics is an integral part of scientific argumentation for many disciplines.",
        "labels": "GAP"
    },
    {
        "idx": 64,
        "text": "One of the fundamental requirements towards mathematical language understanding, is the creation of models able to meaningfully represent variables.",
        "labels": "BAC"
    },
    {
        "idx": 64,
        "text": "This problem is particularly challenging since the meaning of a variable should be assigned exclusively from its defining type, i.e., the representation of a variable should come from its context.",
        "labels": "GAP"
    },
    {
        "idx": 64,
        "text": "Recent research has formalised the variable typing task, a benchmark for the understanding of abstract mathematical types and variables in a sentence.",
        "labels": "BAC"
    },
    {
        "idx": 64,
        "text": "In this work, we propose VarSlot, a Variable Slot-based approach, which not only delivers state-of-the-art results in the task of variable typing, but is also able to create context-based representations for variables.",
        "labels": "PUR"
    },
    {
        "idx": 65,
        "text": " We propose GRS: an unsupervised approach to sentence simplification that combines text generation and text revision",
        "labels": "PUR"
    },
    {
        "idx": 65,
        "text": "We start with an iterative framework in which an input sentence is revised using explicit edit operations, and add paraphrasing as a new edit operation.",
        "labels": "MTD"
    },
    {
        "idx": 65,
        "text": "This allows us to combine the advantages of generative and revision-based approaches: paraphrasing captures complex edit operations, and the use of explicit edit operations in an iterative manner provides controllability and interpretability.",
        "labels": "MTD"
    },
    {
        "idx": 65,
        "text": "We demonstrate these advantages of GRS compared to existing methods on the Newsela and ASSET datasets",
        "labels": "RST"
    },
    {
        "idx": 66,
        "text": "Morphologically-rich polysynthetic languages present a challenge for NLP systems due to data sparsity, and a common strategy to handle this issue is to apply subword segmentation. ",
        "labels": "BAC"
    },
    {
        "idx": 66,
        "text": "We investigate a wide variety of supervised and unsupervised morphological segmentation methods for four polysynthetic languages: Nahuatl, Raramuri, Shipibo-Konibo, and Wixarika.",
        "labels": "MTD"
    },
    {
        "idx": 66,
        "text": "Then, we compare the morphologically inspired segmentation methods against Byte-Pair Encodings (BPEs) as inputs for machine translation (MT) when translating to and from Spanish.",
        "labels": "MTD"
    },
    {
        "idx": 66,
        "text": "We show that for all language pairs except for Nahuatl, an unsupervised morphological segmentation algorithm outperforms BPEs consistently and that, although supervised methods achieve better segmentation scores, they under-perform in MT challenges",
        "labels": "RST"
    },
    {
        "idx": 66,
        "text": " Finally, we contribute two new morphological segmentation datasets for Raramuri and Shipibo-Konibo, and a parallel corpus for Raramuri-Spanish.",
        "labels": "CLN"
    },
    {
        "idx": 67,
        "text": "We introduce distributed NLI, a new NLU task with a goal to predict the distribution of human judgements for natural language inference.",
        "labels": "PUR"
    },
    {
        "idx": 67,
        "text": "We show that by applying additional distribution estimation methods, namely, Monte Carlo (MC) Dropout, Deep Ensemble, Re-Calibration, and Distribution Distillation, models can capture human judgement distribution more effectively than the softmax baseline.",
        "labels": "MTD"
    },
    {
        "idx": 67,
        "text": "We show that MC Dropout is able to achieve decent performance without any distribution annotations while Re-Calibration can give further improvements with extra distribution annotations,",
        "labels": "RST"
    },
    {
        "idx": 67,
        "text": "suggesting the value of multiple annotations for one example in modeling the distribution of human judgements.",
        "labels": "CLN"
    },
    {
        "idx": 67,
        "text": "Despite these improvements, the best results are still far below the estimated human upper-bound, indicating that predicting the distribution of human judgements is still an open, challenging problem with a large room for improvements.",
        "labels": "CLN"
    },
    {
        "idx": 67,
        "text": "We showcase the common errors for MC Dropout and Re-Calibration.",
        "labels": "CLN"
    },
    {
        "idx": 67,
        "text": "Finally, we give guidelines on the usage of these methods with different levels of data availability and encourage future work on modeling the human opinion distribution for language reasoning.",
        "labels": "IMP"
    },
    {
        "idx": 68,
        "text": "Automatic morphological processing can aid downstream natural language processing applications, especially for low-resource languages, and assist language documentation efforts for endangered languages.",
        "labels": "BAC"
    },
    {
        "idx": 68,
        "text": "Having long been multilingual, the field of computational morphology is increasingly moving towards approaches suitable for languages with minimal or no annotated resources",
        "labels": "MTD"
    },
    {
        "idx": 68,
        "text": "First, we survey recent developments in computational morphology with a focus on low-resource languages.",
        "labels": "MTD"
    },
    {
        "idx": 68,
        "text": "Second, we argue that the field is ready to tackle the logical next challenge: understanding a language's morphology from raw text alone",
        "labels": "MTD"
    },
    {
        "idx": 68,
        "text": "We perform an empirical study on a truly unsupervised version of the paradigm completion task and show tha",
        "labels": "MTD"
    },
    {
        "idx": 68,
        "text": "while existing state-of-the-art models bridged by two newly proposed models we devise perform reasonably, there is still much room for improvement",
        "labels": "CLN"
    },
    {
        "idx": 68,
        "text": " The stakes are high: solving this task will increase the language coverage of morphological resources by a number of magnitudes.",
        "labels": "IMP"
    },
    {
        "idx": 69,
        "text": "We address the problem of learning fixed-length vector representations of characters in novels.",
        "labels": "BAC"
    },
    {
        "idx": 69,
        "text": "Recent advances in word embeddings have proven successful in learning entity representations from short texts,",
        "labels": "BAC"
    },
    {
        "idx": 69,
        "text": "but fall short on longer documents because they do not capture full book-level information.",
        "labels": "GAP"
    },
    {
        "idx": 69,
        "text": "To overcome the weakness of such text-based embeddings, we propose two novel methods for representing characters:",
        "labels": "PUR"
    },
    {
        "idx": 69,
        "text": "(i) graph neural network-based embeddings from a full corpus-based character network; and (ii) low-dimensional embeddings constructed from the occurrence pattern of characters in each novel.",
        "labels": "MTD"
    },
    {
        "idx": 69,
        "text": "We test the quality of these character embeddings using a new benchmark suite to evaluate character representations, encompassing 12 different tasks.",
        "labels": "MTD"
    },
    {
        "idx": 69,
        "text": "We show that our representation techniques combined with text-based embeddings lead to the best character representations, outperforming text-based embeddings in four tasks.",
        "labels": "CLN"
    },
    {
        "idx": 69,
        "text": "Our dataset and evaluation script will be made publicly available to stimulate additional work in this area.",
        "labels": "CTN"
    },
    {
        "idx": 70,
        "text": " Machine reading comprehension (MRC) has drawn a lot of attention as an approach for assessing the ability of systems to understand natural language.",
        "labels": "BAC"
    },
    {
        "idx": 70,
        "text": "Usually systems focus on selecting the correct answer to a question given a contextual paragraph.",
        "labels": "BAC"
    },
    {
        "idx": 70,
        "text": "However, for many applications of multiple-choice MRC systems there are two additional considerations",
        "labels": "GAP"
    },
    {
        "idx": 70,
        "text": "For multiple-choice exams there is often a negative marking scheme;",
        "labels": "GAP"
    },
    {
        "idx": 70,
        "text": "there is a penalty for an incorrect answer. In terms of an MRC system this means that the system is required to have an idea of the uncertainty in the predicted answer.",
        "labels": "GAP"
    },
    {
        "idx": 70,
        "text": "The second consideration is that many multiple-choice questions have the option of none-of-the-above (NOA) indicating that none of the answers is applicable, rather than there always being the correct answer in the list of choices.",
        "labels": "GAP"
    },
    {
        "idx": 70,
        "text": "This paper investigates both of these issues by making use of predictive uncertainty.",
        "labels": "PUR"
    },
    {
        "idx": 70,
        "text": "Whether the system should propose an answer is a direct application of answer uncertainty.",
        "labels": "MTD"
    },
    {
        "idx": 70,
        "text": "There are two possibilities when considering the NOA option.",
        "labels": "MTD"
    },
    {
        "idx": 70,
        "text": "The simplest is to explicitly build a system on data that includes this option.",
        "labels": "MTD"
    },
    {
        "idx": 70,
        "text": "Alternatively uncertainty can be applied to detect whether the other options include the correct answer.",
        "labels": "MTD"
    },
    {
        "idx": 70,
        "text": "If the system is not sufficiently confident it will select NOA.",
        "labels": "MTD"
    },
    {
        "idx": 70,
        "text": "As there is no standard corpus available to investigate these topics, the ReClor corpus is modified by removing the correct answer from a subset of possible answers.",
        "labels": "MTD"
    },
    {
        "idx": 70,
        "text": "A high-performance MRC system is used to evaluate whether answer uncertainty can be applied in these situations",
        "labels": "MTD"
    },
    {
        "idx": 70,
        "text": " It is shown that uncertainty does allow questions that the system is not confident about to be detected. ",
        "labels": "CLN"
    },
    {
        "idx": 70,
        "text": "Additionally it is shown that uncertainty outperforms a system explicitly built with an NOA option.",
        "labels": "RST"
    },
    {
        "idx": 71,
        "text": "Being able to reliably estimate self-disclosure - a key component of friendship and intimacy - from language is important for many psychology studies.",
        "labels": "BAC"
    },
    {
        "idx": 71,
        "text": "We build single-task models on five self-disclosure corpora",
        "labels": "MTD"
    },
    {
        "idx": 71,
        "text": "but find that these models generalize poorly; the within-domain accuracy of predicted message-level self-disclosure of the best-performing model (mean Pearson's r=0.69) is much higher than the respective across data set accuracy (mean Pearson's r=0.32), due to both variations in the corpora (e.g., medical vs. general topics) and labeling instructions (target variables: self-disclosure, emotional disclosure, intimacy).",
        "labels": "RST"
    },
    {
        "idx": 71,
        "text": "However, some lexical features, such as expression of negative emotions and use of first person personal pronouns such as I' reliably predict self-disclosure across corpora",
        "labels": "CLN"
    },
    {
        "idx": 71,
        "text": "We develop a multi-task model that yields better results, with an average Pearson's r of 0.37 for out-of-corpora prediction.",
        "labels": "RST"
    },
    {
        "idx": 72,
        "text": "Data Augmentation (DA) is known to improve the generalizability of deep neural networks.",
        "labels": "BAC"
    },
    {
        "idx": 72,
        "text": "Most existing DA techniques naively add a certain number of augmented samples without considering the quality and the added computational cost of these samples.",
        "labels": "GAP"
    },
    {
        "idx": 72,
        "text": "To tackle this problem, a common strategy, adopted by several state-of-the-art DA methods, is to adaptively generate or re-weight augmented samples with respect to the task objective during training.",
        "labels": "PUR"
    },
    {
        "idx": 72,
        "text": "However, these adaptive DA methods: (1) are computationally expensive and not sample-efficient, and (2) are designed merely for a specific setting. In this work, we present a universal DA technique, called Glitter, to overcome both issues.",
        "labels": "MTD"
    },
    {
        "idx": 72,
        "text": "Glitter can be plugged into any DA method, making training sample-efficient without sacrificing performance.",
        "labels": "MTD"
    },
    {
        "idx": 72,
        "text": "From a pre-generated pool of augmented samples, Glitter adaptively selects a subset of worst-case samples with maximal loss, analogous to adversarial DA.",
        "labels": "MTD"
    },
    {
        "idx": 72,
        "text": "Without altering the training strategy, the task objective can be optimized on the selected subset.",
        "labels": "MTD"
    },
    {
        "idx": 72,
        "text": "Our thorough experiments on the GLUE benchmark, SQuAD, and HellaSwag in three widely used training setups including consistency training, self-distillation and knowledge distillation reveal that Glitter is substantially faster to train and achieves a competitive performance, compared to strong baselines.",
        "labels": "RST"
    },
    {
        "idx": 73,
        "text": " Input saliency methods have recently become a popular tool for explaining predictions of deep learning models in NLP.",
        "labels": "BAC"
    },
    {
        "idx": 73,
        "text": "Nevertheless, there has been little work investigating methods for aggregating prediction-level explanations to the class level, nor has a framework for evaluating such class explanations been established.",
        "labels": "GAP"
    },
    {
        "idx": 73,
        "text": "We explore explanations based on XLM-R and the Integrated Gradients input attribution method, and propose 1) the Stable Attribution Class Explanation method (SACX) to extract keyword lists of classes in text classification tasks, and 2) a framework for the systematic evaluation of the keyword lists. ",
        "labels": "PUR"
    },
    {
        "idx": 73,
        "text": "We find that explanations of individual predictions are prone to noise, but that stable explanations can be effectively identified through repeated training and explanation.",
        "labels": "RST"
    },
    {
        "idx": 73,
        "text": "We evaluate on web register data and show that the class explanations are linguistically meaningful and distinguishing of the classes",
        "labels": "CLN"
    },
    {
        "idx": 74,
        "text": "Learning from rationales seeks to augment model prediction accuracy using human-annotated rationales (i.e. subsets of input tokens) that justify their chosen labels, often in the form of intermediate or multitask supervision.",
        "labels": "BAC"
    },
    {
        "idx": 74,
        "text": "While intuitive, this idea has proven elusive in practice.",
        "labels": "GAP"
    },
    {
        "idx": 74,
        "text": "We make two observations about human rationales via empirical analyses:1) maximizing rationale supervision accuracy is not necessarily the optimal objective for improving model accuracy; 2) human rationales vary in whether they provide sufficient information for the model to exploit for prediction",
        "labels": "MTD"
    },
    {
        "idx": 74,
        "text": "Building on these insights, we propose several novel loss functions and learning strategies, and evaluate their effectiveness on three datasets with human rationales.",
        "labels": "MTD"
    },
    {
        "idx": 74,
        "text": "Our results demonstrate consistent improvements over baselines in both label and rationale accuracy, including a 3% accuracy improvement on MultiRC.",
        "labels": "RST"
    },
    {
        "idx": 74,
        "text": "Our work highlights the importance of understanding properties of human explanations and exploiting them accordingly in model training.",
        "labels": "CLN"
    },
    {
        "idx": 75,
        "text": "Building on current work on multilingual hate speech (e.g., Ousidhoum et al. (2019)) and hate speech reduction (e.g., Sap et al. (2020)),",
        "labels": "BAC"
    },
    {
        "idx": 75,
        "text": "we present XTREMESPEECH, a new hate speech dataset containing 20,297 social media passages from Brazil, Germany, India and Kenya.",
        "labels": "PUR"
    },
    {
        "idx": 75,
        "text": "The key novelty is that we directly involve the affected communities in collecting and annotating the data - as opposed to giving companies and governments control over defining and combatting hate speech.",
        "labels": "MTD"
    },
    {
        "idx": 75,
        "text": "This inclusive approach results in datasets more representative of actually occurring online speech and is likely to facilitate the removal of the social media content that marginalized communities view as causing the most harm.",
        "labels": "CLN"
    },
    {
        "idx": 75,
        "text": "Based on XTREMESPEECH, we establish novel tasks with accompanying baselines, provide evidence that cross-country training is generally not feasible due to cultural differences between countries and perform an interpretability analysis of BERT's predictions.",
        "labels": "CLN"
    },
    {
        "idx": 76,
        "text": "Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability. E.g., neural hate speech detection models are strongly influenced by identity terms like gay, or women, resulting in false positives, severe unintended bias, and lower performance",
        "labels": "BAC"
    },
    {
        "idx": 76,
        "text": "Most mitigation techniques use lists of identity terms or samples from the target domain during training",
        "labels": "BAC"
    },
    {
        "idx": 76,
        "text": "However, this approach requires a-priori knowledge and introduces further bias if important terms are neglected",
        "labels": "GAP"
    },
    {
        "idx": 76,
        "text": "Instead, we propose a knowledge-free Entropy-based Attention Regularization (EAR) to discourage overfitting to training-specific terms",
        "labels": "PUR"
    },
    {
        "idx": 76,
        "text": "An additional objective function penalizes tokens with low self-attention entropy",
        "labels": "MTD"
    },
    {
        "idx": 76,
        "text": "We fine-tune BERT via EAR:",
        "labels": "MTD"
    },
    {
        "idx": 76,
        "text": "the resulting model matches or exceeds state-of-the-art performance for hate speech classification and bias metrics on three benchmark corpora in English and Italian",
        "labels": "RST"
    },
    {
        "idx": 76,
        "text": "EAR also reveals overfitting terms, i.e., terms most likely to induce bias, to help identify their effect on the model, task, and predictions.",
        "labels": "CLN"
    },
    {
        "idx": 77,
        "text": "Though successfully applied in research and industry large pretrained language models of the BERT family are not yet fully understood.",
        "labels": "GAP"
    },
    {
        "idx": 77,
        "text": "While much research in the field of BERTology has tested whether specific knowledge can be extracted from layer activations, we invert the popular probing design to analyze the prevailing differences and clusters in BERT's high dimensional space.",
        "labels": "PUR"
    },
    {
        "idx": 77,
        "text": "By extracting coarse features from masked token representations and predicting them by probing models with access to only partial information we can apprehend the variation from BERT's point of view'.",
        "labels": "MTD"
    },
    {
        "idx": 77,
        "text": "By applying our new methodology to different datasets",
        "labels": "MTD"
    },
    {
        "idx": 77,
        "text": "we show how much the differences can be described by syntax but further how they are to a great extent shaped by the most simple positional information.",
        "labels": "CLN"
    },
    {
        "idx": 78,
        "text": "Pre-trained word embeddings, such as GloVe, have shown undesirable gender, racial, and religious biases.",
        "labels": "GAP"
    },
    {
        "idx": 78,
        "text": "To address this problem, we propose DD-GloVe, a train-time debiasing algorithm to learn word embeddings by leveraging underline{dictionary underline{definitions.",
        "labels": "PUR"
    },
    {
        "idx": 78,
        "text": "We introduce dictionary-guided loss functions that encourage word embeddings to be similar to their relatively neutral dictionary definition representations.",
        "labels": "MTD"
    },
    {
        "idx": 78,
        "text": "Existing debiasing algorithms typically need a pre-compiled list of seed words to represent the bias direction, along which biased information gets removed.",
        "labels": "GAP"
    },
    {
        "idx": 78,
        "text": "Producing this list involves subjective decisions and it might be difficult to obtain for some types of biases.",
        "labels": "GAP"
    },
    {
        "idx": 78,
        "text": "We automate the process of finding seed words: our algorithm starts from a single pair of initial seed words and automatically finds more words whose definitions display similar attributes traits.",
        "labels": "MTD"
    },
    {
        "idx": 78,
        "text": "We demonstrate the effectiveness of our approach with benchmark evaluations and empirical analyses.",
        "labels": "RST"
    },
    {
        "idx": 78,
        "text": "Our code is available at https://github.com/haozhe-an/DD-GloVe.",
        "labels": "CTN"
    },
    {
        "idx": 79,
        "text": "Knowledge graph embedding aims to represent entities and relations as low-dimensional vectors, which is an effective way for predicting missing links in knowledge graphs.",
        "labels": "BAC"
    },
    {
        "idx": 79,
        "text": "Designing a strong and effective loss framework is essential for knowledge graph embedding models to distinguish between correct and incorrect triplets.",
        "labels": "BAC"
    },
    {
        "idx": 79,
        "text": "The classic margin-based ranking loss limits the scores of positive and negative triplets to have a suitable margin.",
        "labels": "BAC"
    },
    {
        "idx": 79,
        "text": "The recently proposed Limit-based Scoring Loss independently limits the range of positive and negative triplet scores.",
        "labels": "GAP"
    },
    {
        "idx": 79,
        "text": "However, these loss frameworks use equal or fixed penalty terms to reduce the scores of positive and negative sample pairs, which is inflexible in optimization.",
        "labels": "GAP"
    },
    {
        "idx": 79,
        "text": "Our intuition is that if a triplet score deviates far from the optimum, it should be emphasized.",
        "labels": "PUR"
    },
    {
        "idx": 79,
        "text": "To this end, we propose Adaptive Limit Scoring Loss, which simply re-weights each triplet to highlight the less-optimized triplet scores.",
        "labels": "MTD"
    },
    {
        "idx": 79,
        "text": "We apply this loss framework to several knowledge graph embedding models such as TransE, TransH and ComplEx.",
        "labels": "MTD"
    },
    {
        "idx": 79,
        "text": "The experimental results on link prediction and triplet classification show that our proposed method has achieved performance on par with the state of the art.",
        "labels": "RST"
    },
    {
        "idx": 80,
        "text": "We aim to investigate the performance of current OCR systems on low resource languages and low resource scripts",
        "labels": "PUR"
    },
    {
        "idx": 80,
        "text": "We introduce and make publicly available a novel benchmark, OCR4MT, consisting of real and synthetic data, enriched with noise, for 60 low-resource languages in low resource scripts.",
        "labels": "MTD"
    },
    {
        "idx": 80,
        "text": "We evaluate state-of-the-art OCR systems on our benchmark and analyse most common errors.",
        "labels": "MTD"
    },
    {
        "idx": 80,
        "text": "We show that OCR monolingual data is a valuable resource that can increase performance of Machine Translation models, when used in backtranslation.",
        "labels": "MTD"
    },
    {
        "idx": 80,
        "text": "We then perform an ablation study to investigate how OCR errors impact Machine Translation performance and determine what is the minimum level of OCR quality needed for the monolingual data to be useful for Machine Translation.",
        "labels": "MTD"
    },
    {
        "idx": 81,
        "text": " Large-scale pre-trained language models have demonstrated strong knowledge representation ability.",
        "labels": "BAC"
    },
    {
        "idx": 81,
        "text": "However, recent studies suggest that even though these giant models contain rich simple commonsense knowledge (e.g., bird can fly and fish can swim.), they often struggle with complex commonsense knowledge that involves multiple eventualities (verb-centric phrases, e.g., identifying the relationship between Jim yells at Bob and Bob is upset).",
        "labels": "GAP"
    },
    {
        "idx": 81,
        "text": "To address this issue, in this paper, we propose to help pre-trained language models better incorporate complex commonsense knowledge.",
        "labels": "PUR"
    },
    {
        "idx": 81,
        "text": "Unlike direct fine-tuning approaches, we do not focus on a specific task and instead propose a general language model named CoCoLM",
        "labels": "MTD"
    },
    {
        "idx": 81,
        "text": "Through the careful training over a large-scale eventuality knowledge graph ASER,",
        "labels": "MTD"
    },
    {
        "idx": 81,
        "text": "we successfully teach pre-trained language models (i.e., BERT and RoBERTa) rich multi-hop commonsense knowledge among eventualities",
        "labels": "MTD"
    },
    {
        "idx": 81,
        "text": "Experiments on multiple commonsense tasks that require the correct understanding of eventualities demonstrate the effectiveness of CoCoLM.",
        "labels": "RST"
    },
    {
        "idx": 82,
        "text": "A critical bottleneck in supervised machine learning is the need for large amounts of labeled data which is expensive and time-consuming to obtain.",
        "labels": "BAC"
    },
    {
        "idx": 82,
        "text": "Although a small amount of labeled data cannot be used to train a model, it can be used effectively for the generation of humaninterpretable labeling functions (LFs).",
        "labels": "BAC"
    },
    {
        "idx": 82,
        "text": "These LFs, in turn, have been used to generate a large amount of additional noisy labeled data in a paradigm that is now commonly referred to as data programming.",
        "labels": "BAC"
    },
    {
        "idx": 82,
        "text": "Previous methods of generating LFs do not attempt to use the given labeled data further to train a model, thus missing opportunities for improving performance.",
        "labels": "GAP"
    },
    {
        "idx": 82,
        "text": "Additionally, since the LFs are generated automatically, they are likely to be noisy, and naively aggregating these LFs can lead to suboptimal results.",
        "labels": "GAP"
    },
    {
        "idx": 82,
        "text": "In this work, we propose an LF-based bi-level optimization framework WISDOM to solve these two critical limitations.",
        "labels": "PUR"
    },
    {
        "idx": 82,
        "text": "ISDOM learns a joint model on the (same) labeled dataset used for LF induction along with any unlabeled data in a semi-supervised manner, and more critically, reweighs each LF according to its goodness, influencing its contribution to the semi-supervised loss using a robust bi-level optimization algorithm",
        "labels": "MTD"
    },
    {
        "idx": 82,
        "text": "We show that WISDOM significantly outperforms prior approaches on several text classification datasets",
        "labels": "RST"
    },
    {
        "idx": 83,
        "text": "The emotion cause pair extraction (ECPE) task aims to extract emotions and causes as pairs from documents.",
        "labels": "BAC"
    },
    {
        "idx": 83,
        "text": "We observe that the relative distance distribution of emotions and causes is extremely imbalanced in the typical ECPE dataset.",
        "labels": "GAP"
    },
    {
        "idx": 83,
        "text": "Existing methods have set a fixed size window to capture relations between neighboring clauses.",
        "labels": "BAC"
    },
    {
        "idx": 83,
        "text": "However, they neglect the effective semantic connections between distant clauses, leading to poor generalization ability towards position-insensitive data.",
        "labels": "GAP"
    },
    {
        "idx": 83,
        "text": "To alleviate the problem, we propose a novel Multi-Granularity Semantic Aware Graph model (MGSAG) to incorporate fine-grained and coarse-grained semantic features jointly, without regard to distance limitation",
        "labels": "PUR"
    },
    {
        "idx": 83,
        "text": " In particular, we first explore semantic dependencies between clauses and keywords extracted from the document that convey fine-grained semantic features, obtaining keywords enhanced clause representations.",
        "labels": "MTD"
    },
    {
        "idx": 83,
        "text": "Besides, a clause graph is also established to model coarse-grained semantic relations between clauses.",
        "labels": "MTD"
    },
    {
        "idx": 83,
        "text": "Experimental results indicate that MGSAG surpasses the existing state-of-the-art ECPE models.",
        "labels": "RST"
    },
    {
        "idx": 83,
        "text": "Especially, MGSAG outperforms other models significantly in the condition of position-insensitive data.",
        "labels": "RST"
    },
    {
        "idx": 84,
        "text": "Predicate entailment detection is a crucial task for question-answering from text, where previous work has explored unsupervised learning of entailment graphs from typed open relation triples.",
        "labels": "BAC"
    },
    {
        "idx": 84,
        "text": "In this paper, we present the first pipeline for building Chinese entailment graphs, which involves a novel high-recall open relation extraction (ORE) method and the first Chinese fine-grained entity typing dataset under the FIGER type ontology.",
        "labels": "PUR"
    },
    {
        "idx": 84,
        "text": "Through experiments on the Levy-Holt dataset",
        "labels": "MTD"
    },
    {
        "idx": 84,
        "text": "we verify the strength of our Chinese entailment graph, and reveal the cross-lingual complementarity: on the parallel Levy-Holt dataset, an ensemble of Chinese and English entailment graphs outperforms both monolingual graphs, and raises unsupervised SOTA by 4.7 AUC points",
        "labels": "CLN"
    },
    {
        "idx": 85,
        "text": "Paraphrase generation using deep learning has been a research hotspot of natural language processing in the past few years.",
        "labels": "BAC"
    },
    {
        "idx": 85,
        "text": "While previous studies tackle the problem from different aspects, the essence of paraphrase generation is to retain the key semantics of the source sentence and rewrite the rest of the content.",
        "labels": "GAP"
    },
    {
        "idx": 85,
        "text": "Inspired by this observation, we propose a novel two-stage model, PGKPR, for paraphrase generation with keyword and part-of-speech reconstruction",
        "labels": "PUR"
    },
    {
        "idx": 85,
        "text": "The rationale is to capture simultaneously the possible keywords of a source sentence and the relations between them to facilitate the rewriting",
        "labels": "MTD"
    },
    {
        "idx": 85,
        "text": "In the first stage, we identify the possible keywords using a prediction attribution technique, where the words obtaining higher attribution scores are more likely to be the keywords.",
        "labels": "MTD"
    },
    {
        "idx": 85,
        "text": "In the second stage, we train a transformer-based model via multi-task learning for paraphrase generation.",
        "labels": "MTD"
    },
    {
        "idx": 85,
        "text": "The novel learning task is the reconstruction of the keywords and part-of-speech tags, respectively, from a perturbed sequence of the source sentence.",
        "labels": "MTD"
    },
    {
        "idx": 85,
        "text": "The learned encodings are then decoded to generate the paraphrase",
        "labels": "MTD"
    },
    {
        "idx": 85,
        "text": "We conduct the experiments on two commonly-used datasets, and demonstrate the superior performance of PGKPR over comparative models on multiple evaluation metrics.",
        "labels": "RST"
    },
    {
        "idx": 86,
        "text": "Chinese Spelling Correction (CSC) is a task to detect and correct misspelled characters in Chinese texts. CSC is challenging since many Chinese characters are visually or phonologically similar but with quite different semantic meanings. Many recent works use BERT-based language models to directly correct each character of the input sentence",
        "labels": "BAC"
    },
    {
        "idx": 86,
        "text": "However, these methods can be sub-optimal since they correct every character of the sentence only by the context which is easily negatively affected by the misspelled characters",
        "labels": "GAP"
    },
    {
        "idx": 86,
        "text": "Some other works propose to use an error detector to guide the correction by masking the detected errors.",
        "labels": "BAC"
    },
    {
        "idx": 86,
        "text": "Nevertheless, these methods dampen the visual or phonological features from the misspelled characters which could be critical for correction.",
        "labels": "GAP"
    },
    {
        "idx": 86,
        "text": "In this work, we propose a novel general detector-corrector multi-task framework where the corrector uses BERT to capture the visual and phonological features from each character in the raw sentence and uses a late fusion strategy to fuse the hidden states of the corrector with that of the detector to minimize the negative impact from the misspelled characters.",
        "labels": "PUR"
    },
    {
        "idx": 86,
        "text": "Comprehensive experiments on benchmarks demonstrate that our proposed method can significantly outperform the state-of-the-art methods in the CSC task.",
        "labels": "RST"
    },
    {
        "idx": 87,
        "text": "The task of converting a natural language question into an executable SQL query, known as text-to-SQL, is an important branch of semantic parsing.",
        "labels": "BAC"
    },
    {
        "idx": 87,
        "text": "The state-of-the-art graph-based encoder has been successfully used in this task but does not model the question syntax well.",
        "labels": "GAP"
    },
    {
        "idx": 87,
        "text": "In this paper, we propose S^2SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL parsers, which effectively leverages the syntactic dependency information of questions in text-to-SQL to improve the performance.",
        "labels": "PUR"
    },
    {
        "idx": 87,
        "text": "We also employ the decoupling constraint to induce diverse relational edge embedding, which further improves the network's performance.",
        "labels": "MTD"
    },
    {
        "idx": 87,
        "text": "Experiments on the Spider and robustness setting Spider-Syn demonstrate that the proposed approach outperforms all existing methods when pre-training models are used, resulting in a performance ranks first on the Spider leaderboard.",
        "labels": "RST"
    },
    {
        "idx": 88,
        "text": "This paper presents the first multi-objective transformer model for generating open cloze tests that exploits generation and discrimination capabilities to improve performance.",
        "labels": "PUR"
    },
    {
        "idx": 88,
        "text": "Our model is further enhanced by tweaking its loss function and applying a post-processing re-ranking algorithm that improves overall test structure.",
        "labels": "MTD"
    },
    {
        "idx": 88,
        "text": "Experiments using automatic and human evaluation show that our approach can achieve up to 82% accuracy according to experts, outperforming previous work and baselines.",
        "labels": "RST"
    },
    {
        "idx": 88,
        "text": "We also release a collection of high-quality open cloze tests along with sample system output and human annotations that can serve as a future benchmark.",
        "labels": "RST"
    },
    {
        "idx": 89,
        "text": "We introduce a method for unsupervised parsing that relies on bootstrapping classifiers to identify if a node dominates a specific span in a sentence.",
        "labels": "PUR"
    },
    {
        "idx": 89,
        "text": "There are two types of classifiers, an inside classifier that acts on a span, and an outside classifier that acts on everything outside of a given span.",
        "labels": "MTD"
    },
    {
        "idx": 89,
        "text": "Through self-training and co-training with the two classifiers,",
        "labels": "MTD"
    },
    {
        "idx": 89,
        "text": "we show that the interplay between them helps improve the accuracy of both, and as a result, effectively parse.",
        "labels": "RST"
    },
    {
        "idx": 89,
        "text": "A seed bootstrapping technique prepares the data to train these classifiers",
        "labels": "MTD"
    },
    {
        "idx": 89,
        "text": "Our analyses further validate that such an approach in conjunction with weak supervision using prior branching knowledge of a known language (left/right-branching) and minimal heuristics injects strong inductive bias into the parser, achieving 63.1 F_1 on the English (PTB) test set",
        "labels": "CLN"
    },
    {
        "idx": 89,
        "text": "In addition, we show the effectiveness of our architecture by evaluating on treebanks for Chinese (CTB) and Japanese (KTB) and achieve new state-of-the-art results.",
        "labels": "RST"
    },
    {
        "idx": 90,
        "text": "Transformer-based language models usually treat texts as linear sequences.",
        "labels": "BAC"
    },
    {
        "idx": 90,
        "text": "However, most texts also have an inherent hierarchical structure, i.e., parts of a text can be identified using their position in this hierarchy.",
        "labels": "GAP"
    },
    {
        "idx": 90,
        "text": "In addition, section titles usually indicate the common topic of their respective sentences.",
        "labels": "GAP"
    },
    {
        "idx": 90,
        "text": "We propose a novel approach to formulate, extract, encode and inject hierarchical structure information explicitly into an extractive summarization model based on a pre-trained, encoder-only Transformer language model (HiStruct+ model), which improves SOTA ROUGEs for extractive summarization on PubMed and arXiv substantially.",
        "labels": "PUR"
    },
    {
        "idx": 90,
        "text": "Using various experimental settings on three datasets (i.e., CNN/DailyMail, PubMed and arXiv),",
        "labels": "MTD"
    },
    {
        "idx": 90,
        "text": "our HiStruct+ model outperforms a strong baseline collectively, which differs from our model only in that the hierarchical structure information is not injected.",
        "labels": "RST"
    },
    {
        "idx": 90,
        "text": "It is also observed that the more conspicuous hierarchical structure the dataset has, the larger improvements our method gains.",
        "labels": "CLN"
    },
    {
        "idx": 90,
        "text": "The ablation study demonstrates that the hierarchical position information is the main contributor to our model's SOTA performance.",
        "labels": "CLN"
    },
    {
        "idx": 91,
        "text": "Several studies have explored various advantages of multilingual pre-trained models (such as multilingual BERT) in capturing shared linguistic knowledge",
        "labels": "BAC"
    },
    {
        "idx": 91,
        "text": "However, less attention has been paid to their limitations.",
        "labels": "GAP"
    },
    {
        "idx": 91,
        "text": "In this paper, we investigate the multilingual BERT for two known issues of the monolingual models: anisotropic embedding space and outlier dimensions.",
        "labels": "PUR"
    },
    {
        "idx": 91,
        "text": "We show that, unlike its monolingual counterpart, the multilingual BERT model exhibits no outlier dimension in its representations while it has a highly anisotropic space.",
        "labels": "RST"
    },
    {
        "idx": 91,
        "text": "There are a few dimensions in the monolingual BERT with high contributions to the anisotropic distribution.",
        "labels": "RST"
    },
    {
        "idx": 91,
        "text": "However, we observe no such dimensions in the multilingual BERT.",
        "labels": "RST"
    },
    {
        "idx": 91,
        "text": "Furthermore, our experimental results demonstrate that increasing the isotropy of multilingual space can significantly improve its representation power and performance, similarly to what had been observed for monolingual CWRs on semantic similarity tasks.",
        "labels": "RST"
    },
    {
        "idx": 91,
        "text": "Our analysis indicates that, despite having different degenerated directions, the embedding spaces in various languages tend to be partially similar with respect to their structures.",
        "labels": "CLN"
    },
    {
        "idx": 92,
        "text": "Existing knowledge-grounded dialogue systems typically use finetuned versions of a pretrained language model (LM) and large-scale knowledge bases.",
        "labels": "BAC"
    },
    {
        "idx": 92,
        "text": "These models typically fail to generalize on topics outside of the knowledge base, and require maintaining separate potentially large checkpoints each time finetuning is needed",
        "labels": "GAP"
    },
    {
        "idx": 92,
        "text": "In this paper, we aim to address these limitations by leveraging the inherent knowledge stored in the pretrained LM as well as its powerful generation ability.",
        "labels": "PUR"
    },
    {
        "idx": 92,
        "text": "We propose a multi-stage prompting approach to generate knowledgeable responses from a single pretrained LM.",
        "labels": "MTD"
    },
    {
        "idx": 92,
        "text": "We first prompt the LM to generate knowledge based on the dialogue context.",
        "labels": "MTD"
    },
    {
        "idx": 92,
        "text": "Then, we further prompt it to generate responses based on the dialogue context and the previously generated knowledge.",
        "labels": "MTD"
    },
    {
        "idx": 92,
        "text": "Results show that our knowledge generator outperforms the state-of-the-art retrieval-based model by 5.8% when combining knowledge relevance and correctness.",
        "labels": "RST"
    },
    {
        "idx": 92,
        "text": "In addition, our multi-stage prompting outperforms the finetuning-based dialogue model in terms of response knowledgeability and engagement by up to 10% and 5%, respectively.",
        "labels": "RST"
    },
    {
        "idx": 92,
        "text": "Furthermore, we scale our model up to 530 billion parameters and demonstrate that larger LMs improve the generation correctness score by up to 10%, and response relevance, knowledgeability and engagement by up to 10%.",
        "labels": "RST"
    },
    {
        "idx": 92,
        "text": "Our code is available at: https://github.com/NVIDIA/Megatron-LM.",
        "labels": "CTN"
    },
    {
        "idx": 93,
        "text": "Open-domain question answering has been used in a wide range of applications, such as web search and enterprise search, which usually takes clean texts extracted from various formats of documents (e.g., web pages, PDFs, or Word documents) as the information source.",
        "labels": "BAC"
    },
    {
        "idx": 93,
        "text": "However, designing different text extraction approaches is time-consuming and not scalable.",
        "labels": "GAP"
    },
    {
        "idx": 93,
        "text": "In order to reduce human cost and improve the scalability of QA systems, we propose and study an Open-domain Document Visual Question Answering (Open-domain DocVQA) task, which requires answering questions based on a collection of document images directly instead of only document texts, utilizing layouts and visual features additionally",
        "labels": "PUR"
    },
    {
        "idx": 93,
        "text": "Towards this end, we introduce the first Chinese Open-domain DocVQA dataset called DuReader_{vis, containing about 15K question-answering pairs and 158K document images from the Baidu search engine.",
        "labels": "MTD"
    },
    {
        "idx": 93,
        "text": "There are three main challenges in DuReader_{vis: (1) long document understanding, (2) noisy texts, and (3) multi-span answer extraction.",
        "labels": "MTD"
    },
    {
        "idx": 93,
        "text": "The extensive experiments demonstrate that the dataset is challenging.",
        "labels": "RST"
    },
    {
        "idx": 93,
        "text": "Additionally, we propose a simple approach that incorporates the layout and visual features,",
        "labels": "MTD"
    },
    {
        "idx": 93,
        "text": "and the experimental results show the effectiveness of the proposed approach.",
        "labels": "RST"
    },
    {
        "idx": 93,
        "text": "The dataset and code will be publicly available at https://github.com/baidu/DuReader/tree/master/DuReader-vis.",
        "labels": "CTN"
    },
    {
        "idx": 94,
        "text": " Relations between words are governed by hierarchical structure rather than linear ordering.",
        "labels": "BAC"
    },
    {
        "idx": 94,
        "text": "Sequence-to-sequence (seq2seq) models, despite their success in downstream NLP applications, often fail to generalize in a hierarchy-sensitive manner when performing syntactic transformations-for example, transforming declarative sentences into questions.",
        "labels": "GAP"
    },
    {
        "idx": 94,
        "text": "However, syntactic evaluations of seq2seq models have only observed models that were not pre-trained on natural language data before being trained to perform syntactic transformations, in spite of the fact that pre-training has been found to induce hierarchical linguistic generalizations in language models; in other words, the syntactic capabilities of seq2seq models may have been greatly understated.",
        "labels": "GAP"
    },
    {
        "idx": 94,
        "text": "We address this gap using the pre-trained seq2seq models T5 and BART, as well as their multilingual variants mT5 and mBART.",
        "labels": "MTD"
    },
    {
        "idx": 94,
        "text": "We evaluate whether they generalize hierarchically on two transformations in two languages: question formation and passivization in English and German.",
        "labels": "MTD"
    },
    {
        "idx": 94,
        "text": "We find that pre-trained seq2seq models generalize hierarchically when performing syntactic transformations, whereas models trained from scratch on syntactic transformations do not.",
        "labels": "RST"
    },
    {
        "idx": 94,
        "text": "This result presents evidence for the learnability of hierarchical syntactic information from non-annotated natural language text while also demonstrating that seq2seq models are capable of syntactic generalization, though only after exposure to much more language data than human learners receive.",
        "labels": "CLN"
    },
    {
        "idx": 95,
        "text": " Existing commonsense knowledge bases often organize tuples in an isolated manner, which is deficient for commonsense conversational models to plan the next steps",
        "labels": "BAC"
    },
    {
        "idx": 95,
        "text": "To fill the gap, we curate a large-scale multi-turn human-written conversation corpus, and create the first Chinese commonsense conversation knowledge graph which incorporates both social commonsense knowledge and dialog flow information.",
        "labels": "PUR"
    },
    {
        "idx": 95,
        "text": "To show the potential of our graph, we develop a graph-conversation matching approach, and benchmark two graph-grounded conversational tasks.",
        "labels": "RST"
    },
    {
        "idx": 95,
        "text": "All the resources in this work will be released to foster future research.",
        "labels": "CTN"
    },
    {
        "idx": 96,
        "text": " After a period of decrease, interest in word alignments is increasing again for their usefulness in domains such as typological research, cross-lingual annotation projection and machine translation.",
        "labels": "BAC"
    },
    {
        "idx": 96,
        "text": "Generally, alignment algorithms only use bitext and do not make use of the fact that many parallel corpora are multiparallel.",
        "labels": "GAP"
    },
    {
        "idx": 96,
        "text": "Here, we compute high-quality word alignments between multiple language pairs by considering all language pairs together.",
        "labels": "PUR"
    },
    {
        "idx": 96,
        "text": "First, we create a multiparallel word alignment graph, joining all bilingual word alignment pairs in one graph.",
        "labels": "MTD"
    },
    {
        "idx": 96,
        "text": "Next, we use graph neural networks (GNNs) to exploit the graph structure.",
        "labels": "MTD"
    },
    {
        "idx": 96,
        "text": "Our GNN approach (i) utilizes information about the meaning, position and language of the input words, (ii) incorporates information from multiple parallel sentences, (iii) adds and removes edges from the initial alignments, and (iv) yields a prediction model that can generalize beyond the training sentences.",
        "labels": "MTD"
    },
    {
        "idx": 96,
        "text": "We show that community detection algorithms can provide valuable information for multiparallel word alignment",
        "labels": "CLN"
    },
    {
        "idx": 96,
        "text": "Our method outperforms previous work on three word alignment datasets and on a downstream task.",
        "labels": "RST"
    },
    {
        "idx": 98,
        "text": "Medical code prediction from clinical notes aims at automatically associating medical codes with the clinical notes.",
        "labels": "BAC"
    },
    {
        "idx": 98,
        "text": "Rare code problem, the medical codes with low occurrences, is prominent in medical code prediction.",
        "labels": "GAP"
    },
    {
        "idx": 98,
        "text": "Recent studies employ deep neural networks and the external knowledge to tackle it.",
        "labels": "BAC"
    },
    {
        "idx": 98,
        "text": "However, such approaches lack interpretability which is a vital issue in medical application.",
        "labels": "GAP"
    },
    {
        "idx": 98,
        "text": "Moreover, due to the lengthy and noisy clinical notes, such approaches fail to achieve satisfactory results",
        "labels": "GAP"
    },
    {
        "idx": 98,
        "text": "Therefore, in this paper, we propose a novel framework based on medical concept driven attention to incorporate external knowledge for explainable medical code prediction.",
        "labels": "PUR"
    },
    {
        "idx": 98,
        "text": "In specific, both the clinical notes and Wikipedia documents are aligned into topic space to extract medical concepts using topic modeling.",
        "labels": "MTD"
    },
    {
        "idx": 98,
        "text": "Then, the medical concept-driven attention mechanism is applied to uncover the medical code related concepts which provide explanations for medical code prediction.",
        "labels": "MTD"
    },
    {
        "idx": 98,
        "text": "Experimental results on the benchmark dataset show the superiority of the proposed framework over several state-of-the-art baselines.",
        "labels": "RST"
    },
    {
        "idx": 99,
        "text": "Unsupervised constrained text generation aims to generate text under a given set of constraints without any supervised data.",
        "labels": "BAC"
    },
    {
        "idx": 99,
        "text": "Current state-of-the-art methods stochastically sample edit positions and actions, which may cause unnecessary search steps.",
        "labels": "GAP"
    },
    {
        "idx": 99,
        "text": "In this paper, we propose PMCTG to improve effectiveness by searching for the best edit position and action in each step.",
        "labels": "PUR"
    },
    {
        "idx": 99,
        "text": "Specifically, PMCTG extends perturbed masking technique to effectively search for the most incongruent token to edit.",
        "labels": "MTD"
    },
    {
        "idx": 99,
        "text": "Then it introduces four multi-aspect scoring functions to select edit action to further reduce search difficulty.",
        "labels": "MTD"
    },
    {
        "idx": 99,
        "text": "Since PMCTG does not require supervised data, it could be applied to different generation tasks.",
        "labels": "RST"
    },
    {
        "idx": 99,
        "text": "We show that under the unsupervised setting, PMCTG achieves new state-of-the-art results in two representative tasks, namely keywords- to-sentence generation and paraphrasing.",
        "labels": "CLN"
    },
    {
        "idx": 100,
        "text": " Graph-based methods, which decompose the score of a dependency tree into scores of dependency arcs, are popular in dependency parsing for decades.",
        "labels": "BAC"
    },
    {
        "idx": 100,
        "text": "Recently, (CITATION) propose a headed-span-based method that decomposes the score of a dependency tree into scores of headed spans.",
        "labels": "BAC"
    },
    {
        "idx": 100,
        "text": "They show improvement over first-order graph-based methods.",
        "labels": "BAC"
    },
    {
        "idx": 100,
        "text": "However, their method does not score dependency arcs at all, and dependency arcs are implicitly induced by their cubic-time algorithm, which is possibly sub-optimal since modeling dependency arcs is intuitively usefu",
        "labels": "GAP"
    },
    {
        "idx": 100,
        "text": "In this work, we aim to combine graph-based and headed-span-based methods, incorporating both arc scores and headed span scores into our model.",
        "labels": "PUR"
    },
    {
        "idx": 100,
        "text": "First, we show a direct way to combine with O(n^4) parsing complexity.",
        "labels": "MTD"
    },
    {
        "idx": 100,
        "text": "To decrease complexity, inspired by the classical head-splitting trick, we show two O(n^3) dynamic programming algorithms to combine first- and second-order graph-based and headed-span-based methods.",
        "labels": "MTD"
    },
    {
        "idx": 100,
        "text": "Our experiments on PTB, CTB, and UD show that combining first-order graph-based and headed-span-based methods is effective.",
        "labels": "RST"
    },
    {
        "idx": 100,
        "text": "We also confirm the effectiveness of second-order graph-based parsing in the deep learning age, however, we observe marginal or no improvement when combining second-order graph-based and headed-span-based methods .",
        "labels": "CLN"
    },
    {
        "idx": 101,
        "text": "Code switching (CS) refers to the phenomenon of interchangeably using words and phrases from different languages.",
        "labels": "BAC"
    },
    {
        "idx": 101,
        "text": "CS can pose significant accuracy challenges to NLP, due to the often monolingual nature of the underlying systems.",
        "labels": "BAC"
    },
    {
        "idx": 101,
        "text": "In this work, we focus on CS in the context of English/Spanish conversations for the task of speech translation (ST), generating and evaluating both transcript and translation.",
        "labels": "PUR"
    },
    {
        "idx": 101,
        "text": "To evaluate model performance on this task, we create a novel ST corpus derived from existing public data sets.",
        "labels": "MTD"
    },
    {
        "idx": 101,
        "text": "We explore various ST architectures across two dimensions: cascaded (transcribe then translate) vs end-to-end (jointly transcribe and translate) and unidirectional (source -greater target) vs bidirectional (source less-greater target).",
        "labels": "MTD"
    },
    {
        "idx": 101,
        "text": "We show that our ST architectures, and especially our bidirectional end-to-end architecture, perform well on CS speech, even when no CS training data is used.",
        "labels": "RST"
    },
    {
        "idx": 102,
        "text": "Recent interest in entity linking has focused in the zero-shot scenario, where at test time the entity mention to be labelled is never seen during training, or may belong to a different domain from the source domain.",
        "labels": "BAC"
    },
    {
        "idx": 102,
        "text": "Current work leverage pre-trained BERT with the implicit assumption that it bridges the gap between the source and target domain distributions.",
        "labels": "BAC"
    },
    {
        "idx": 102,
        "text": "However, fine-tuned BERT has a considerable underperformance at zero-shot when applied in a different domain.",
        "labels": "GAP"
    },
    {
        "idx": 102,
        "text": "We solve this problem by proposing a Transformational Biencoder that incorporates a transformation into BERT to perform a zero-shot transfer from the source domain during training.",
        "labels": "PUR"
    },
    {
        "idx": 102,
        "text": "As like previous work, we rely on negative entities to encourage our model to discriminate the golden entities during training.",
        "labels": "MTD"
    },
    {
        "idx": 102,
        "text": "To generate these negative entities, we propose a simple but effective strategy that takes the domain of the golden entity into perspective.",
        "labels": "MTD"
    },
    {
        "idx": 102,
        "text": "Our experimental results on the benchmark dataset Zeshel show effectiveness of our approach and achieve new state-of-the-art.",
        "labels": "RST"
    },
    {
        "idx": 103,
        "text": "The Lottery Ticket Hypothesis suggests that for any over-parameterized model, a small subnetwork exists to achieve competitive performance compared to the backbone architecture",
        "labels": "BAC"
    },
    {
        "idx": 103,
        "text": "In this paper, we study whether there is a winning lottery ticket for pre-trained language models, which allow the practitioners to fine-tune the parameters in the ticket but achieve good downstream performance.",
        "labels": "PUR"
    },
    {
        "idx": 103,
        "text": "To achieve this, we regularize the fine-tuning process with L1 distance and explore the subnetwork structure (what we refer to as the dominant winning ticket).",
        "labels": "MTD"
    },
    {
        "idx": 103,
        "text": "Empirically, we show that (a) the dominant winning ticket can achieve performance that is comparable with that of the full-parameter model, (b) the dominant winning ticket is transferable across different tasks, (c) and the dominant winning ticket has a natural structure within each parameter matrix.",
        "labels": "RST"
    },
    {
        "idx": 103,
        "text": "Strikingly, we find that a dominant winning ticket that takes up 0.05% of the parameters can already achieve satisfactory performance, indicating that the PLM is significantly reducible during fine-tuning.",
        "labels": "CLN"
    },
    {
        "idx": 104,
        "text": "This paper presents the first Thai Nested Named Entity Recognition (N-NER) dataset",
        "labels": "BAC"
    },
    {
        "idx": 104,
        "text": "Thai N-NER consists of 264,798 mentions, 104 classes, and a maximum depth of 8 layers obtained from 4,894 documents in the domains of news articles and restaurant reviews",
        "labels": "BAC"
    },
    {
        "idx": 104,
        "text": "Our work, to the best of our knowledge, presents the largest non-English N-NER dataset and the first non-English one with fine-grained classes.",
        "labels": "CTN"
    },
    {
        "idx": 104,
        "text": "To understand the new challenges our proposed dataset brings to the field,",
        "labels": "PUR"
    },
    {
        "idx": 104,
        "text": "we conduct an experimental study on (i) cutting edge N-NER models with the state-of-the-art accuracy in English and (ii) baseline methods based on well-known language model architectures.",
        "labels": "MTD"
    },
    {
        "idx": 104,
        "text": "From the experimental results, we obtained two key findings.",
        "labels": "RST"
    },
    {
        "idx": 104,
        "text": "First, all models produced poor F1 scores in the tail region of the class distribution.",
        "labels": "RST"
    },
    {
        "idx": 104,
        "text": "There is little or no performance improvement provided by these models with respect to the baseline methods with our Thai dataset.",
        "labels": "RST"
    },
    {
        "idx": 104,
        "text": "These findings suggest that further investigation is required to make a multilingual N-NER solution that works well across different languages.",
        "labels": "CLN"
    },
    {
        "idx": 105,
        "text": "The retriever-reader pipeline has shown promising performance in open-domain QA but suffers from a very slow inference speed.",
        "labels": "BAC"
    },
    {
        "idx": 105,
        "text": "Recently proposed question retrieval models tackle this problem by indexing question-answer pairs and searching for similar questions.",
        "labels": "BAC"
    },
    {
        "idx": 105,
        "text": "These models have shown a significant increase in inference speed, but at the cost of lower QA performance compared to the retriever-reader models.",
        "labels": "GAP"
    },
    {
        "idx": 105,
        "text": "This paper proposes a two-step question retrieval model, SQuID (Sequential Question-Indexed Dense retrieval) and distant supervision for training.",
        "labels": "PUR"
    },
    {
        "idx": 105,
        "text": "SQuID uses two bi-encoders for question retrieval.",
        "labels": "MTD"
    },
    {
        "idx": 105,
        "text": "The first-step retriever selects top-k similar questions, and the second-step retriever finds the most similar question from the top-k questions.",
        "labels": "MTD"
    },
    {
        "idx": 105,
        "text": "We evaluate the performance and the computational efficiency of SQuID.",
        "labels": "MTD"
    },
    {
        "idx": 105,
        "text": "The results show that SQuID significantly increases the performance of existing question retrieval models with a negligible loss on inference speed.",
        "labels": "RST"
    },
    {
        "idx": 106,
        "text": "Analysis of vision-and-language models has revealed their brittleness under linguistic phenomena such as paraphrasing, negation, textual entailment, and word substitutions with synonyms or antonyms",
        "labels": "BAC"
    },
    {
        "idx": 106,
        "text": "While data augmentation techniques have been designed to mitigate against these failure modes, methods that can integrate this knowledge into the training pipeline remain under-explored",
        "labels": "GAP"
    },
    {
        "idx": 106,
        "text": "In this paper, we present SDRO, a model-agnostic method that utilizes a set linguistic transformations in a distributed robust optimization setting, along with an ensembling technique to leverage these transformations during inference",
        "labels": "GAP"
    },
    {
        "idx": 106,
        "text": "Experiments on benchmark datasets with images (NLVR^2) and video (VIOLIN) demonstrate performance improvements as well as robustness to adversarial attacks",
        "labels": "RST"
    },
    {
        "idx": 106,
        "text": "Experiments on binary VQA explore the generalizability of this method to other V{&L tasks.",
        "labels": "RST"
    },
    {
        "idx": 107,
        "text": "Commonsense inference poses a unique challenge to reason and generate the physical, social, and causal conditions of a given event.",
        "labels": "BAC"
    },
    {
        "idx": 107,
        "text": "Existing approaches to commonsense inference utilize commonsense transformers, which are large-scale language models that learn commonsense knowledge graphs.",
        "labels": "BAC"
    },
    {
        "idx": 107,
        "text": "However, they suffer from a lack of coverage and expressive diversity of the graphs, resulting in a degradation of the representation quality.",
        "labels": "GAP"
    },
    {
        "idx": 107,
        "text": "In this paper, we focus on addressing missing relations in commonsense knowledge graphs, and propose a novel contrastive learning framework called SOLAR.",
        "labels": "PUR"
    },
    {
        "idx": 107,
        "text": "Our framework contrasts sets of semantically similar and dissimilar events, learning richer inferential knowledge compared to existing approaches.",
        "labels": "MTD"
    },
    {
        "idx": 107,
        "text": "Empirical results demonstrate the efficacy of SOLAR in commonsense inference of diverse commonsense knowledge graphs.",
        "labels": "RST"
    },
    {
        "idx": 107,
        "text": "Specifically, SOLAR outperforms the state-of-the-art commonsense transformer on commonsense inference with ConceptNet by 1.84% on average among 8 automatic evaluation metrics.",
        "labels": "RST"
    },
    {
        "idx": 107,
        "text": "In-depth analysis of SOLAR sheds light on the effects of the missing relations utilized in learning commonsense knowledge graphs",
        "labels": "CLN"
    },
    {
        "idx": 108,
        "text": "Natural Language Inference (NLI) datasets contain examples with highly ambiguous labels due to its subjectivity.",
        "labels": "BAC"
    },
    {
        "idx": 108,
        "text": "Several recent efforts have been made to acknowledge and embrace the existence of ambiguity, and explore how to capture the human disagreement distribution.",
        "labels": "BAC"
    },
    {
        "idx": 108,
        "text": "In contrast with directly learning from gold ambiguity labels, relying on special resource,",
        "labels": "BAC"
    },
    {
        "idx": 108,
        "text": "we argue that the model has naturally captured the human ambiguity distribution as long as it's calibrated, i.e. the predictive probability can reflect the true correctness likelihood.",
        "labels": "PUR"
    },
    {
        "idx": 108,
        "text": "Our experiments show that when model is well-calibrated, either by label smoothing or temperature scaling, it can obtain competitive performance as prior work, on both divergence scores between predictive probability and the true human opinion distribution, and the accuracy.",
        "labels": "RST"
    },
    {
        "idx": 108,
        "text": "This reveals the overhead of collecting gold ambiguity labels can be cut, by broadly solving how to calibrate the NLI network.",
        "labels": "CLN"
    },
    {
        "idx": 109,
        "text": "To maximize the accuracy and increase the overall acceptance of text classifiers, we propose a framework for the efficient, in-operation moderation of classifiers' output.",
        "labels": "PUR"
    },
    {
        "idx": 109,
        "text": "Our framework focuses on use cases in which F1-scores of modern Neural Networks classifiers (ca. 90%) are still inapplicable in practice.",
        "labels": "MTD"
    },
    {
        "idx": 109,
        "text": "We suggest a semi-automated approach that uses prediction uncertainties to pass unconfident, probably incorrect classifications to human moderators.",
        "labels": "MTD"
    },
    {
        "idx": 109,
        "text": "To minimize the workload, we limit the human moderated data to the point where the accuracy gains saturate and further human effort does not lead to substantial improvements.",
        "labels": "RST"
    },
    {
        "idx": 109,
        "text": "A series of benchmarking experiments based on three different datasets and three state-of-the-art classifiers show that our framework can improve the classification F1-scores by 5.1 to 11.2% (up to approx. 98 to 99%), while reducing the moderation load up to 73.3% compared to a random moderation.",
        "labels": "RST"
    },
    {
        "idx": 110,
        "text": "It has been the norm for a long time to evaluate automated summarization tasks using the popular ROUGE metric.",
        "labels": "BAC"
    },
    {
        "idx": 110,
        "text": "Although several studies in the past have highlighted the limitations of ROUGE, researchers have struggled to reach a consensus on a better alternative until today.",
        "labels": "GAP"
    },
    {
        "idx": 110,
        "text": "One major limitation of the traditional ROUGE metric is the lack of semantic understanding (relies on direct overlap of n-grams).",
        "labels": "GAP"
    },
    {
        "idx": 110,
        "text": "In this paper, we exclusively focus on the extractive summarization task and propose a semantic-aware nCG (normalized cumulative gain)-based evaluation metric (called Sem-nCG) for evaluating this task.",
        "labels": "PUR"
    },
    {
        "idx": 110,
        "text": "One fundamental contribution of the paper is that it demonstrates how we can generate more reliable semantic-aware ground truths for evaluating extractive summarization tasks without any additional human intervention.",
        "labels": "CTN"
    },
    {
        "idx": 110,
        "text": "To the best of our knowledge, this work is the first of its kind.",
        "labels": "CTN"
    },
    {
        "idx": 110,
        "text": "We have conducted extensive experiments with this new metric using the widely used CNN/DailyMail dataset.",
        "labels": "MTD"
    },
    {
        "idx": 110,
        "text": "Experimental results show that the new Sem-nCG metric is indeed semantic-aware, shows higher correlation with human judgement (more reliable) and yields a large number of disagreements with the original ROUGE metric (suggesting that ROUGE often leads to inaccurate conclusions also verified by humans).",
        "labels": "CLN"
    },
    {
        "idx": 111,
        "text": "The extreme multi-label classification (XMC) task aims at tagging content with a subset of labels from an extremely large label set.",
        "labels": "BAC"
    },
    {
        "idx": 111,
        "text": "The label vocabulary is typically defined in advance by domain experts and assumed to capture all necessary tags.",
        "labels": "BAC"
    },
    {
        "idx": 111,
        "text": "However in real world scenarios this label set, although large, is often incomplete and experts frequently need to refine it.",
        "labels": "GAP"
    },
    {
        "idx": 111,
        "text": "To develop systems that simplify this process, we introduce the task of open vocabulary XMC (OXMC):",
        "labels": "PUR"
    },
    {
        "idx": 111,
        "text": "given a piece of content, predict a set of labels, some of which may be outside of the known tag set.",
        "labels": "PUR"
    },
    {
        "idx": 111,
        "text": "Hence, in addition to not having training data for some labels-as is the case in zero-shot classification-models need to invent some labels on-thefly.",
        "labels": "PUR"
    },
    {
        "idx": 111,
        "text": "We propose GROOV, a fine-tuned seq2seq model for OXMC that generates the set of labels as a flat sequence and is trained using a novel loss independent of predicted label order.",
        "labels": "MTD"
    },
    {
        "idx": 111,
        "text": "We show the efficacy of the approach, experimenting with popular XMC datasets for which GROOV is able to predict meaningful labels outside the given vocabulary while performing on par with state-of-the-art solutions for known labels.",
        "labels": "MTD"
    },
    {
        "idx": 112,
        "text": "Few-shot named entity recognition (NER) systems aim at recognizing novel-class named entities based on only a few labeled examples.",
        "labels": "BAC"
    },
    {
        "idx": 112,
        "text": "In this paper, we present a decomposed meta-learning approach which addresses the problem of few-shot NER",
        "labels": "PUR"
    },
    {
        "idx": 112,
        "text": "by sequentially tackling few-shot span detection and few-shot entity typing using meta-learning.",
        "labels": "MTD"
    },
    {
        "idx": 112,
        "text": "In particular, we take the few-shot span detection as a sequence labeling problem and train the span detector by introducing the model-agnostic meta-learning (MAML) algorithm to find a good model parameter initialization that could fast adapt to new entity classes.",
        "labels": "MTD"
    },
    {
        "idx": 112,
        "text": " For few-shot entity typing, we propose MAML-ProtoNet, i.e., MAML-enhanced prototypical networks to find a good embedding space that can better distinguish text span representations from different entity classes.",
        "labels": "MTD"
    },
    {
        "idx": 112,
        "text": "Extensive experiments on various benchmarks show that our approach achieves superior performance over prior methods.",
        "labels": "RST"
    },
    {
        "idx": 113,
        "text": "Generating natural and informative texts has been a long-standing problem in NLP. Much effort has been dedicated into incorporating pre-trained language models (PLMs) with various open-world knowledge, such as knowledge graphs or wiki pages.",
        "labels": "BAC"
    },
    {
        "idx": 113,
        "text": " However, their ability to access and manipulate the task-specific knowledge is still limited on downstream tasks, as this type of knowledge is usually not well covered in PLMs and is hard to acquire.",
        "labels": "GAP"
    },
    {
        "idx": 113,
        "text": "To address the problem, we propose augmenting TExt Generation via Task-specific and Open-world Knowledge (TegTok) in a unified framework",
        "labels": "PUR"
    },
    {
        "idx": 113,
        "text": "Our model selects knowledge entries from two types of knowledge sources through dense retrieval and then injects them into the input encoding and output decoding stages respectively on the basis of PLMs.",
        "labels": "MTD"
    },
    {
        "idx": 113,
        "text": "With the help of these two types of knowledge, our model can learn what and how to generate.",
        "labels": "RST"
    },
    {
        "idx": 113,
        "text": "Experiments on two text generation tasks of dialogue generation and question generation, and on two datasets show that our method achieves better performance than various baseline models.",
        "labels": "CLN"
    },
    {
        "idx": 114,
        "text": "Emotion recognition in conversation (ERC) aims to analyze the speaker's state and identify their emotion in the conversation.",
        "labels": "BAC"
    },
    {
        "idx": 114,
        "text": "Recent works in ERC focus on context modeling but ignore the representation of contextual emotional tendency.",
        "labels": "GAP"
    },
    {
        "idx": 114,
        "text": "In order to extract multi-modal information and the emotional tendency of the utterance effectively, we propose a new structure named Emoformer to extract multi-modal emotion vectors from different modalities and fuse them with sentence vector to be an emotion capsule.",
        "labels": "PUR"
    },
    {
        "idx": 114,
        "text": "Furthermore, we design an end-to-end ERC model called EmoCaps, which extracts emotion vectors through the Emoformer structure and obtain the emotion classification results from a context analysis model.",
        "labels": "MTD"
    },
    {
        "idx": 114,
        "text": "Through the experiments with two benchmark datasets, our model shows better performance than the existing state-of-the-art models.",
        "labels": "RST"
    },
    {
        "idx": 115,
        "text": " Logical reasoning of text requires identifying critical logical structures in the text and performing inference over them.",
        "labels": "BAC"
    },
    {
        "idx": 115,
        "text": "Existing methods for logical reasoning mainly focus on contextual semantics of text while struggling to explicitly model the logical inference process.",
        "labels": "GAP"
    },
    {
        "idx": 115,
        "text": "In this paper, we not only put forward a logic-driven context extension framework but also propose a logic-driven data augmentation algorithm. ",
        "labels": "PUR"
    },
    {
        "idx": 115,
        "text": "The former follows a three-step reasoning paradigm, and each step is respectively to extract logical expressions as elementary reasoning units, symbolically infer the implicit expressions following equivalence laws and extend the context to validate the options.",
        "labels": "MTD"
    },
    {
        "idx": 115,
        "text": "The latter augments literally similar but logically different instances and incorporates contrastive learning to better capture logical information, especially logical negative and conditional relationships.",
        "labels": "MTD"
    },
    {
        "idx": 115,
        "text": "We conduct experiments on two benchmark datasets, ReClor and LogiQA.",
        "labels": "MTD"
    },
    {
        "idx": 115,
        "text": "The results show that our method achieves state-of-the-art performance on both datasets, and even surpasses human performance on the ReClor dataset.",
        "labels": "RST"
    },
    {
        "idx": 116,
        "text": " Toxic span detection is the task of recognizing offensive spans in a text snippet.",
        "labels": "BAC"
    },
    {
        "idx": 116,
        "text": "Although there has been prior work on classifying text snippets as offensive or not, the task of recognizing spans responsible for the toxicity of a text is not explored yet.",
        "labels": "GAP"
    },
    {
        "idx": 116,
        "text": "In this work, we introduce a novel multi-task framework for toxic span detection in which the model seeks to simultaneously predict offensive words and opinion phrases to leverage their inter-dependencies and improve the performance.",
        "labels": "PUR"
    },
    {
        "idx": 116,
        "text": "Moreover, we introduce a novel regularization mechanism to encourage the consistency of the model predictions across similar inputs for toxic span detection.",
        "labels": "PUR"
    },
    {
        "idx": 116,
        "text": "Our extensive experiments demonstrate the effectiveness of the proposed model compared to strong baselines.",
        "labels": "RST"
    },
    {
        "idx": 117,
        "text": "Relational triple extraction is a critical task for constructing knowledge graphs.",
        "labels": "BAC"
    },
    {
        "idx": 117,
        "text": "Existing methods focused on learning text patterns from explicit relational mentions.",
        "labels": "BAC"
    },
    {
        "idx": 117,
        "text": "However, they usually suffered from ignoring relational reasoning patterns, thus failed to extract the implicitly implied triples.",
        "labels": "GAP"
    },
    {
        "idx": 117,
        "text": "Fortunately, the graph structure of a sentence's relational triples can help find multi-hop reasoning paths.",
        "labels": "GAP"
    },
    {
        "idx": 117,
        "text": "Moreover, the type inference logic through the paths can be captured with the sentence's supplementary relational expressions that represent the real-world conceptual meanings of the paths' composite relations.",
        "labels": "BAC"
    },
    {
        "idx": 117,
        "text": "In this paper, we propose a unified framework to learn the relational reasoning patterns for this task.",
        "labels": "PUR"
    },
    {
        "idx": 117,
        "text": "To identify multi-hop reasoning paths, we construct a relational graph from the sentence (text-to-graph generation) and apply multi-layer graph convolutions to it.",
        "labels": "MTD"
    },
    {
        "idx": 117,
        "text": "To capture the relation type inference logic of the paths, we propose to understand the unlabeled conceptual expressions by reconstructing the sentence from the relational graph (graph-to-text generation) in a self-supervised manner.",
        "labels": "MTD"
    },
    {
        "idx": 117,
        "text": "Experimental results on several benchmark datasets demonstrate the effectiveness of our method.",
        "labels": "RST"
    },
    {
        "idx": 118,
        "text": "Event Argument Extraction (EAE) is one of the sub-tasks of event extraction, aiming to recognize the role of each entity mention toward a specific event trigger.",
        "labels": "BAC"
    },
    {
        "idx": 118,
        "text": "Despite the success of prior works in sentence-level EAE, the document-level setting is less explored.",
        "labels": "GAP"
    },
    {
        "idx": 118,
        "text": "In particular, whereas syntactic structures of sentences have been shown to be effective for sentence-level EAE, prior document-level EAE models totally ignore syntactic structures for documents.",
        "labels": "GAP"
    },
    {
        "idx": 118,
        "text": "Hence, in this work, we study the importance of syntactic structures in document-level EAE.",
        "labels": "PUR"
    },
    {
        "idx": 118,
        "text": "Specifically, we propose to employ Optimal Transport (OT) to induce structures of documents based on sentence-level syntactic structures and tailored to EAE task.",
        "labels": "MTD"
    },
    {
        "idx": 118,
        "text": "Furthermore, we propose a novel regularization technique to explicitly constrain the contributions of unrelated context words in the final prediction for EAE.",
        "labels": "MTD"
    },
    {
        "idx": 118,
        "text": "We perform extensive experiments on the benchmark document-level EAE dataset RAMS that leads to the state-of-the-art performance.",
        "labels": "RST"
    },
    {
        "idx": 118,
        "text": "Moreover, our experiments on the ACE 2005 dataset reveals the effectiveness of the proposed model in the sentence-level EAE by establishing new state-of-the-art results.",
        "labels": "RST"
    },
    {
        "idx": 119,
        "text": "Augmentation of task-oriented dialogues has followed standard methods used for plain-text such as back-translation, word-level manipulation, and paraphrasing despite its richly annotated structure",
        "labels": "BAC"
    },
    {
        "idx": 119,
        "text": "In this work, we introduce an augmentation framework that utilizes belief state annotations to match turns from various dialogues and form new synthetic dialogues in a bottom-up manner.",
        "labels": "PUR"
    },
    {
        "idx": 119,
        "text": "Unlike other augmentation strategies, it operates with as few as five examples. ",
        "labels": "MTD"
    },
    {
        "idx": 119,
        "text": "Our augmentation strategy yields significant improvements when both adapting a DST model to a new domain, and when adapting a language model to the DST task, on evaluations with TRADE and TOD-BERT models",
        "labels": "RST"
    },
    {
        "idx": 119,
        "text": "Further analysis shows that our model performs better on seen values during training, and it is also more robust to unseen values",
        "labels": "RST"
    },
    {
        "idx": 119,
        "text": "We conclude that exploiting belief state annotations enhances dialogue augmentation and results in improved models in n-shot training scenarios.",
        "labels": "CLN"
    },
    {
        "idx": 120,
        "text": "Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart.",
        "labels": "BAC"
    },
    {
        "idx": 120,
        "text": "It aims to extract relations from multiple sentences at once",
        "labels": "BAC"
    },
    {
        "idx": 120,
        "text": "In this paper, we propose a semi-supervised framework for DocRE with three novel components.",
        "labels": "PUR"
    },
    {
        "idx": 120,
        "text": "Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations.",
        "labels": "MTD"
    },
    {
        "idx": 120,
        "text": "Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE.",
        "labels": "MTD"
    },
    {
        "idx": 120,
        "text": "Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data.",
        "labels": "MTD"
    },
    {
        "idx": 120,
        "text": "We conducted experiments on two DocRE datasets.",
        "labels": "MTD"
    },
    {
        "idx": 120,
        "text": "Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign{_F1 score on the DocRED leaderboard.",
        "labels": "RST"
    },
    {
        "idx": 121,
        "text": "In typical machine learning systems, an estimate of the probability of the prediction is used to assess the system's confidence in the prediction",
        "labels": "BAC"
    },
    {
        "idx": 121,
        "text": "This confidence measure is usually uncalibrated; i.e. the system's confidence in the prediction does not match the true probability of the predicted output",
        "labels": "GAP"
    },
    {
        "idx": 121,
        "text": "In this paper, we present an investigation into calibrating open setting machine reading systemssuch as open-domain question answering and claim verification systems",
        "labels": "PUR"
    },
    {
        "idx": 121,
        "text": "We show that calibrating such complex systems which contain discrete retrieval and deep reading components is challenging and current calibration techniques fail to scale to these settings.",
        "labels": "GAP"
    },
    {
        "idx": 121,
        "text": "We propose simple extensions to existing calibration approaches that allows us to adapt them to these settings.",
        "labels": "MTD"
    },
    {
        "idx": 121,
        "text": "Our experimental results reveal that the approach works well, and can be useful to selectively predict answers when question answering systems are posed with unanswerable or out-of-the-training distribution questions.",
        "labels": "RST"
    },
    {
        "idx": 122,
        "text": "Most of the existing defense methods improve the adversarial robustness by making the models adapt to the training set augmented with some adversarial examples. ",
        "labels": "BAC"
    },
    {
        "idx": 122,
        "text": "However, the augmented adversarial examples may not be natural, which might distort the training distribution, resulting in inferior performance both in clean accuracy and adversarial robustness.",
        "labels": "GAP"
    },
    {
        "idx": 122,
        "text": "In this study, we explore the feasibility of introducing a reweighting mechanism to calibrate the training distribution to obtain robust models.",
        "labels": "PUR"
    },
    {
        "idx": 122,
        "text": "We propose to train text classifiers by a sample reweighting method in which the example weights are learned to minimize the loss of a validation set mixed with the clean examples and their adversarial ones in an online learning manner.",
        "labels": "MTD"
    },
    {
        "idx": 122,
        "text": "Through extensive experiments, we show that there exists a reweighting mechanism to make the models more robust against adversarial attacks without the need to craft the adversarial examples for the entire training set.",
        "labels": "RST"
    },
    {
        "idx": 123,
        "text": "We present state-of-the-art results on morphosyntactic tagging across different varieties of Arabic using fine-tuned pre-trained transformer language models.",
        "labels": "PUR"
    },
    {
        "idx": 123,
        "text": "Our models consistently outperform existing systems in Modern Standard Arabic and all the Arabic dialects we study, achieving 2.6% absolute improvement over the previous state-of-the-art in Modern Standard Arabic, 2.8% in Gulf, 1.6% in Egyptian, and 8.3% in Levantine.",
        "labels": "RST"
    },
    {
        "idx": 123,
        "text": "We explore different training setups for fine-tuning pre-trained transformer language models, including training data size, the use of external linguistic resources, and the use of annotated data from other dialects in a low-resource scenario.",
        "labels": "MTD"
    },
    {
        "idx": 123,
        "text": "Our results show that strategic fine-tuning using datasets from other high-resource dialects is beneficial for a low-resource dialect.",
        "labels": "CLN"
    },
    {
        "idx": 123,
        "text": "Additionally, we show that high-quality morphological analyzers as external linguistic resources are beneficial especially in low-resource settings.",
        "labels": "CLN"
    },
    {
        "idx": 124,
        "text": "Recently, there has been a trend to investigate the factual knowledge captured by Pre-trained Language Models (PLMs).",
        "labels": "BAC"
    },
    {
        "idx": 124,
        "text": "Many works show the PLMs' ability to fill in the missing factual words in cloze-style prompts such as Dante was born in [MASK]",
        "labels": "BAC"
    },
    {
        "idx": 124,
        "text": "However, it is still a mystery how PLMs generate the results correctly: relying on effective clues or shortcut patterns?",
        "labels": "GAP"
    },
    {
        "idx": 124,
        "text": "We try to answer this question by a causal-inspired analysis that quantitatively measures and evaluates the word-level patterns that PLMs depend on to generate the missing words.",
        "labels": "PUR"
    },
    {
        "idx": 124,
        "text": "We check the words that have three typical associations with the missing words: knowledge-dependent, positionally close, and highly co-occurred.",
        "labels": "MTD"
    },
    {
        "idx": 124,
        "text": "Our analysis shows: (1) PLMs generate the missing factual words more by the positionally close and highly co-occurred words than the knowledge-dependent words;",
        "labels": "RST"
    },
    {
        "idx": 124,
        "text": "(2) the dependence on the knowledge-dependent words is more effective than the positionally close and highly co-occurred words.",
        "labels": "RST"
    },
    {
        "idx": 124,
        "text": " Accordingly, we conclude that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.",
        "labels": "CLN"
    },
    {
        "idx": 125,
        "text": "Popular language models (LMs) struggle to capture knowledge about rare tail facts and entities.",
        "labels": "BAC"
    },
    {
        "idx": 125,
        "text": "Since widely used systems such as search and personal-assistants must support the long tail of entities that users ask about, there has been significant effort towards enhancing these base LMs with factual knowledge.",
        "labels": "BAC"
    },
    {
        "idx": 125,
        "text": "We observe proposed methods typically start with a base LM and data that has been annotated with entity metadata, then change the model, by modifying the architecture or introducing auxiliary loss terms to better capture entity knowledge.",
        "labels": "BAC"
    },
    {
        "idx": 125,
        "text": "In this work, we question this typical process and ask to what extent can we match the quality of model modifications, with a simple alternative: using a base LM and only changing the data.",
        "labels": "PUR"
    },
    {
        "idx": 125,
        "text": "We propose metadata shaping, a method which inserts substrings corresponding to the readily available entity metadata, e.g. types and descriptions, into examples at train and inference time based on mutual information",
        "labels": "MTD"
    },
    {
        "idx": 125,
        "text": "Despite its simplicity, metadata shaping is quite effective.",
        "labels": "MTD"
    },
    {
        "idx": 125,
        "text": "On standard evaluation benchmarks for knowledge-enhanced LMs, the method exceeds the base-LM baseline by an average of 4.3 F1 points and achieves state-of-the-art results.",
        "labels": "RST"
    },
    {
        "idx": 125,
        "text": "We further show the gains are on average 4.4x larger for the slice of examples containing tail vs. popular entities",
        "labels": "RST"
    },
    {
        "idx": 126,
        "text": "We study how to enhance text representation via textual commonsense.",
        "labels": "PUR"
    },
    {
        "idx": 126,
        "text": "We point out that commonsense has the nature of domain discrepancy.",
        "labels": "BAC"
    },
    {
        "idx": 126,
        "text": "Namely, commonsense has different data formats and is domain-independent from the downstream task.",
        "labels": "BAC"
    },
    {
        "idx": 126,
        "text": "This nature brings challenges to introducing commonsense in general text understanding tasks.",
        "labels": "BAC"
    },
    {
        "idx": 126,
        "text": "A typical method of introducing textual knowledge is continuing pre-training over the commonsense corpus",
        "labels": "BAC"
    },
    {
        "idx": 126,
        "text": "However, it will cause catastrophic forgetting to the downstream task due to the domain discrepancy.",
        "labels": "GAP"
    },
    {
        "idx": 126,
        "text": "In addition, previous methods of directly using textual descriptions as extra input information cannot apply to large-scale commonsense",
        "labels": "GAP"
    },
    {
        "idx": 126,
        "text": "In this paper, we propose to use large-scale out-of-domain commonsense to enhance text representation.",
        "labels": "PUR"
    },
    {
        "idx": 126,
        "text": "In order to effectively incorporate the commonsense, we proposed OK-Transformer (Out-of-domain Knowledge enhanced Transformer).",
        "labels": "MTD"
    },
    {
        "idx": 126,
        "text": "OK-Transformer effectively integrates commonsense descriptions and enhances them to the target text representation.",
        "labels": "RST"
    },
    {
        "idx": 126,
        "text": "In addition, OK-Transformer can adapt to the Transformer-based language models (e.g. BERT, RoBERTa) for free, without pre-training on large-scale unsupervised corpora.",
        "labels": "RST"
    },
    {
        "idx": 126,
        "text": "We have verified the effectiveness of OK-Transformer in multiple applications such as commonsense reasoning, general text classification, and low-resource commonsense settings.",
        "labels": "CLN"
    },
    {
        "idx": 127,
        "text": "Recent researches show that multi-criteria resources and n-gram features are beneficial to Chinese Word Segmentation (CWS).",
        "labels": "BAC"
    },
    {
        "idx": 127,
        "text": "However, these methods rely heavily on such additional information mentioned above and focus less on the model itself.",
        "labels": "GAP"
    },
    {
        "idx": 127,
        "text": "We thus propose a novel neural framework, named Weighted self Distillation for Chinese word segmentation (WeiDC).",
        "labels": "PUR"
    },
    {
        "idx": 127,
        "text": "The framework, which only requires unigram features, adopts self-distillation technology with four hand-crafted weight modules and two teacher models configurations.",
        "labels": "MTD"
    },
    {
        "idx": 127,
        "text": "Experiment results show that WeiDC can make use of character features to learn contextual knowledge and successfully achieve state-of-the-art or competitive performance in terms of strictly closed test settings on SIGHAN Bakeoff benchmark datasets.",
        "labels": "RST"
    },
    {
        "idx": 127,
        "text": "Moreover, further experiments and analyses also demonstrate the robustness of WeiDC",
        "labels": "RST"
    },
    {
        "idx": 127,
        "text": "Source codes of this paper are available on Github.",
        "labels": "CTN"
    },
    {
        "idx": 128,
        "text": "The vast majority of text transformation techniques in NLP are inherently limited in their ability to expand input space coverage due to an implicit constraint to preserve the original class label.",
        "labels": "GAP"
    },
    {
        "idx": 128,
        "text": "In this work, we propose the notion of sibylvariance (SIB) to describe the broader set of transforms that relax the label-preserving constraint, knowably vary the expected class, and lead to significantly more diverse input distributions.",
        "labels": "PUR"
    },
    {
        "idx": 128,
        "text": "We offer a unified framework to organize all data transformations, including two types of SIB: (1) Transmutations convert one discrete kind into another, (2) Mixture Mutations blend two or more classes together.",
        "labels": "MTD"
    },
    {
        "idx": 128,
        "text": "To explore the role of sibylvariance within NLP, we implemented 41 text transformations, including several novel techniques like Concept2Sentence and SentMix.",
        "labels": "MTD"
    },
    {
        "idx": 128,
        "text": "Sibylvariance also enables a unique form of adaptive training that generates new input mixtures for the most confused class pairs, challenging the learner to differentiate with greater nuance.",
        "labels": "MTD"
    },
    {
        "idx": 128,
        "text": "Our experiments on six benchmark datasets strongly support the efficacy of sibylvariance for generalization performance, defect detection, and adversarial robustness.",
        "labels": "CLN"
    },
    {
        "idx": 129,
        "text": " Domain Adaptation (DA) of Neural Machine Translation (NMT) model often relies on a pre-trained general NMT model which is adapted to the new domain on a sample of in-domain parallel data.",
        "labels": "BAC"
    },
    {
        "idx": 129,
        "text": "Without parallel data, there is no way to estimate the potential benefit of DA, nor the amount of parallel samples it would require.",
        "labels": "BAC"
    },
    {
        "idx": 129,
        "text": "It is however a desirable functionality that could help MT practitioners to make an informed decision before investing resources in dataset creation.",
        "labels": "GAP"
    },
    {
        "idx": 129,
        "text": "We propose a Domain adaptation Learning Curve prediction (DaLC) model that predicts prospective DA performance",
        "labels": "PUR"
    },
    {
        "idx": 129,
        "text": "based on in-domain monolingual samples in the source language.",
        "labels": "MTD"
    },
    {
        "idx": 129,
        "text": "Our model relies on the NMT encoder representations combined with various instance and corpus-level features.",
        "labels": "MTD"
    },
    {
        "idx": 129,
        "text": "We demonstrate that instance-level is better able to distinguish between different domains compared to corpus-level frameworks proposed in previous studies",
        "labels": "RST"
    },
    {
        "idx": 129,
        "text": "Finally, we perform in-depth analyses of the results highlighting the limitations of our approach, and provide directions for future research.",
        "labels": "CLN"
    },
    {
        "idx": 130,
        "text": "Training giant models from scratch for each complex task is resource- and data-inefficient.",
        "labels": "BAC"
    },
    {
        "idx": 130,
        "text": "To help develop models that can leverage existing systems, we propose a new challenge: Learning to solve complex tasks by communicating with existing agents (or models) in natural language.",
        "labels": "PUR"
    },
    {
        "idx": 130,
        "text": "We design a synthetic benchmark, CommaQA, with three complex reasoning tasks (explicit, implicit, numeric) designed to be solved by communicating with existing QA agents.",
        "labels": "MTD"
    },
    {
        "idx": 130,
        "text": "For instance, using text and table QA agents to answer questions such as Who had the longest javelin throw from USA?.",
        "labels": "MTD"
    },
    {
        "idx": 130,
        "text": "We show that black-box models struggle to learn this task from scratch (accuracy under 50%) even with access to each agent's knowledge and gold facts supervision.",
        "labels": "RST"
    },
    {
        "idx": 130,
        "text": "In contrast, models that learn to communicate with agents outperform black-box models, reaching scores of 100% when given gold decomposition supervision.",
        "labels": "RST"
    },
    {
        "idx": 130,
        "text": "However, we show that the challenge of learning to solve complex tasks by communicating with existing agents without relying on any auxiliary supervision or data still remains highly elusive.",
        "labels": "CLN"
    },
    {
        "idx": 130,
        "text": "We will release CommaQA, along with a compositional generalization test split, to advance research in this direction.",
        "labels": "CTN"
    },
    {
        "idx": 131,
        "text": "In multimodal machine learning, additive late-fusion is a straightforward approach to combine the feature representations from different modalities, in which the final prediction can be formulated as the sum of unimodal predictions.",
        "labels": "BAC"
    },
    {
        "idx": 131,
        "text": "While it has been found that certain late-fusion models can achieve competitive performance with lower computational costs compared to complex multimodal interactive models, how to effectively search for a good late-fusion model is still an open question.",
        "labels": "GAP"
    },
    {
        "idx": 131,
        "text": "Moreover, for different modalities, the best unimodal models may work under significantly different learning rates due to the nature of the modality and the computational flow of the model; thus, selecting a global learning rate for late-fusion models can result in a vanishing gradient for some modalities.",
        "labels": "GAP"
    },
    {
        "idx": 131,
        "text": "To help address these issues, we propose a Modality-Specific Learning Rate (MSLR) method to effectively build late-fusion multimodal models from fine-tuned unimodal models.",
        "labels": "PUR"
    },
    {
        "idx": 131,
        "text": "We investigate three different strategies to assign learning rates to different modalities.",
        "labels": "MTD"
    },
    {
        "idx": 131,
        "text": "Our experiments show that MSLR outperforms global learning rates on multiple tasks and settings, and enables the models to effectively learn each modality.",
        "labels": "RST"
    },
    {
        "idx": 132,
        "text": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that aims to align aspects and corresponding sentiments for aspect-specific sentiment polarity inference.",
        "labels": "BAC"
    },
    {
        "idx": 132,
        "text": "It is challenging because a sentence may contain multiple aspects or complicated (e.g., conditional, coordinating, or adversative) relations.",
        "labels": "BAC"
    },
    {
        "idx": 132,
        "text": "Recently, exploiting dependency syntax information with graph neural networks has been the most popular trend.",
        "labels": "BAC"
    },
    {
        "idx": 132,
        "text": "Despite its success, methods that heavily rely on the dependency tree pose challenges in accurately modeling the alignment of the aspects and their words indicative of sentiment, since the dependency tree may provide noisy signals of unrelated associations (e.g., the conj relation between great and dreadful in Figure 2).",
        "labels": "GAP"
    },
    {
        "idx": 132,
        "text": "In this paper, to alleviate this problem, we propose a Bi-Syntax aware Graph Attention Network (BiSyn-GAT+).",
        "labels": "PUR"
    },
    {
        "idx": 132,
        "text": "Specifically, BiSyn-GAT+ fully exploits the syntax information (e.g., phrase segmentation and hierarchical structure) of the constituent tree of a sentence to model the sentiment-aware context of every single aspect (called intra-context) and the sentiment relations across aspects (called inter-context) for learning.",
        "labels": "MTD"
    },
    {
        "idx": 132,
        "text": "Experiments on four benchmark datasets demonstrate that BiSyn-GAT+ outperforms the state-of-the-art methods consistently.",
        "labels": "RST"
    },
    {
        "idx": 133,
        "text": " In this paper, we study pre-trained sequence-to-sequence models for a group of related languages, with a focus on Indic languages.",
        "labels": "PUR"
    },
    {
        "idx": 133,
        "text": "We present IndicBART, a multilingual, sequence-to-sequence pre-trained model focusing on 11 Indic languages and English.",
        "labels": "MTD"
    },
    {
        "idx": 133,
        "text": "IndicBART utilizes the orthographic similarity between Indic scripts to improve transfer learning between similar Indic languages.",
        "labels": "MTD"
    },
    {
        "idx": 133,
        "text": "We evaluate IndicBART on two NLG tasks: Neural Machine Translation (NMT) and extreme summarization.",
        "labels": "MTD"
    },
    {
        "idx": 133,
        "text": "Our experiments on NMT and extreme summarization show that a model specific to related languages like IndicBART is competitive with large pre-trained models like mBART50 despite being significantly smaller.",
        "labels": "RST"
    },
    {
        "idx": 133,
        "text": "It also performs well on very low-resource translation scenarios where languages are not included in pre-training or fine-tuning.",
        "labels": "RST"
    },
    {
        "idx": 133,
        "text": "Script sharing, multilingual training, and better utilization of limited model capacity contribute to the good performance of the compact IndicBART model.",
        "labels": "CLN"
    },
    {
        "idx": 134,
        "text": "We provide the first exploration of sentence embeddings from text-to-text transformers (T5) including the effects of scaling up sentence encoders to 11B parameters.",
        "labels": "BAC"
    },
    {
        "idx": 134,
        "text": "Sentence embeddings are broadly useful for language processing tasks.",
        "labels": "BAC"
    },
    {
        "idx": 134,
        "text": "While T5 achieves impressive performance on language tasks, it is unclear how to produce sentence embeddings from encoder-decoder models.",
        "labels": "GAP"
    },
    {
        "idx": 134,
        "text": "We investigate three methods to construct Sentence-T5 (ST5) models: two utilize only the T5 encoder and one using the full T5 encoder-decoder.",
        "labels": "MTD"
    },
    {
        "idx": 134,
        "text": "We establish a new sentence representation transfer benchmark, SentGLUE, which extends the SentEval toolkit to nine tasks from the GLUE benchmark.",
        "labels": "MTD"
    },
    {
        "idx": 134,
        "text": "Our encoder-only models outperform the previous best models on both SentEval and SentGLUE transfer tasks, including semantic textual similarity (STS).",
        "labels": "RST"
    },
    {
        "idx": 134,
        "text": "Scaling up ST5 from millions to billions of parameters shown to consistently improve performance.",
        "labels": "RST"
    },
    {
        "idx": 134,
        "text": "Finally, our encoder-decoder method achieves a new state-of-the-art on STS when using sentence embeddings.",
        "labels": "CLN"
    },
    {
        "idx": 135,
        "text": "Relation extraction (RE) is an important natural language processing task that predicts the relation between two given entities, where a good understanding of the contextual information is essential to achieve an outstanding model performance.",
        "labels": "BAC"
    },
    {
        "idx": 135,
        "text": "Among different types of contextual information, the auto-generated syntactic information (namely, word dependencies) has shown its effectiveness for the task.",
        "labels": "BAC"
    },
    {
        "idx": 135,
        "text": "However, most existing studies require modifications to the existing baseline architectures (e.g., adding new components, such as GCN,",
        "labels": "GAP"
    },
    {
        "idx": 135,
        "text": "on the top of an encoder) to leverage the syntactic information.",
        "labels": "GAP"
    },
    {
        "idx": 135,
        "text": "To offer an alternative solution, we propose to leverage syntactic information to improve RE by training a syntax-induced encoder on auto-parsed data through dependency masking.",
        "labels": "PUR"
    },
    {
        "idx": 135,
        "text": "Specifically, the syntax-induced encoder is trained by recovering the masked dependency connections and types in first, second, and third orders, which significantly differs from existing studies that train language models or word embeddings by predicting the context words along the dependency paths.",
        "labels": "MTD"
    },
    {
        "idx": 135,
        "text": "Experimental results on two English benchmark datasets, namely, ACE2005EN and SemEval 2010 Task 8 datasets, demonstrate the effectiveness of our approach for RE, where our approach outperforms strong baselines and achieve state-of-the-art results on both datasets.",
        "labels": "RST"
    },
    {
        "idx": 136,
        "text": "While fine-tuning pre-trained models for downstream classification is the conventional paradigm in NLP, often task-specific nuances may not get captured in the resultant models.",
        "labels": "GAP"
    },
    {
        "idx": 136,
        "text": "Specifically, for tasks that take two inputs and require the output to be invariant of the order of the inputs, inconsistency is often observed in the predicted labels or confidence scores",
        "labels": "GAP"
    },
    {
        "idx": 136,
        "text": "We highlight this model shortcoming and apply a consistency loss function to alleviate inconsistency in symmetric classification.",
        "labels": "PUR"
    },
    {
        "idx": 136,
        "text": "Our results show an improved consistency in predictions for three paraphrase detection datasets without a significant drop in the accuracy scores.",
        "labels": "RST"
    },
    {
        "idx": 136,
        "text": "We examine the classification performance of six datasets (both symmetric and non-symmetric) to showcase the strengths and limitations of our approach.",
        "labels": "CLN"
    },
    {
        "idx": 137,
        "text": " Generative commonsense reasoning (GCR) in natural language is to reason about the commonsense while generating coherent text.",
        "labels": "BAC"
    },
    {
        "idx": 137,
        "text": "Recent years have seen a surge of interest in improving the generation quality of commonsense reasoning tasks.",
        "labels": "BAC"
    },
    {
        "idx": 137,
        "text": "Nevertheless, these approaches have seldom investigated diversity in the GCR tasks, which aims to generate alternative explanations for a real-world situation or predict all possible outcomes",
        "labels": "GAP"
    },
    {
        "idx": 137,
        "text": "Diversifying GCR is challenging as it expects to generate multiple outputs that are not only semantically different but also grounded in commonsense knowledge.",
        "labels": "BAC"
    },
    {
        "idx": 137,
        "text": " In this paper, we propose MoKGE, a novel method that diversifies the generative reasoning by a mixture of expert (MoE) strategy on commonsense knowledge graphs (KG)",
        "labels": "PUR"
    },
    {
        "idx": 137,
        "text": "A set of knowledge experts seek diverse reasoning on KG to encourage various generation outputs.",
        "labels": "MTD"
    },
    {
        "idx": 137,
        "text": "Empirical experiments demonstrated that MoKGE can significantly improve the diversity while achieving on par performance on accuracy on two GCR benchmarks, based on both automatic and human evaluations.",
        "labels": "RST"
    },
    {
        "idx": 138,
        "text": "Pre-trained language models (PLMs) aim to learn universal language representations by conducting self-supervised training tasks on large-scale corpora.",
        "labels": "BAC"
    },
    {
        "idx": 138,
        "text": "Since PLMs capture word semantics in different contexts, the quality of word representations highly depends on word frequency, which usually follows a heavy-tailed distributions in the pre-training corpus. ",
        "labels": "BAC"
    },
    {
        "idx": 138,
        "text": "Therefore, the embeddings of rare words on the tail are usually poorly optimized.",
        "labels": "GAP"
    },
    {
        "idx": 138,
        "text": "In this work, we focus on enhancing language model pre-training by leveraging definitions of the rare words in dictionaries (e.g., Wiktionary).",
        "labels": "PUR"
    },
    {
        "idx": 138,
        "text": "To incorporate a rare word definition as a part of input, we fetch its definition from the dictionary and append it to the end of the input text sequence.",
        "labels": "MTD"
    },
    {
        "idx": 138,
        "text": "In addition to training with the masked language modeling objective, we propose two novel self-supervised pre-training tasks on word and sentence-level alignment between input text sequence and rare word definitions to enhance language modeling representation with dictionary.",
        "labels": "MTD"
    },
    {
        "idx": 138,
        "text": "We evaluate the proposed Dict-BERT model on the language understanding benchmark GLUE and eight specialized domain benchmark datasets.",
        "labels": "MTD"
    },
    {
        "idx": 138,
        "text": "Extensive experiments demonstrate that Dict-BERT can significantly improve the understanding of rare words and boost model performance on various NLP downstream tasks.",
        "labels": "RST"
    },
    {
        "idx": 139,
        "text": "We conduct a feasibility study into the applicability of answer-unaware question generation models to textbook passages.",
        "labels": "PUR"
    },
    {
        "idx": 139,
        "text": "We show that a significant portion of errors in such systems arise from asking irrelevant or un-interpretable questions and that such errors can be ameliorated by providing summarized input.",
        "labels": "RST"
    },
    {
        "idx": 139,
        "text": "We find that giving these models human-written summaries instead of the original text results in a significant increase in acceptability of generated questions (33% rightarrow 83%) as determined by expert annotators.",
        "labels": "CLN"
    },
    {
        "idx": 139,
        "text": "We also find that, in the absence of human-written summaries, automatic summarization can serve as a good middle ground.",
        "labels": "CLN"
    },
    {
        "idx": 140,
        "text": "We study the challenge of learning causal reasoning over procedural text to answer What if... questions when external commonsense knowledge is required.",
        "labels": "PUR"
    },
    {
        "idx": 140,
        "text": "We propose a novel multi-hop graph reasoning model to 1) efficiently extract a commonsense subgraph with the most relevant information from a large knowledge graph; 2) predict the causal answer by reasoning over the representations obtained from the commonsense subgraph and the contextual interactions between the questions and context.",
        "labels": "MTD"
    },
    {
        "idx": 140,
        "text": "We evaluate our model on WIQA benchmark and achieve state-of-the-art performance compared to the recent models.",
        "labels": "RST"
    },
    {
        "idx": 141,
        "text": "Knowledge graph integration typically suffers from the widely existing dangling entities that cannot find alignment cross knowledge graphs (KGs)",
        "labels": "BAC"
    },
    {
        "idx": 141,
        "text": " The dangling entity set is unavailable in most real-world scenarios, and manually mining the entity pairs that consist of entities with the same meaning is labor-consuming.",
        "labels": "GAP"
    },
    {
        "idx": 141,
        "text": "In this paper, we propose a novel accurate Unsupervised method for joint Entity alignment (EA) and Dangling entity detection (DED), called UED.",
        "labels": "PUR"
    },
    {
        "idx": 141,
        "text": "The UED mines the literal semantic information to generate pseudo entity pairs and globally guided alignment information for EA and then utilizes the EA results to assist the DED",
        "labels": "MTD"
    },
    {
        "idx": 141,
        "text": "We construct a medical cross-lingual knowledge graph dataset, MedED, providing data for both the EA and DED tasks.",
        "labels": "MTD"
    },
    {
        "idx": 141,
        "text": "Extensive experiments demonstrate that in the EA task, UED achieves EA results comparable to those of state-of-the-art supervised EA baselines and outperforms the current state-of-the-art EA methods by combining supervised EA data",
        "labels": "RST"
    },
    {
        "idx": 141,
        "text": "For the DED task, UED obtains high-quality results without supervision.",
        "labels": "RST"
    },
    {
        "idx": 142,
        "text": "The prototypical NLP experiment trains a standard architecture on labeled English data and optimizes for accuracy, without accounting for other dimensions such as fairness, interpretability, or computational efficiency.",
        "labels": "BAC"
    },
    {
        "idx": 142,
        "text": "We show through a manual classification of recent NLP research papers that this is indeed the case and refer to it as the square one experimental setup.",
        "labels": "PUR"
    },
    {
        "idx": 142,
        "text": "We observe that NLP research often goes beyond the square one setup, e.g, focusing not only on accuracy, but also on fairness or interpretability, but typically only along a single dimension.",
        "labels": "MTD"
    },
    {
        "idx": 142,
        "text": "Most work targeting multilinguality, for example, considers only accuracy; most work on fairness or interpretability considers only English; and so on.",
        "labels": "RST"
    },
    {
        "idx": 142,
        "text": "Such one-dimensionality of most research means we are only exploring a fraction of the NLP research search space.",
        "labels": "RST"
    },
    {
        "idx": 142,
        "text": "We provide historical and recent examples of how the square one bias has led researchers to draw false conclusions or make unwise choices, point to promising yet unexplored directions on the research manifold, and make practical recommendations to enable more multi-dimensional research.",
        "labels": "CLN"
    },
    {
        "idx": 142,
        "text": "We open-source the results of our annotations to enable further analysis.",
        "labels": "CTN"
    },
    {
        "idx": 143,
        "text": "Metamorphic testing has recently been used to check the safety of neural NLP models. Its main advantage is that it does not rely on a ground truth to generate test cases.",
        "labels": "BAC"
    },
    {
        "idx": 143,
        "text": "However, existing studies are mostly concerned with robustness-like metamorphic relations, limiting the scope of linguistic properties they can test.",
        "labels": "GAP"
    },
    {
        "idx": 143,
        "text": "We propose three new classes of metamorphic relations, which address the properties of systematicity, compositionality and transitivity.",
        "labels": "PUR"
    },
    {
        "idx": 143,
        "text": "Unlike robustness, our relations are defined over multiple source inputs, thus increasing the number of test cases that we can produce by a polynomial factor.",
        "labels": "MTD"
    },
    {
        "idx": 143,
        "text": "With them, we test the internal consistency of state-of-the-art NLP models",
        "labels": "MTD"
    },
    {
        "idx": 143,
        "text": "and show that they do not always behave according to their expected linguistic properties",
        "labels": "RST"
    },
    {
        "idx": 143,
        "text": "Lastly, we introduce a novel graphical notation that efficiently summarises the inner structure of metamorphic relations.",
        "labels": "RST"
    },
    {
        "idx": 144,
        "text": "Many tasks in text-based computational social science (CSS) involve the classification of political statements into categories based on a domain-specific codebook",
        "labels": "BAC"
    },
    {
        "idx": 144,
        "text": " In order to be useful for CSS analysis, these categories must be fine-grained.",
        "labels": "BAC"
    },
    {
        "idx": 144,
        "text": "The typically skewed distribution of fine-grained categories, however, results in a challenging classification problem on the NLP side.",
        "labels": "GAP"
    },
    {
        "idx": 144,
        "text": "This paper proposes to make use of the hierarchical relations among categories typically present in such codebooks:e.g., markets and taxation are both subcategories of economy, while borders is a subcategory of security.",
        "labels": "PUR"
    },
    {
        "idx": 144,
        "text": "We use these ontological relations as prior knowledge to establish additional constraints on the learned model, thusimproving performance overall and in particular for infrequent categories.",
        "labels": "MTD"
    },
    {
        "idx": 144,
        "text": "We evaluate several lightweight variants of this intuition by extending state-of-the-art transformer-based textclassifiers on two datasets and multiple languages.",
        "labels": "MTD"
    },
    {
        "idx": 144,
        "text": "We find the most consistent improvement for an approach based on regularization.",
        "labels": "RST"
    },
    {
        "idx": 145,
        "text": "The recent large-scale vision-language pre-training (VLP) of dual-stream architectures (e.g., CLIP) with a tremendous amount of image-text pair data, has shown its superiority on various multimodal alignment tasks",
        "labels": "BAC"
    },
    {
        "idx": 145,
        "text": "Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder.",
        "labels": "GAP"
    },
    {
        "idx": 145,
        "text": "To tackle this problem, we propose to augment the dual-stream VLP model with a textual pre-trained language model (PLM) via vision-language knowledge distillation (VLKD), enabling the capability for multimodal generation. ",
        "labels": "PUR"
    },
    {
        "idx": 145,
        "text": "VLKD is pretty data- and computation-efficient compared to the pre-training from scratch",
        "labels": "MTD"
    },
    {
        "idx": 145,
        "text": "Experimental results show that the resulting model has strong zero-shot performance on multimodal generation tasks, such as open-ended visual question answering and image captioning.",
        "labels": "RST"
    },
    {
        "idx": 145,
        "text": "For example, it achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous state-of-the-art zero-shot model with 7times fewer parameters.",
        "labels": "RST"
    },
    {
        "idx": 145,
        "text": "Furthermore, the original textual language understanding and generation ability of the PLM is maintained after VLKD, which makes our model versatile for both multimodal and unimodal tasks.",
        "labels": "CLN"
    },
    {
        "idx": 146,
        "text": "Most existing approaches to Visual Question Answering (VQA) answer questions directly",
        "labels": "BAC"
    },
    {
        "idx": 146,
        "text": "however, people usually decompose a complex question into a sequence of simple sub questions and finally obtain the answer to the original question after answering the sub question sequence(SQS).",
        "labels": "GAP"
    },
    {
        "idx": 146,
        "text": "By simulating the process, this paper proposes a conversation-based VQA (Co-VQA) framework, which consists of three components: Questioner, Oracle, and Answerer.",
        "labels": "PUR"
    },
    {
        "idx": 146,
        "text": "Questioner raises the sub questions using an extending HRED model, and Oracle answers them one-by-one.",
        "labels": "MTD"
    },
    {
        "idx": 146,
        "text": "An Adaptive Chain Visual Reasoning Model (ACVRM) for Answerer is also proposed, where the question-answer pair is used to update the visual representation sequentially.",
        "labels": "MTD"
    },
    {
        "idx": 146,
        "text": "To perform supervised learning for each model",
        "labels": "MTD"
    },
    {
        "idx": 146,
        "text": "we introduce a well-designed method to build a SQS for each question on VQA 2.0 and VQA-CP v2 datasets.",
        "labels": "MTD"
    },
    {
        "idx": 146,
        "text": "Experimental results show that our method achieves state-of-the-art on VQA-CP v2.",
        "labels": "RST"
    },
    {
        "idx": 146,
        "text": "Further analyses show that SQSs help build direct semantic connections between questions and images, provide question-adaptive variable-length reasoning chains, and with explicit interpretability as well as error traceability.",
        "labels": "CLN"
    },
    {
        "idx": 147,
        "text": "Early exiting allows instances to exit at different layers according to the estimation of difficulty",
        "labels": "BAC"
    },
    {
        "idx": 147,
        "text": "Previous works usually adopt heuristic metrics such as the entropy of internal outputs to measure instance difficulty, which suffers from generalization and threshold-tuning",
        "labels": "BAC"
    },
    {
        "idx": 147,
        "text": "In contrast, learning to exit, or learning to predict instance difficulty is a more appealing way.",
        "labels": "BAC"
    },
    {
        "idx": 147,
        "text": "Though some effort has been devoted to employing such learn-to-exit modules, it is still unknown whether and how well the instance difficulty can be learned.",
        "labels": "GAP"
    },
    {
        "idx": 147,
        "text": "As a response, we first conduct experiments on the learnability of instance difficulty, which demonstrates that modern neural models perform poorly on predicting instance difficulty.",
        "labels": "MTD"
    },
    {
        "idx": 147,
        "text": "Based on this observation, we propose a simple-yet-effective Hash-based Early Exiting approach HashEE) that replaces the learn-to-exit modules with hash functions to assign each token to a fixed exiting layer.",
        "labels": "MTD"
    },
    {
        "idx": 147,
        "text": "Different from previous methods, HashEE requires no internal classifiers nor extra parameters, and therefore is more efficient",
        "labels": "MTD"
    },
    {
        "idx": 147,
        "text": "HashEE can be used in various tasks (including language understanding and generation) and model architectures such as seq2seq models",
        "labels": "CLN"
    },
    {
        "idx": 147,
        "text": "Experimental results on classification, regression, and generation tasks demonstrate that HashEE can achieve higher performance with fewer FLOPs and inference time compared with previous state-of-the-art early exiting methods.",
        "labels": "RST"
    },
    {
        "idx": 148,
        "text": "The biaffine parser of (CITATION) was successfully extended to semantic dependency parsing (SDP) (CITATION)",
        "labels": "BAC"
    },
    {
        "idx": 148,
        "text": "Its performance on graphs is surprisingly high given that, without the constraint of producing a tree, all arcs for a given sentence are predicted independently from each other (modulo a shared representation of tokens)",
        "labels": "BAC"
    },
    {
        "idx": 148,
        "text": "To circumvent such an independence of decision, while retaining the O(n^2) complexity and highly parallelizable architecture, we propose to use simple auxiliary tasks that introduce some form of interdependence between arcs",
        "labels": "PUR"
    },
    {
        "idx": 148,
        "text": "Experiments on the three English acyclic datasets of SemEval-2015 task 18 (CITATION), and on French deep syntactic cyclic graphs (CITATION) show modest but systematic performance gains on a near-state-of-the-art baseline using transformer-based contextualized representations.",
        "labels": "RST"
    },
    {
        "idx": 148,
        "text": "This provides a simple and robust method to boost SDP performance",
        "labels": "RST"
    },
    {
        "idx": 149,
        "text": "Syntactic information has been proved to be useful for transformer-based pre-trained language models.",
        "labels": "BAC"
    },
    {
        "idx": 149,
        "text": "Previous studies often rely on additional syntax-guided attention components to enhance the transformer, which require more parameters and additional syntactic parsing in downstream tasks.",
        "labels": "GAP"
    },
    {
        "idx": 149,
        "text": "This increase in complexity severely limits the application of syntax-enhanced language model in a wide range of scenarios.",
        "labels": "GAP"
    },
    {
        "idx": 149,
        "text": "In order to inject syntactic knowledge effectively and efficiently into pre-trained language models, we propose a novel syntax-guided contrastive learning method which does not change the transformer architecture.",
        "labels": "PUR"
    },
    {
        "idx": 149,
        "text": "Based on constituency and dependency structures of syntax trees, we design phrase-guided and tree-guided contrastive objectives, and optimize them in the pre-training stage, so as to help the pre-trained language model to capture rich syntactic knowledge in its representations.",
        "labels": "MTD"
    },
    {
        "idx": 149,
        "text": "Experimental results show that our contrastive method achieves consistent improvements in a variety of tasks, including grammatical error detection, entity tasks, structural probing and GLUE.",
        "labels": "RST"
    },
    {
        "idx": 149,
        "text": "Detailed analysis further verifies that the improvements come from the utilization of syntactic information, and the learned attention weights are more explainable in terms of linguistics.",
        "labels": "CLN"
    },
    {
        "idx": 150,
        "text": " In document classification for, e.g., legal and biomedical text, we often deal with hundreds of classes, including very infrequent ones, as well as temporal concept drift caused by the influence of real world events, e.g., policy changes, conflicts, or pandemics.",
        "labels": "BAC"
    },
    {
        "idx": 150,
        "text": "Class imbalance and drift can sometimes be mitigated by resampling the training data to simulate (or compensate for) a known target distribution",
        "labels": "BAC"
    },
    {
        "idx": 150,
        "text": "but what if the target distribution is determined by unknown future events?",
        "labels": "GAP"
    },
    {
        "idx": 150,
        "text": "Instead of simply resampling uniformly to hedge our bets, we focus on the underlying optimization algorithms used to train such document classifiers and evaluate several group-robust optimization algorithms, initially proposed to mitigate group-level disparities. ",
        "labels": "PUR"
    },
    {
        "idx": 150,
        "text": "Reframing group-robust algorithms as adaptation algorithms under concept drift",
        "labels": "MTD"
    },
    {
        "idx": 150,
        "text": "we find that Invariant Risk Minimization and Spectral Decoupling outperform sampling-based approaches to class imbalance and concept drift, and lead to much better performance on minority classes.",
        "labels": "RST"
    },
    {
        "idx": 150,
        "text": "The effect is more pronounced the larger the label set.",
        "labels": "RST"
    },
    {
        "idx": 151,
        "text": "Prompt-based learning, which exploits knowledge from pre-trained language models by providing textual prompts and designing appropriate answer-category mapping methods, has achieved impressive successes on few-shot text classification and natural language inference (NLI)",
        "labels": "BAC"
    },
    {
        "idx": 151,
        "text": "Because of the diverse linguistic expression, there exist many answer tokens for the same category.",
        "labels": "BAC"
    },
    {
        "idx": 151,
        "text": "However, both manual answer design and automatic answer search constrain answer space and therefore hardly achieve ideal performance.",
        "labels": "GAP"
    },
    {
        "idx": 151,
        "text": "To address this issue, we propose an answer space clustered prompting model (ASCM) together with a synonym initialization method (SI) which automatically categorizes all answer tokens in a semantic-clustered embedding space.",
        "labels": "PUR"
    },
    {
        "idx": 151,
        "text": "We also propose a stable semi-supervised method named stair learning (SL) that orderly distills knowledge from better models to weaker models.",
        "labels": "PUR"
    },
    {
        "idx": 151,
        "text": "Extensive experiments demonstrate that our ASCM+SL significantly outperforms existing state-of-the-art techniques in few-shot settings.",
        "labels": "RST"
    },
    {
        "idx": 152,
        "text": "We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT).",
        "labels": "PUR"
    },
    {
        "idx": 152,
        "text": "Despite evidence in the literature that character-level systems are comparable with subword systems,",
        "labels": "BAC"
    },
    {
        "idx": 152,
        "text": "they are virtually never used in competitive setups in WMT competitions.",
        "labels": "GAP"
    },
    {
        "idx": 152,
        "text": "We empirically show that even with recent modeling innovations in character-level natural language processing, character-level MT systems still struggle to match their subword-based counterparts.",
        "labels": "RST"
    },
    {
        "idx": 152,
        "text": "Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated.",
        "labels": "RST"
    },
    {
        "idx": 152,
        "text": "However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time.",
        "labels": "RST"
    },
    {
        "idx": 153,
        "text": " Math Word Problem (MWP) solving needs to discover the quantitative relationships over natural language narratives.",
        "labels": "BAC"
    },
    {
        "idx": 153,
        "text": "Recent work shows that existing models memorize procedures from context and rely on shallow heuristics to solve MWPs.",
        "labels": "GAP"
    },
    {
        "idx": 153,
        "text": "In this paper, we look at this issue and argue that the cause is a lack of overall understanding of MWP patterns.",
        "labels": "PUR"
    },
    {
        "idx": 153,
        "text": "We first investigate how a neural network understands patterns only from semantics, and observe that, if the prototype equations are the same, most problems get closer representations and those representations apart from them or close to other prototypes tend to produce wrong solutions",
        "labels": "MTD"
    },
    {
        "idx": 153,
        "text": " Inspired by it, we propose a contrastive learning approach, where the neural network perceives the divergence of patterns.",
        "labels": "MTD"
    },
    {
        "idx": 153,
        "text": "We collect contrastive examples by converting the prototype equation into a tree and seeking similar tree structures.",
        "labels": "MTD"
    },
    {
        "idx": 153,
        "text": "The solving model is trained with an auxiliary objective on the collected examples, resulting in the representations of problems with similar prototypes being pulled closer.",
        "labels": "MTD"
    },
    {
        "idx": 153,
        "text": "We conduct experiments on the Chinese dataset Math23k and the English dataset MathQA.",
        "labels": "MTD"
    },
    {
        "idx": 153,
        "text": "Our method greatly improves the performance in monolingual and multilingual settings.",
        "labels": "RST"
    },
    {
        "idx": 154,
        "text": "Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts.",
        "labels": "GAP"
    },
    {
        "idx": 154,
        "text": "In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task.",
        "labels": "PUR"
    },
    {
        "idx": 154,
        "text": "We extend the established English GQA dataset to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering.",
        "labels": "MTD"
    },
    {
        "idx": 154,
        "text": "We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and-vice versa-multilingual models to become multimodal.",
        "labels": "MTD"
    },
    {
        "idx": 154,
        "text": "Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the accuracy remains low across the board",
        "labels": "RST"
    },
    {
        "idx": 154,
        "text": "a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task.",
        "labels": "RST"
    },
    {
        "idx": 154,
        "text": "Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling.",
        "labels": "CLN"
    },
    {
        "idx": 155,
        "text": "We investigate the exploitation of self-supervised models for two Creole languages with few resources: Gwadloup{'eyen and Morisien.",
        "labels": "PUR"
    },
    {
        "idx": 155,
        "text": "Automatic language processing tools are almost non-existent for these two languages.",
        "labels": "BAC"
    },
    {
        "idx": 155,
        "text": "We propose to use about one hour of annotated data to design an automatic speech recognition system for each language.",
        "labels": "MTD"
    },
    {
        "idx": 155,
        "text": "We evaluate how much data is needed to obtain a query-by-example system that is usable by linguists.",
        "labels": "MTD"
    },
    {
        "idx": 155,
        "text": "Moreover, our experiments show that multilingual self-supervised models are not necessarily the most efficient for Creole languages.",
        "labels": "RST"
    },
    {
        "idx": 156,
        "text": "When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited.",
        "labels": "BAC"
    },
    {
        "idx": 156,
        "text": "A typical example is when using CNN/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences.",
        "labels": "BAC"
    },
    {
        "idx": 156,
        "text": "A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with deep understanding of the domain knowledge.",
        "labels": "BAC"
    },
    {
        "idx": 156,
        "text": "Motivated by this vision, our paper introduces a new text generation dataset, named MReD.",
        "labels": "PUR"
    },
    {
        "idx": 156,
        "text": "Our new dataset consists of 7,089 meta-reviews and all its 45k meta-review sentences are manually annotated with one of the 9 carefully defined categories, including abstract, strength, decision, etc.",
        "labels": "MTD"
    },
    {
        "idx": 156,
        "text": "We present experimental results on start-of-the-art summarization models",
        "labels": "MTD"
    },
    {
        "idx": 156,
        "text": "and propose methods for structure-controlled generation with both extractive and abstractive models using our annotated data.",
        "labels": "MTD"
    },
    {
        "idx": 156,
        "text": "By exploring various settings and analyzing the model behavior with respect to the control signal,",
        "labels": "MTD"
    },
    {
        "idx": 156,
        "text": "we demonstrate the challenges of our proposed task and the values of our dataset MReD.",
        "labels": "RST"
    },
    {
        "idx": 156,
        "text": "Meanwhile, MReD also allows us to have a better understanding of the meta-review domain.",
        "labels": "CTN"
    },
    {
        "idx": 157,
        "text": "Subword regularizations use multiple subword segmentations during training to improve the robustness of neural machine translation models",
        "labels": "BAC"
    },
    {
        "idx": 157,
        "text": "In previous subword regularizations, we use multiple segmentations in the training process but use only one segmentation in the inference",
        "labels": "GAP"
    },
    {
        "idx": 157,
        "text": "In this study, we propose an inference strategy to address this discrepancy",
        "labels": "PUR"
    },
    {
        "idx": 157,
        "text": "The proposed strategy approximates the marginalized likelihood by using multiple segmentations including the most plausible segmentation and several sampled segmentations",
        "labels": "MTD"
    },
    {
        "idx": 157,
        "text": "Because the proposed strategy aggregates predictions from several segmentations, we can regard it as a single model ensemble that does not require any additional cost for training",
        "labels": "MTD"
    },
    {
        "idx": 157,
        "text": "Experimental results show that the proposed strategy improves the performance of models trained with subword regularization in low-resource machine translation tasks.",
        "labels": "RST"
    },
    {
        "idx": 158,
        "text": "The filtering and/or selection of training data is one of the core aspects to be considered when building a strong machine translation system",
        "labels": "BAC"
    },
    {
        "idx": 158,
        "text": "In their influential work, Khayrallah and Koehn (2018) investigated the impact of different types of noise on the performance of machine translation systems",
        "labels": "BAC"
    },
    {
        "idx": 158,
        "text": "In the same year the WMT introduced a shared task on parallel corpus filtering, which went on to be repeated in the following years, and resulted in many different filtering approaches being proposed.",
        "labels": "BAC"
    },
    {
        "idx": 158,
        "text": "In this work we aim to combine the recent achievements in data filtering with the original analysis of Khayrallah and Koehn (2018) and investigate whether state-of-the-art filtering systems are capable of removing all the suggested noise types",
        "labels": "PUR"
    },
    {
        "idx": 158,
        "text": "We observe that most of these types of noise can be detected with an accuracy of over 90% by modern filtering systems when operating in a well studied high resource setting.",
        "labels": "RST"
    },
    {
        "idx": 158,
        "text": "However, we also find that when confronted with more refined noise categories or when working with a less common language pair, the performance of the filtering systems is far from optimal, showing that there is still room for improvement in this area of research.",
        "labels": "RST"
    },
    {
        "idx": 159,
        "text": "Due to the limitations of the model structure and pre-training objectives, existing vision-and-language generation models cannot utilize pair-wise images and text through bi-directional generation.",
        "labels": "GAP"
    },
    {
        "idx": 159,
        "text": "In this paper, we propose DU-VLG, a framework which unifies vision-and-language generation as sequence generation problems.",
        "labels": "PUR"
    },
    {
        "idx": 159,
        "text": "DU-VLG is trained with novel dual pre-training tasks: multi-modal denoising autoencoder tasks and modality translation tasks.",
        "labels": "MTD"
    },
    {
        "idx": 159,
        "text": "To bridge the gap between image understanding and generation",
        "labels": "MTD"
    },
    {
        "idx": 159,
        "text": "we further design a novel commitment loss",
        "labels": "MTD"
    },
    {
        "idx": 159,
        "text": "We compare pre-training objectives on image captioning and text-to-image generation datasets.",
        "labels": "MTD"
    },
    {
        "idx": 159,
        "text": "Results show that DU-VLG yields better performance than variants trained with uni-directional generation objectives or the variant without the commitment loss.",
        "labels": "RST"
    },
    {
        "idx": 159,
        "text": "We also obtain higher scores compared to previous state-of-the-art systems on three vision-and-language generation tasks.",
        "labels": "RST"
    },
    {
        "idx": 159,
        "text": "In addition, human judges further confirm that our model generates real and relevant images as well as faithful and informative captions",
        "labels": "RST"
    },
    {
        "idx": 160,
        "text": "Distant supervision assumes that any sentence containing the same entity pairs reflects identical relationships.",
        "labels": "BAC"
    },
    {
        "idx": 160,
        "text": "Previous works of distantly supervised relation extraction (DSRE) task generally focus on sentence-level or bag-level de-noising techniques independently, neglecting the explicit interaction with cross levels.",
        "labels": "GAP"
    },
    {
        "idx": 160,
        "text": "In this paper, we propose a hierarchical contrastive learning Framework for Distantly Supervised relation extraction (HiCLRE) to reduce noisy sentences, which integrate the global structural information and local fine-grained interaction.",
        "labels": "PUR"
    },
    {
        "idx": 160,
        "text": "Specifically, we propose a three-level hierarchical learning framework to interact with cross levels, generating the de-noising context-aware representations via adapting the existing multi-head self-attention, named Multi-Granularity Recontextualization",
        "labels": "MTD"
    },
    {
        "idx": 160,
        "text": "Meanwhile, pseudo positive samples are also provided in the specific level for contrastive learning via a dynamic gradient-based data augmentation strategy, named Dynamic Gradient Adversarial Perturbation.",
        "labels": "MTD"
    },
    {
        "idx": 160,
        "text": "Experiments demonstrate that HiCLRE significantly outperforms strong baselines in various mainstream DSRE datasets",
        "labels": "RST"
    },
    {
        "idx": 161,
        "text": "Neural machine translation (NMT) has obtained significant performance improvement over the recent years.",
        "labels": "BAC"
    },
    {
        "idx": 161,
        "text": "However, NMT models still face various challenges including fragility and lack of style flexibility.",
        "labels": "GAP"
    },
    {
        "idx": 161,
        "text": "Moreover, current methods for instance-level constraints are limited in that they are either constraint-specific or model-specific",
        "labels": "GAP"
    },
    {
        "idx": 161,
        "text": "To this end, we propose prompt-driven neural machine translation to incorporate prompts for enhancing translation control and enriching flexibility. ",
        "labels": "PUR"
    },
    {
        "idx": 161,
        "text": "Empirical results demonstrate the effectiveness of our method in both prompt responding and translation quality.",
        "labels": "RST"
    },
    {
        "idx": 161,
        "text": "Through human evaluation, we further show the flexibility of prompt control and the efficiency in human-in-the-loop translation.",
        "labels": "RST"
    },
    {
        "idx": 162,
        "text": "Dialogue agents can leverage external textual knowledge to generate responses of a higher quality.",
        "labels": "BAC"
    },
    {
        "idx": 162,
        "text": "To our best knowledge, most existing works on knowledge grounded dialogue settings assume that the user intention is always answerable.",
        "labels": "BAC"
    },
    {
        "idx": 162,
        "text": "Unfortunately, this is impractical as there is no guarantee that the knowledge retrievers could always retrieve the desired knowledge.",
        "labels": "GAP"
    },
    {
        "idx": 162,
        "text": "Therefore, this is crucial to incorporate fallback responses to respond to unanswerable contexts appropriately while responding to the answerable contexts in an informative manner.",
        "labels": "GAP"
    },
    {
        "idx": 162,
        "text": "We propose a novel framework that automatically generates a control token with the generator to bias the succeeding response towards informativeness for answerable contexts and fallback for unanswerable contexts in an end-to-end manner.",
        "labels": "PUR"
    },
    {
        "idx": 162,
        "text": "Since no existing knowledge grounded dialogue dataset considers this aim, we augment the existing dataset with unanswerable contexts to conduct our experiments.",
        "labels": "MTD"
    },
    {
        "idx": 162,
        "text": "Automatic and human evaluation results indicate that naively incorporating fallback responses with controlled text generation still hurts informativeness for answerable context.",
        "labels": "RST"
    },
    {
        "idx": 162,
        "text": "In contrast, our proposed framework effectively mitigates this problem while still appropriately presenting fallback responses to unanswerable contexts.",
        "labels": "CLN"
    },
    {
        "idx": 162,
        "text": "Such a framework also reduces the extra burden of the additional classifier and the overheads introduced in the previous works, which operates in a pipeline manner.",
        "labels": "CLN"
    },
    {
        "idx": 163,
        "text": "Humans are able to perceive, understand and reason about causal events.",
        "labels": "BAC"
    },
    {
        "idx": 163,
        "text": "Developing models with similar physical and causal understanding capabilities is a long-standing goal of artificial intelligence.",
        "labels": "BAC"
    },
    {
        "idx": 163,
        "text": "As a step towards this direction, we introduce CRAFT, a new video question answering dataset that requires causal reasoning about physical forces and object interactions.",
        "labels": "PUR"
    },
    {
        "idx": 163,
        "text": "It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene.",
        "labels": "MTD"
    },
    {
        "idx": 163,
        "text": "Two question categories in CRAFT include previously studied descriptive and counterfactual questions",
        "labels": "MTD"
    },
    {
        "idx": 163,
        "text": "Additionally, inspired by the Force Dynamics Theory in cognitive linguistics, we introduce a new causal question category that involves understanding the causal interactions between objects through notions like cause, enable, and prevent",
        "labels": "MTD"
    },
    {
        "idx": 163,
        "text": "Our results show that even though the questions in CRAFT are easy for humans, the tested baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark.",
        "labels": "RST"
    },
    {
        "idx": 164,
        "text": "Predicting the subsequent event for an existing event context is an important but challenging task, as it requires understanding the underlying relationship between events.",
        "labels": "BAC"
    },
    {
        "idx": 164,
        "text": "Previous methods propose to retrieve relational features from event graph to enhance the modeling of event correlation",
        "labels": "BAC"
    },
    {
        "idx": 164,
        "text": "However, the sparsity of event graph may restrict the acquisition of relevant graph information, and hence influence the model performance.",
        "labels": "GAP"
    },
    {
        "idx": 164,
        "text": "To address this issue, we consider automatically building of event graph using a BERT model",
        "labels": "PUR"
    },
    {
        "idx": 164,
        "text": "To this end, we incorporate an additional structured variable into BERT to learn to predict the event connections in the training process",
        "labels": "MTD"
    },
    {
        "idx": 164,
        "text": "Hence, in the test process, the connection relationship for unseen events can be predicted by the structured variable",
        "labels": "MTD"
    },
    {
        "idx": 164,
        "text": "Results on two event prediction tasks: script event prediction and story ending prediction, show that our approach can outperform state-of-the-art baseline methods.",
        "labels": "RST"
    },
    {
        "idx": 165,
        "text": "Most of the open-domain dialogue models tend to perform poorly in the setting of long-term human-bot conversations",
        "labels": "BAC"
    },
    {
        "idx": 165,
        "text": "The possible reason is that they lack the capability of understanding and memorizing long-term dialogue history information",
        "labels": "GAP"
    },
    {
        "idx": 165,
        "text": "To address this issue, we present a novel task of Long-term Memory Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a dialogue generation framework with Long-Term Memory (LTM) mechanism (called PLATO-LTM)",
        "labels": "PUR"
    },
    {
        "idx": 165,
        "text": "This LTM mechanism enables our system to accurately extract and continuously update long-term persona memory without requiring multiple-session dialogue datasets for model training",
        "labels": "MTD"
    },
    {
        "idx": 165,
        "text": "To our knowledge, this is the first attempt to conduct real-time dynamic management of persona information of both parties, including the user and the bot.",
        "labels": "CTN"
    },
    {
        "idx": 165,
        "text": "Results on DuLeMon indicate that PLATO-LTM can significantly outperform baselines in terms of long-term dialogue consistency, leading to better dialogue engagingness.",
        "labels": "RST"
    },
    {
        "idx": 166,
        "text": "Word embeddings are powerful dictionaries, which may easily capture language variations",
        "labels": "BAC"
    },
    {
        "idx": 166,
        "text": "However, these dictionaries fail to give sense to rare words, which are surprisingly often covered by traditional dictionaries",
        "labels": "GAP"
    },
    {
        "idx": 166,
        "text": "In this paper, we propose to use definitions retrieved in traditional dictionaries to produce word embeddings for rare words.",
        "labels": "PUR"
    },
    {
        "idx": 166,
        "text": "For this purpose, we introduce two methods: Definition Neural Network (DefiNNet) and Define BERT (DefBERT).",
        "labels": "MTD"
    },
    {
        "idx": 166,
        "text": "In our experiments, DefiNNet and DefBERT significantly outperform state-of-the-art as well as baseline methods devised for producing embeddings of unknown words",
        "labels": "MTD"
    },
    {
        "idx": 166,
        "text": " In fact, DefiNNet significantly outperforms FastText, which implements a method for the same task-based on n-grams, and DefBERT significantly outperforms the BERT method for OOV words",
        "labels": "RST"
    },
    {
        "idx": 166,
        "text": "Then, definitions in traditional dictionaries are useful to build word embeddings for rare words.",
        "labels": "MTD"
    },
    {
        "idx": 167,
        "text": "Existing news recommendation methods usually learn news representations solely based on news titles.",
        "labels": "BAC"
    },
    {
        "idx": 167,
        "text": "To sufficiently utilize other fields of news information such as category and entities, some methods treat each field as an additional feature and combine different feature vectors with attentive pooling",
        "labels": "BAC"
    },
    {
        "idx": 167,
        "text": "With the adoption of large pre-trained models like BERT in news recommendation, the above way to incorporate multi-field information may encounter challenges: the shallow feature encoding to compress the category and entity information is not compatible with the deep BERT encoding",
        "labels": "GAP"
    },
    {
        "idx": 167,
        "text": "In this paper, we propose a multi-task method to incorporate the multi-field information into BERT, which improves its news encoding capability.",
        "labels": "PUR"
    },
    {
        "idx": 167,
        "text": "Besides, we modify the gradients of auxiliary tasks based on their gradient conflicts with the main task, which further boosts the model performance.",
        "labels": "MTD"
    },
    {
        "idx": 167,
        "text": "Extensive experiments on the MIND news recommendation benchmark show the effectiveness of our approach.",
        "labels": "RST"
    },
    {
        "idx": 168,
        "text": "Cross-domain NER is a practical yet challenging problem since the data scarcity in the real-world scenario",
        "labels": "BAC"
    },
    {
        "idx": 168,
        "text": "A common practice is first to learn a NER model in a rich-resource general domain and then adapt the model to specific domains.",
        "labels": "BAC"
    },
    {
        "idx": 168,
        "text": "Due to the mismatch problem between entity types across domains, the wide knowledge in the general domain can not effectively transfer to the target domain NER model.",
        "labels": "GAP"
    },
    {
        "idx": 168,
        "text": "To this end, we model the label relationship as a probability distribution and construct label graphs in both source and target label spaces.",
        "labels": "PUR"
    },
    {
        "idx": 168,
        "text": "To enhance the contextual representation with label structures",
        "labels": "MTD"
    },
    {
        "idx": 168,
        "text": "we fuse the label graph into the word embedding output by BERT.",
        "labels": "MTD"
    },
    {
        "idx": 168,
        "text": "By representing label relationships as graphs, we formulate cross-domain NER as a graph matching problem.",
        "labels": "MTD"
    },
    {
        "idx": 168,
        "text": "Furthermore, the proposed method has good applicability with pre-training methods and is potentially capable of other cross-domain prediction tasks",
        "labels": "MTD"
    },
    {
        "idx": 168,
        "text": "Empirical results on four datasets show that our method outperforms a series of transfer learning, multi-task learning, and few-shot learning methods",
        "labels": "RST"
    },
    {
        "idx": 169,
        "text": "Recently pre-trained multimodal models, such as CLIP, have shown exceptional capabilities towards connecting images and natural language",
        "labels": "BAC"
    },
    {
        "idx": 169,
        "text": "The textual representations in English can be desirably transferred to multilingualism and support downstream multimodal tasks for different languages.",
        "labels": "BAC"
    },
    {
        "idx": 169,
        "text": "Nevertheless, the principle of multilingual fairness is rarely scrutinized: do multilingual multimodal models treat languages equally? Are their performances biased towards particular languages?",
        "labels": "GAP"
    },
    {
        "idx": 169,
        "text": "To answer these questions, we view language as the fairness recipient and introduce two new fairness notions, multilingual individual fairness and multilingual group fairness, for pre-trained multimodal models.",
        "labels": "PUR"
    },
    {
        "idx": 169,
        "text": "Multilingual individual fairness requires that text snippets expressing similar semantics in different languages connect similarly to images, while multilingual group fairness requires equalized predictive performance across languages.",
        "labels": "MTD"
    },
    {
        "idx": 169,
        "text": "We characterize the extent to which pre-trained multilingual vision-and-language representations are individually fair across languages.",
        "labels": "MTD"
    },
    {
        "idx": 169,
        "text": "However, extensive experiments demonstrate that multilingual representations do not satisfy group fairness: (1) there is a severe multilingual accuracy disparity issue; (2) the errors exhibit biases across languages conditioning the group of people in the images, including race, gender and age.",
        "labels": "RST"
    },
    {
        "idx": 170,
        "text": "Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a collection of documents to discover their latent topics using word-document co-occurrences.",
        "labels": "BAC"
    },
    {
        "idx": 170,
        "text": "Previous studies show that representing bigrams collocations in the input can improve topic coherence in English.",
        "labels": "BAC"
    },
    {
        "idx": 170,
        "text": "However, it is unclear how to achieve the best results for languages without marked word boundaries such as Chinese and Thai.",
        "labels": "GAP"
    },
    {
        "idx": 170,
        "text": "Here, we explore the use of retokenization based on chi-squared measures, t-statistics, and raw frequency to merge frequent token ngrams into collocations when preparing input to the LDA model.",
        "labels": "PUR"
    },
    {
        "idx": 170,
        "text": "Based on the goodness of fit and the coherence metric",
        "labels": "MTD"
    },
    {
        "idx": 170,
        "text": "we show that topics trained with merged tokens result in topic keys that are clearer, more coherent, and more effective at distinguishing topics than those of unmerged models.",
        "labels": "RST"
    },
    {
        "idx": 171,
        "text": "Training the deep neural networks that dominate NLP requires large datasets.",
        "labels": "BAC"
    },
    {
        "idx": 171,
        "text": "These are often collected automatically or via crowdsourcing, and may exhibit systematic biases or annotation artifacts.",
        "labels": "BAC"
    },
    {
        "idx": 171,
        "text": "By the latter we mean spurious correlations between inputs and outputs that do not represent a generally held causal relationship between features and classes; models that exploit such correlations may appear to perform a given task well, but fail on out of sample data.",
        "labels": "GAP"
    },
    {
        "idx": 171,
        "text": "In this paper, we evaluate use of different attribution methods for aiding identification of training data artifacts.",
        "labels": "PUR"
    },
    {
        "idx": 171,
        "text": "We propose new hybrid approaches that combine saliency maps (which highlight important input features) with instance attribution methods (which retrieve training samples influential to a given prediction)",
        "labels": "MTD"
    },
    {
        "idx": 171,
        "text": "We show that this proposed training-feature attribution can be used to efficiently uncover artifacts in training data when a challenging validation set is available",
        "labels": "CLN"
    },
    {
        "idx": 171,
        "text": "We also carry out a small user study to evaluate whether these methods are useful to NLP researchers in practice, with promising results.",
        "labels": "MTD"
    },
    {
        "idx": 171,
        "text": "We make code for all methods and experiments in this paper available",
        "labels": "CTN"
    },
    {
        "idx": 172,
        "text": "Named Entity Recognition (NER) systems often demonstrate great performance on in-distribution data,",
        "labels": "BAC"
    },
    {
        "idx": 172,
        "text": "but perform poorly on examples drawn from a shifted distribution.",
        "labels": "GAP"
    },
    {
        "idx": 172,
        "text": "One way to evaluate the generalization ability of NER models is to use adversarial examples, on which the specific variations associated with named entities are rarely considered.",
        "labels": "BAC"
    },
    {
        "idx": 172,
        "text": "To this end, we propose leveraging expert-guided heuristics to change the entity tokens and their surrounding contexts thereby altering their entity types as adversarial attacks.",
        "labels": "PUR"
    },
    {
        "idx": 172,
        "text": "Using expert-guided heuristics, we augmented the CoNLL 2003 test set and manually annotated it to construct a high-quality challenging set.",
        "labels": "MTD"
    },
    {
        "idx": 172,
        "text": "We found that state-of-the-art NER systems trained on CoNLL 2003 training data drop performance dramatically on our challenging set.",
        "labels": "RST"
    },
    {
        "idx": 172,
        "text": "By training on adversarial augmented training examples and using mixup for regularization, we were able to significantly improve the performance on the challenging set as well as improve out-of-domain generalization which we evaluated by using OntoNotes data.",
        "labels": "CLN"
    },
    {
        "idx": 172,
        "text": "We have publicly released our dataset and code at https://github.com/GT-SALT/Guided-Adversarial-Augmentation.",
        "labels": "CTN"
    },
    {
        "idx": 173,
        "text": "We study the problem of few shot learning for named entity recognition.",
        "labels": "PUR"
    },
    {
        "idx": 173,
        "text": "Specifically, we leverage the semantic information in the names of the labels as a way of giving the model additional signal and enriched priors.",
        "labels": "MTD"
    },
    {
        "idx": 173,
        "text": "We propose a neural architecture that consists of two BERT encoders, one to encode the document and its tokens and another one to encode each of the labels in natural language format.",
        "labels": "MTD"
    },
    {
        "idx": 173,
        "text": "Our model learns to match the representations of named entities computed by the first encoder with label representations computed by the second encoder.",
        "labels": "MTD"
    },
    {
        "idx": 173,
        "text": "The label semantics signal is shown to support improved state-of-the-art results in multiple few shot NER benchmarks and on-par performance in standard benchmarks.",
        "labels": "RST"
    },
    {
        "idx": 173,
        "text": "Our model is especially effective in low resource settings.",
        "labels": "CLN"
    },
    {
        "idx": 174,
        "text": "We propose an autoregressive entity linking model, that is trained with two auxiliary tasks, and learns to re-rank generated samples at inference time. ",
        "labels": "PUR"
    },
    {
        "idx": 174,
        "text": "Our proposed novelties address two weaknesses in the literature.",
        "labels": "MTD"
    },
    {
        "idx": 174,
        "text": "First, a recent method proposes to learn mention detection and then entity candidate selection, but relies on predefined sets of candidates.",
        "labels": "MTD"
    },
    {
        "idx": 174,
        "text": "We use encoder-decoder autoregressive entity linking in order to bypass this need, and propose to train mention detection as an auxiliary task instead. ",
        "labels": "MTD"
    },
    {
        "idx": 174,
        "text": "Second, previous work suggests that re-ranking could help correct prediction errors.",
        "labels": "MTD"
    },
    {
        "idx": 174,
        "text": "We add a new, auxiliary task, match prediction, to learn re-ranking.",
        "labels": "MTD"
    },
    {
        "idx": 174,
        "text": "Without the use of a knowledge base or candidate sets, our model sets a new state of the art in two benchmark datasets of entity linking: COMETA in the biomedical domain, and AIDA-CoNLL in the news domain.",
        "labels": "RST"
    },
    {
        "idx": 174,
        "text": "We show through ablation studies that each of the two auxiliary tasks increases performance, and that re-ranking is an important factor to the increase. ",
        "labels": "CLN"
    },
    {
        "idx": 174,
        "text": "Finally, our low-resource experimental results suggest that performance on the main task benefits from the knowledge learned by the auxiliary tasks, and not just from the additional training data.",
        "labels": "IMP"
    },
    {
        "idx": 175,
        "text": "Interactive robots navigating photo-realistic environments need to be trained to effectively leverage and handle the dynamic nature of dialogue in addition to the challenges underlying vision-and-language navigation (VLN)",
        "labels": "BAC"
    },
    {
        "idx": 175,
        "text": "In this paper, we present VISITRON, a multi-modal Transformer-based navigator better suited to the interactive regime inherent to Cooperative Vision-and-Dialog Navigation (CVDN).",
        "labels": "PUR"
    },
    {
        "idx": 175,
        "text": "VISITRON is trained to: i) identify and associate object-level concepts and semantics between the environment and dialogue history, ii) identify when to interact vs. navigate via imitation learning of a binary classification head",
        "labels": "MTD"
    },
    {
        "idx": 175,
        "text": "We perform extensive pre-training and fine-tuning ablations with VISITRON to gain empirical insights and improve performance on CVDN",
        "labels": "MTD"
    },
    {
        "idx": 175,
        "text": "VISITRON's ability to identify when to interact leads to a natural generalization of the game-play mode introduced by Roman et al. (2020) for enabling the use of such models in different environments.",
        "labels": "RST"
    },
    {
        "idx": 175,
        "text": "VISITRON is competitive with models on the static CVDN leaderboard and attains state-of-the-art performance on the Success weighted by Path Length (SPL) metric.",
        "labels": "CLN"
    },
    {
        "idx": 176,
        "text": " In order to equip NLP systems with selective prediction' capability, several task-specific approaches have been proposed.",
        "labels": "BAC"
    },
    {
        "idx": 176,
        "text": " However, which approaches work best across tasks or even if they consistently outperform the simplest baseline MaxProb remains to be explored. ",
        "labels": "GAP"
    },
    {
        "idx": 176,
        "text": "To this end, we systematically study selective prediction in a large-scale setup of 17 datasets across several NLP tasks.",
        "labels": "PUR"
    },
    {
        "idx": 176,
        "text": "Through comprehensive experiments under in-domain (IID), out-of-domain (OOD), and adversarial (ADV) settings,",
        "labels": "MTD"
    },
    {
        "idx": 176,
        "text": "we show that despite leveraging additional resources (held-out data/computation), none of the existing approaches consistently and considerably outperforms MaxProb in all three settings.",
        "labels": "RST"
    },
    {
        "idx": 176,
        "text": "Furthermore, their performance does not translate well across tasks.",
        "labels": "RST"
    },
    {
        "idx": 176,
        "text": "For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate Detection datasets but does not fare well on NLI datasets, especially in the OOD setting.",
        "labels": "RST"
    },
    {
        "idx": 176,
        "text": "Thus, we recommend that future selective prediction approaches should be evaluated across tasks and settings for reliable estimation of their capabilities.",
        "labels": "IMP"
    },
    {
        "idx": 177,
        "text": "Transformer-based models achieve impressive performance on numerous Natural Language Inference (NLI) benchmarks when trained on respective training datasets.",
        "labels": "BAC"
    },
    {
        "idx": 177,
        "text": "However, in certain cases, training samples may not be available or collecting them could be time-consuming and resource-intensive.",
        "labels": "GAP"
    },
    {
        "idx": 177,
        "text": "In this work, we address the above challenge and present an explorative study on unsupervised NLI, a paradigm in which no human-annotated training samples are available. ",
        "labels": "PUR"
    },
    {
        "idx": 177,
        "text": "We investigate it under three settings: PH, P, and NPH that differ in the extent of unlabeled data available for learning.",
        "labels": "MTD"
    },
    {
        "idx": 177,
        "text": "As a solution, we propose a procedural data generation approach that leverages a set of sentence transformations to collect PHL (Premise, Hypothesis, Label) triplets for training NLI models, bypassing the need for human-annotated training data.",
        "labels": "MTD"
    },
    {
        "idx": 177,
        "text": "Comprehensive experiments with several NLI datasets show that the proposed approach results in accuracies of up to 66.75%, 65.9%, 65.39% in PH, P, and NPH settings respectively, outperforming all existing unsupervised baselines.",
        "labels": "RST"
    },
    {
        "idx": 177,
        "text": "Furthermore, fine-tuning our model with as little as asciitilde0.1% of the human-annotated training dataset (500 instances) leads to 12.2% higher accuracy than the model trained from scratch on the same 500 instances.",
        "labels": "RST"
    },
    {
        "idx": 177,
        "text": " Supported by this superior performance, we conclude with a recommendation for collecting high-quality task-specific data.",
        "labels": "CLN"
    },
    {
        "idx": 178,
        "text": "Scaling dialogue systems to a multitude of domains, tasks and languages relies on costly and time-consuming data annotation for different domain-task-language configurations. ",
        "labels": "BAC"
    },
    {
        "idx": 178,
        "text": "The annotation efforts might be substantially reduced by the methods that generalise well in zero- and few-shot scenarios, and also effectively leverage external unannotated data sources (e.g., Web-scale corpora)",
        "labels": "BAC"
    },
    {
        "idx": 178,
        "text": "We propose two methods to this aim, offering improved dialogue natural language understanding (NLU) across multiple languages: 1) Multi-SentAugment, and 2) LayerAgg.",
        "labels": "PUR"
    },
    {
        "idx": 178,
        "text": "Multi-SentAugment is a self-training method which augments available (typically few-shot) training data with similar (automatically labelled) in-domain sentences from large monolingual Web-scale corpora.",
        "labels": "MTD"
    },
    {
        "idx": 178,
        "text": "LayerAgg learns to select and combine useful semantic information scattered across different layers of a Transformer model (e.g., mBERT);",
        "labels": "MTD"
    },
    {
        "idx": 178,
        "text": "it is especially suited for zero-shot scenarios as semantically richer representations should strengthen the model's cross-lingual capabilities.",
        "labels": "MTD"
    },
    {
        "idx": 178,
        "text": "Applying the two methods with state-of-the-art NLU models obtains consistent improvements across two standard multilingual NLU datasets covering 16 diverse languages.",
        "labels": "RST"
    },
    {
        "idx": 178,
        "text": "The gains are observed in zero-shot, few-shot, and even in full-data scenarios.",
        "labels": "RST"
    },
    {
        "idx": 178,
        "text": "The results also suggest that the two methods achieve a synergistic effect: the best overall performance in few-shot setups is attained when the methods are used together.",
        "labels": "IMP"
    },
    {
        "idx": 179,
        "text": "We propose a novel approach that jointly utilizes the labels and elicited rationales for text classification to speed up the training of deep learning models with limited training data.",
        "labels": "PUR"
    },
    {
        "idx": 179,
        "text": "We define and optimize a ranking-constrained loss function that combines cross-entropy loss with ranking losses as rationale constraints.",
        "labels": "MTD"
    },
    {
        "idx": 179,
        "text": "We evaluate our proposed rationale-augmented learning approach on three human-annotated datasets,",
        "labels": "MTD"
    },
    {
        "idx": 179,
        "text": "and show that our approach provides significant improvements over classification approaches that do not utilize rationales as well as other state-of-the-art rationale-augmented baselines.",
        "labels": "CLN"
    },
    {
        "idx": 180,
        "text": "Content is created for a well-defined purpose, often described by a metric or signal represented in the form of structured information. ",
        "labels": "BAC"
    },
    {
        "idx": 180,
        "text": "The relationship between the goal (metrics) of target content and the content itself is non-trivial.",
        "labels": "BAC"
    },
    {
        "idx": 180,
        "text": "While large-scale language models show promising text generation capabilities, guiding the generated text with external metrics is challenging.",
        "labels": "GAP"
    },
    {
        "idx": 180,
        "text": "These metrics and content tend to have inherent relationships and not all of them may be of consequence.",
        "labels": "GAP"
    },
    {
        "idx": 180,
        "text": "We introduce CaM-Gen: Causally aware Generative Networks guided by user-defined target metrics incorporating the causal relationships between the metric and content features.",
        "labels": "PUR"
    },
    {
        "idx": 180,
        "text": "We leverage causal inference techniques to identify causally significant aspects of a text that lead to the target metric and then explicitly guide generative models towards these by a feedback mechanism.",
        "labels": "MTD"
    },
    {
        "idx": 180,
        "text": "We propose this mechanism for variational autoencoder and Transformer-based generative models. ",
        "labels": "MTD"
    },
    {
        "idx": 180,
        "text": "The proposed models beat baselines in terms of the target metric control while maintaining fluency and language quality of the generated text.",
        "labels": "RST"
    },
    {
        "idx": 180,
        "text": "To the best of our knowledge, this is one of the early attempts at controlled generation incorporating a metric guide using causal inference.",
        "labels": "CTN"
    },
    {
        "idx": 181,
        "text": "Pre-trained language models (e.g. BART) have shown impressive results when fine-tuned on large summarization datasets.",
        "labels": "BAC"
    },
    {
        "idx": 181,
        "text": "However, little is understood about this fine-tuning process, including what knowledge is retained from pre-training time or how content selection and generation strategies are learnt across iterations.",
        "labels": "GAP"
    },
    {
        "idx": 181,
        "text": "In this work, we analyze the training dynamics for generation models, focusing on summarization. ",
        "labels": "PUR"
    },
    {
        "idx": 181,
        "text": "Across different datasets (CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and hallucination, we study what the model learns at different stages of its fine-tuning process.",
        "labels": "MTD"
    },
    {
        "idx": 181,
        "text": "We find that a propensity to copy the input is learned early in the training process consistently across all datasets studied.",
        "labels": "RST"
    },
    {
        "idx": 181,
        "text": "On the other hand, factual errors, such as hallucination of unsupported facts, are learnt in the later stages, though this behavior is more varied across domains.",
        "labels": "RST"
    },
    {
        "idx": 181,
        "text": "Based on these observations, we explore complementary approaches for modifying training: first, disregarding high-loss tokens that are challenging to learn and second, disregarding low-loss tokens that are learnt very quickly in the latter stages of the training process.",
        "labels": "MTD"
    },
    {
        "idx": 181,
        "text": "We show that these simple training modifications allow us to configure our model to achieve different goals, such as improving factuality or improving abstractiveness.",
        "labels": "CLN"
    },
    {
        "idx": 182,
        "text": "We examine whether some countries are more richly represented in embedding space than others. ",
        "labels": "PUR"
    },
    {
        "idx": 182,
        "text": "We find that countries whose names occur with low frequency in training corpora are more likely to be tokenized into subwords, are less semantically distinct in embedding space, and are less likely to be correctly predicted: e.g., Ghana (the correct answer and in-vocabulary) is not predicted for, The country producing the most cocoa is [MASK]..",
        "labels": "RST"
    },
    {
        "idx": 182,
        "text": "Although these performance discrepancies and representational harms are due to frequency, we find that frequency is highly correlated with a country's GDP",
        "labels": "RST"
    },
    {
        "idx": 182,
        "text": "thus perpetuating historic power and wealth inequalities.",
        "labels": "RST"
    },
    {
        "idx": 182,
        "text": "We analyze the effectiveness of mitigation strategies",
        "labels": "MTD"
    },
    {
        "idx": 182,
        "text": "recommend that researchers report training word frequencies; and recommend future work for the community to define and design representational guarantees.",
        "labels": "CTN"
    },
    {
        "idx": 183,
        "text": "It is well documented that NLP models learn social biases, ",
        "labels": "BAC"
    },
    {
        "idx": 183,
        "text": "but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). ",
        "labels": "GAP"
    },
    {
        "idx": 183,
        "text": "We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts.",
        "labels": "PUR"
    },
    {
        "idx": 183,
        "text": "Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model's biases override a correct answer choice. ",
        "labels": "MTD"
    },
    {
        "idx": 183,
        "text": "We find that models often rely on stereotypes when the context is under-informative, meaning the model's outputs consistently reproduce harmful biases in this setting.",
        "labels": "RST"
    },
    {
        "idx": 183,
        "text": "Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.",
        "labels": "RST"
    },
    {
        "idx": 184,
        "text": "Grapheme-to-Phoneme (G2P) has many applications in NLP and speech fields.",
        "labels": "BAC"
    },
    {
        "idx": 184,
        "text": "Most existing work focuses heavily on languages with abundant training datasets, which limits the scope of target languages to less than 100 languages. ",
        "labels": "GAP"
    },
    {
        "idx": 184,
        "text": "This work attempts to apply zero-shot learning to approximate G2P models for all low-resource and endangered languages in Glottolog (about 8k languages). ",
        "labels": "PUR"
    },
    {
        "idx": 184,
        "text": "For any unseen target language, we first build the phylogenetic tree (i.e. language family tree) to identify top-k nearest languages for which we have training sets.",
        "labels": "MTD"
    },
    {
        "idx": 184,
        "text": "Then we run models of those languages to obtain a hypothesis set, which we combine into a confusion network to propose a most likely hypothesis as an approximation to the target language.",
        "labels": "MTD"
    },
    {
        "idx": 184,
        "text": "We test our approach on over 600 unseen languages and demonstrate it significantly outperforms baselines.",
        "labels": "IMP"
    },
    {
        "idx": 185,
        "text": "Recent progress in NLP is driven by pretrained models leveraging massive datasets and has predominantly benefited the world's political and economic superpowers. ",
        "labels": "BAC"
    },
    {
        "idx": 185,
        "text": "Technologically underserved languages are left behind because they lack such resources.",
        "labels": "GAP"
    },
    {
        "idx": 185,
        "text": "Hundreds of underserved languages, nevertheless, have available data sources in the form of interlinear glossed text (IGT) from language documentation efforts.",
        "labels": "GAP"
    },
    {
        "idx": 185,
        "text": "IGT remains underutilized in NLP work, perhaps because its annotations are only semi-structured and often language-specific.",
        "labels": "GAP"
    },
    {
        "idx": 185,
        "text": "With this paper, we make the case that IGT data can be leveraged successfully provided that target language expertise is available.",
        "labels": "PUR"
    },
    {
        "idx": 185,
        "text": "We specifically advocate for collaboration with documentary linguists.",
        "labels": "MTD"
    },
    {
        "idx": 185,
        "text": "Our paper provides a roadmap for successful projects utilizing IGT data: (1) It is essential to define which NLP tasks can be accomplished with the given IGT data and how these will benefit the speech community.",
        "labels": "CTN"
    },
    {
        "idx": 185,
        "text": "(2) Great care and target language expertise is required when converting the data into structured formats commonly employed in NLP.",
        "labels": "CTN"
    },
    {
        "idx": 185,
        "text": "(3) Task-specific and user-specific evaluation can help to ascertain that the tools which are created benefit the target language speech community.",
        "labels": "CTN"
    },
    {
        "idx": 185,
        "text": "We illustrate each step through a case study on developing a morphological reinflection system for the Tsimchianic language Gitksan.",
        "labels": "CTN"
    },
    {
        "idx": 186,
        "text": "Reading is integral to everyday life",
        "labels": "BAC"
    },
    {
        "idx": 186,
        "text": "and yet learning to read is a struggle for many young learners.",
        "labels": "GAP"
    },
    {
        "idx": 186,
        "text": "During lessons, teachers can use comprehension questions to increase engagement, test reading skills, and improve retention.",
        "labels": "BAC"
    },
    {
        "idx": 186,
        "text": "Historically such questions were written by skilled teachers, but recently language models have been used to generate comprehension questions.",
        "labels": "GAP"
    },
    {
        "idx": 186,
        "text": "However, many existing Question Generation (QG) systems focus on generating extractive questions from the text, and have no way to control the type of the generated question.",
        "labels": "GAP"
    },
    {
        "idx": 186,
        "text": "In this paper, we study QG for reading comprehension where inferential questions are critical and extractive techniques cannot be used. ",
        "labels": "PUR"
    },
    {
        "idx": 186,
        "text": "We propose a two-step model (HTA-WTA) that takes advantage of previous datasets, and can generate questions for a specific targeted comprehension skill.",
        "labels": "MTD"
    },
    {
        "idx": 186,
        "text": "We propose a new reading comprehension dataset that contains questions annotated with story-based reading comprehension skills (SBRCS), allowing for a more complete reader assessment.",
        "labels": "MTD"
    },
    {
        "idx": 186,
        "text": " Across several experiments, our results show that HTA-WTA outperforms multiple strong baselines on this new dataset.",
        "labels": "RST"
    },
    {
        "idx": 186,
        "text": "We show that the HTA-WTA model tests for strong SCRS by asking deep inferential questions.",
        "labels": "CLN"
    },
    {
        "idx": 187,
        "text": "Entity retrieval-retrieving information about entity mentions in a query-is a key step in open-domain tasks, such as question answering or fact checking. ",
        "labels": "BAC"
    },
    {
        "idx": 187,
        "text": "However, state-of-the-art entity retrievers struggle to retrieve rare entities for ambiguous mentions due to biases towards popular entities.",
        "labels": "GAP"
    },
    {
        "idx": 187,
        "text": "Incorporating knowledge graph types during training could help overcome popularity biases, but there are several challenges: (1) existing type-based retrieval methods require mention boundaries as input, but open-domain tasks run on unstructured text, (2) type-based methods should not compromise overall performance, and (3) type-based methods should be robust to noisy and missing types. ",
        "labels": "GAP"
    },
    {
        "idx": 187,
        "text": "In this work, we introduce TABi, a method to jointly train bi-encoders on knowledge graph types and unstructured text for entity retrieval for open-domain tasks. ",
        "labels": "PUR"
    },
    {
        "idx": 187,
        "text": "TABi leverages a type-enforced contrastive loss to encourage entities and queries of similar types to be close in the embedding space. ",
        "labels": "MTD"
    },
    {
        "idx": 187,
        "text": "TABi improves retrieval of rare entities on the Ambiguous Entity Retrieval (AmbER) sets",
        "labels": "RST"
    },
    {
        "idx": 187,
        "text": "while maintaining strong overall retrieval performance on open-domain tasks in the KILT benchmark compared to state-of-the-art retrievers. ",
        "labels": "RST"
    },
    {
        "idx": 187,
        "text": "TABi is also robust to incomplete type systems, improving rare entity retrieval over baselines with only 5% type coverage of the training dataset.",
        "labels": "RST"
    },
    {
        "idx": 187,
        "text": "We make our code publicly available.",
        "labels": "CTN"
    },
    {
        "idx": 188,
        "text": "Large pretrained models enable transfer learning to low-resource domains for language generation tasks.",
        "labels": "BAC"
    },
    {
        "idx": 188,
        "text": "However, previous end-to-end approaches do not account for the fact that some generation sub-tasks, specifically aggregation and lexicalisation, can benefit from transfer learning in different extents.",
        "labels": "GAP"
    },
    {
        "idx": 188,
        "text": "To exploit these varying potentials for transfer learning, we propose a new hierarchical approach for few-shot and zero-shot generation. ",
        "labels": "PUR"
    },
    {
        "idx": 188,
        "text": "Our approach consists of a three-moduled jointly trained architecture: the first module independently lexicalises the distinct units of information in the input as sentence sub-units (e.g. phrases), the second module recurrently aggregates these sub-units to generate a unified intermediate output, while the third module subsequently post-edits it to generate a coherent and fluent final text. ",
        "labels": "MTD"
    },
    {
        "idx": 188,
        "text": "We perform extensive empirical analysis and ablation studies on few-shot and zero-shot settings across 4 datasets.",
        "labels": "MTD"
    },
    {
        "idx": 188,
        "text": "Automatic and human evaluation shows that the proposed hierarchical approach is consistently capable of achieving state-of-the-art results when compared to previous work.",
        "labels": "CLN"
    },
    {
        "idx": 189,
        "text": "Recent advances in NLP often stem from large transformer-based pre-trained models, which rapidly grow in size and use more and more training data.",
        "labels": "BAC"
    },
    {
        "idx": 189,
        "text": "Such models are often released to the public so that end users can fine-tune them on a task dataset.",
        "labels": "BAC"
    },
    {
        "idx": 189,
        "text": "While it is common to treat pre-training data as public, it may still contain personally identifiable information (PII), such as names, phone numbers, and copyrighted material.",
        "labels": "GAP"
    },
    {
        "idx": 189,
        "text": "Recent findings show that the capacity of these models allows them to memorize parts of the training data, ",
        "labels": "BAC"
    },
    {
        "idx": 189,
        "text": "and suggest differentially private (DP) training as a potential mitigation. ",
        "labels": "BAC"
    },
    {
        "idx": 189,
        "text": "While there is recent work on DP fine-tuning of NLP models, the effects of DP pre-training are less well understood: it is not clear how downstream performance is affected by DP pre-training, and whether DP pre-training mitigates some of the memorization concerns. ",
        "labels": "GAP"
    },
    {
        "idx": 189,
        "text": "We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE).",
        "labels": "CLN"
    },
    {
        "idx": 189,
        "text": "Moreover, we show that T5's span corruption is a good defense against data memorization.",
        "labels": "CLN"
    },
    {
        "idx": 190,
        "text": "Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. ",
        "labels": "BAC"
    },
    {
        "idx": 190,
        "text": "As most research on active learning has been carried out before transformer-based language models (transformers) became popular, despite its practical importance, comparably few papers have investigated how transformers can be combined with active learning to date.",
        "labels": "GAP"
    },
    {
        "idx": 190,
        "text": "This can be attributed to the fact that using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings.",
        "labels": "GAP"
    },
    {
        "idx": 190,
        "text": "For this reason, we revisit uncertainty-based query strategies, which had been largely outperformed before, but are particularly suited in the context of fine-tuning transformers.",
        "labels": "PUR"
    },
    {
        "idx": 190,
        "text": "In an extensive evaluation, we connect transformers to experiments from previous research, assessing their performance on five widely used text classification benchmarks.",
        "labels": "MTD"
    },
    {
        "idx": 190,
        "text": "For active learning with transformers, several other uncertainty-based approaches outperform the well-known prediction entropy query strategy,",
        "labels": "RST"
    },
    {
        "idx": 190,
        "text": "thereby challenging its status as most popular uncertainty baseline in active learning for text classification.",
        "labels": "CLN"
    },
    {
        "idx": 191,
        "text": "Considering the seq2seq architecture of Yin and Neubig (2018) for natural language to code translation, we identify four key components of importance: grammatical constraints, lexical preprocessing, input representations, and copy mechanisms. ",
        "labels": "PUR"
    },
    {
        "idx": 191,
        "text": "To study the impact of these components,",
        "labels": "PUR"
    },
    {
        "idx": 191,
        "text": "we use a state-of-the-art architecture that relies on BERT encoder and a grammar-based decoder for which a formalization is provided.",
        "labels": "MTD"
    },
    {
        "idx": 191,
        "text": "The paper highlights the importance of the lexical substitution component in the current natural language to code systems.",
        "labels": "CTN"
    },
    {
        "idx": 192,
        "text": "Aspect-based sentiment analysis (ABSA) tasks aim to extract sentiment tuples from a sentence. ",
        "labels": "BAC"
    },
    {
        "idx": 192,
        "text": "Recent generative methods such as Seq2Seq models have achieved good performance by formulating the output as a sequence of sentiment tuples. ",
        "labels": "BAC"
    },
    {
        "idx": 192,
        "text": "However, the orders between the sentiment tuples do not naturally exist and the generation of the current tuple should not condition on the previous ones.",
        "labels": "GAP"
    },
    {
        "idx": 192,
        "text": "In this paper, we propose Seq2Path to generate sentiment tuples as paths of a tree.",
        "labels": "PUR"
    },
    {
        "idx": 192,
        "text": "A tree can represent 1-to-n relations (e.g., an aspect term may correspond to multiple opinion terms) and the paths of a tree are independent and do not have orders. ",
        "labels": "MTD"
    },
    {
        "idx": 192,
        "text": "For training, we treat each path as an independent target, and we calculate the average loss of the ordinary Seq2Seq model over paths.",
        "labels": "MTD"
    },
    {
        "idx": 192,
        "text": "For inference, we apply beam search with constrained decoding.",
        "labels": "MTD"
    },
    {
        "idx": 192,
        "text": "By introducing an additional discriminative token and applying a data augmentation technique, valid paths can be automatically selected.",
        "labels": "MTD"
    },
    {
        "idx": 192,
        "text": "We conduct experiments on five tasks including AOPE, ASTE, TASD, UABSA, ACOS. ",
        "labels": "MTD"
    },
    {
        "idx": 192,
        "text": "We evaluate our method on four common benchmark datasets including Laptop14, Rest14, Rest15, Rest16.",
        "labels": "MTD"
    },
    {
        "idx": 192,
        "text": "Our proposed method achieves state-of-the-art results in almost all cases.",
        "labels": "RST"
    },
    {
        "idx": 193,
        "text": "Neural networks are widely used in various NLP tasks for their remarkable performance. ",
        "labels": "BAC"
    },
    {
        "idx": 193,
        "text": "However, the complexity makes them difficult to interpret, i.e., they are not guaranteed right for the right reason.",
        "labels": "GAP"
    },
    {
        "idx": 193,
        "text": "Besides the complexity, we reveal that the model pathology - the inconsistency between word saliency and model confidence, further hurts the interpretability. ",
        "labels": "GAP"
    },
    {
        "idx": 193,
        "text": "We show that the pathological inconsistency is caused by the representation collapse issue, which means that the representation of the sentences with tokens in different saliency reduced is somehow collapsed, and thus the important words cannot be distinguished from unimportant words in terms of model confidence changing.",
        "labels": "BAC"
    },
    {
        "idx": 193,
        "text": "In this paper, to mitigate the pathology and obtain more interpretable models, we propose Pathological Contrastive Training (PCT) framework, which adopts contrastive learning and saliency-based samples augmentation to calibrate the sentences representation. ",
        "labels": "PUR"
    },
    {
        "idx": 193,
        "text": "Combined with qualitative analysis, we also conduct extensive quantitative experiments and measure the interpretability with eight reasonable metrics. ",
        "labels": "MTD"
    },
    {
        "idx": 193,
        "text": "Experiments show that our method can mitigate the model pathology and generate more interpretable models while keeping the model performance.",
        "labels": "CLN"
    },
    {
        "idx": 193,
        "text": "Ablation study also shows the effectiveness.",
        "labels": "CLN"
    },
    {
        "idx": 194,
        "text": "The popularity of pretrained language models in natural language processing systems calls for a careful evaluation of such models in down-stream tasks, which have a higher potential for societal impact.",
        "labels": "BAC"
    },
    {
        "idx": 194,
        "text": "The evaluation of such systems usually focuses on accuracy measures.",
        "labels": "BAC"
    },
    {
        "idx": 194,
        "text": "Our findings in this paper call for attention to be paid to fairness measures as well.",
        "labels": "PUR"
    },
    {
        "idx": 194,
        "text": "Through the analysis of more than a dozen pretrained language models of varying sizes on two toxic text classification tasks (English),",
        "labels": "MTD"
    },
    {
        "idx": 194,
        "text": "we demonstrate that focusing on accuracy measures alone can lead to models with wide variation in fairness characteristics.",
        "labels": "CLN"
    },
    {
        "idx": 194,
        "text": "Specifically, we observe that fairness can vary even more than accuracy with increasing training data size and different random initializations.",
        "labels": "CLN"
    },
    {
        "idx": 194,
        "text": "At the same time, we find that little of the fairness variation is explained by model size, despite claims in the literature.",
        "labels": "CLN"
    },
    {
        "idx": 194,
        "text": "To improve model fairness without retraining,",
        "labels": "PUR"
    },
    {
        "idx": 194,
        "text": " we show that two post-processing methods developed for structured, tabular data can be successfully applied to a range of pretrained language models. ",
        "labels": "CLN"
    },
    {
        "idx": 194,
        "text": "Warning: This paper contains samples of offensive text.",
        "labels": "MTD"
    },
    {
        "idx": 195,
        "text": "Charts are very popular for analyzing data. ",
        "labels": "BAC"
    },
    {
        "idx": 195,
        "text": "When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations.",
        "labels": "BAC"
    },
    {
        "idx": 195,
        "text": "They also commonly refer to visual features of a chart in their questions. ",
        "labels": "BAC"
    },
    {
        "idx": 195,
        "text": "However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary.",
        "labels": "GAP"
    },
    {
        "idx": 195,
        "text": "In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. ",
        "labels": "PUR"
    },
    {
        "idx": 195,
        "text": "To address the unique challenges in our benchmark involving visual and logical reasoning over charts,",
        "labels": "PUR"
    },
    {
        "idx": 195,
        "text": "we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions.",
        "labels": "MTD"
    },
    {
        "idx": 195,
        "text": "While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.",
        "labels": "CLN"
    },
    {
        "idx": 196,
        "text": "Many recent deep learning-based solutions have adopted the attention mechanism in various tasks in the field of NLP. ",
        "labels": "BAC"
    },
    {
        "idx": 196,
        "text": "However, the inherent characteristics of deep learning models and the flexibility of the attention mechanism increase the models' complexity, thus leading to challenges in model explainability. ",
        "labels": "GAP"
    },
    {
        "idx": 196,
        "text": "To address this challenge, we propose a novel practical framework by utilizing a two-tier attention architecture to decouple the complexity of explanation and the decision-making process.",
        "labels": "PUR"
    },
    {
        "idx": 196,
        "text": "We apply it in the context of a news article classification task. ",
        "labels": "MTD"
    },
    {
        "idx": 196,
        "text": "The experiments on two large-scaled news corpora demonstrate that the proposed model can achieve competitive performance with many state-of-the-art alternatives and illustrate its appropriateness from an explainability perspective. ",
        "labels": "CLN"
    },
    {
        "idx": 196,
        "text": "We release the source code here.",
        "labels": "CTN"
    },
    {
        "idx": 197,
        "text": "Traditional methods for named entity recognition (NER) classify mentions into a fixed set of pre-defined entity types. ",
        "labels": "BAC"
    },
    {
        "idx": 197,
        "text": "However, in many real-world scenarios, new entity types are incrementally involved. ",
        "labels": "GAP"
    },
    {
        "idx": 197,
        "text": "To investigate this problem, continual learning is introduced for NER. ",
        "labels": "GAP"
    },
    {
        "idx": 197,
        "text": "However, the existing method depends on the relevance between tasks and is prone to inter-type confusion.",
        "labels": "GAP"
    },
    {
        "idx": 197,
        "text": "In this paper, we propose a novel two-stage framework Learn-and-Review (L{&R) for continual NER under the type-incremental setting to alleviate the above issues.",
        "labels": "PUR"
    },
    {
        "idx": 197,
        "text": "Specifically, for the learning stage, we distill the old knowledge from teacher to a student on the current dataset. ",
        "labels": "MTD"
    },
    {
        "idx": 197,
        "text": "For the reviewing stage, we first generate synthetic samples of old types to augment the dataset. ",
        "labels": "MTD"
    },
    {
        "idx": 197,
        "text": "Then, we further distill new knowledge from the above student and old knowledge from the teacher to get an enhanced student on the augmented dataset. ",
        "labels": "MTD"
    },
    {
        "idx": 197,
        "text": "This stage has the following advantages: (1) The synthetic samples mitigate the gap between the old and new task and thus enhance the further distillation; (2) Different types of entities are jointly seen during training which alleviates the inter-type confusion. ",
        "labels": "MTD"
    },
    {
        "idx": 197,
        "text": "Experimental results show that L{&R outperforms the state-of-the-art method on CoNLL-03 and OntoNotes-5.0.",
        "labels": "CLN"
    },
    {
        "idx": 198,
        "text": "Transcription is often reported as the bottleneck in endangered language documentation, requiring large efforts from scarce speakers and transcribers.",
        "labels": "BAC"
    },
    {
        "idx": 198,
        "text": "In general, automatic speech recognition (ASR) can be accurate enough to accelerate transcription only if trained on large amounts of transcribed data. ",
        "labels": "BAC"
    },
    {
        "idx": 198,
        "text": "However, when a single speaker is involved, several studies have reported encouraging results for phonetic transcription even with small amounts of training. ",
        "labels": "GAP"
    },
    {
        "idx": 198,
        "text": "Here we expand this body of work on speaker-dependent transcription by comparing four ASR approaches, notably recent transformer and pretrained multilingual models, on a common dataset of 11 languages. ",
        "labels": "PUR"
    },
    {
        "idx": 198,
        "text": "To automate data preparation, training and evaluation steps,",
        "labels": "PUR"
    },
    {
        "idx": 198,
        "text": "we also developed a phoneme recognition setup which handles morphologically complex languages and writing systems for which no pronunciation dictionary exists.",
        "labels": "MTD"
    },
    {
        "idx": 198,
        "text": "We find that fine-tuning a multilingual pretrained model yields an average phoneme error rate (PER) of 15% for 6 languages with 99 minutes or less of transcribed data for training. ",
        "labels": "RST"
    },
    {
        "idx": 198,
        "text": "For the 5 languages with between 100 and 192 minutes of training, we achieved a PER of 8.4% or less. ",
        "labels": "RST"
    },
    {
        "idx": 198,
        "text": "These results on a number of varied languages suggest that ASR can now significantly reduce transcription efforts in the speaker-dependent situation common in endangered language work.",
        "labels": "CLN"
    },
    {
        "idx": 199,
        "text": "Although transformer-based Neural Language Models demonstrate impressive performance on a variety of tasks, their generalization abilities are not well understood.",
        "labels": "GAP"
    },
    {
        "idx": 199,
        "text": "They have been shown to perform strongly on subject-verb number agreement in a wide array of settings, suggesting that they learned to track syntactic dependencies during their training even without explicit supervision.",
        "labels": "BAC"
    },
    {
        "idx": 199,
        "text": "In this paper, we examine the extent to which BERT is able to perform lexically-independent subject-verb number agreement (NA) on targeted syntactic templates. ",
        "labels": "PUR"
    },
    {
        "idx": 199,
        "text": "To do so, we disrupt the lexical patterns found in naturally occurring stimuli for each targeted structure in a novel fine-grained analysis of BERT's behavior. ",
        "labels": "MTD"
    },
    {
        "idx": 199,
        "text": "Our results on nonce sentences suggest that the model generalizes well for simple templates, but fails to perform lexically-independent syntactic generalization when as little as one attractor is present.",
        "labels": "CLN"
    }
]